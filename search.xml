<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker 常用命令</title>
    <url>/frank-wong.github.io/posts/f1efb8a3/</url>
    <content><![CDATA[<p>docker常用命令参考</p>
<a id="more"></a>
<h1 id="一-docker常用命令"><a href="#一-docker常用命令" class="headerlink" title="一. docker常用命令"></a>一. docker常用命令</h1><h2 id="1-1-Docker环境信息"><a href="#1-1-Docker环境信息" class="headerlink" title="1.1 Docker环境信息"></a>1.1 Docker环境信息</h2><p>用于检测Docker是否正确安装，一般结合docker version命令使用</p>
<h3 id="1-1-1-docker-info"><a href="#1-1-1-docker-info" class="headerlink" title="1.1.1 docker info"></a>1.1.1 docker info</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker info : 显示 Docker 系统信息，包括镜像和容器数。。</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker info [OPTIONS]</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">查看docker系统信息</span><br><span class="line"></span><br><span class="line">[root@frankwong yunwei]<span class="comment"># docker info</span></span><br><span class="line">Client:</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 1</span><br><span class="line">  Running: 0</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 1</span><br><span class="line"> Images: 2</span><br><span class="line"> Server Version: 18.09.1</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: <span class="literal">true</span></span><br><span class="line">  Native Overlay Diff: <span class="literal">true</span></span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: cgroupfs</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: <span class="built_in">local</span></span><br><span class="line">  Network: bridge host macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file <span class="built_in">local</span> logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: c4446665cb9c30056f4998ed953e6d4ff22c7c39</span><br><span class="line"> runc version: 4fc53a81fb7c994640722ac585fa9ca548971871</span><br><span class="line"> init version: fec3683</span><br><span class="line"> Security Options:</span><br><span class="line">  seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 4.18.0-147.el8.x86_64</span><br><span class="line"> Operating System: CentOS Linux 8 (Core)</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: x86_64</span><br><span class="line"> CPUs: 2</span><br><span class="line"> Total Memory: 1.919GiB</span><br><span class="line"> Name: frankwong</span><br><span class="line"> ID: ZMS7:D2RT:4JJT:7IB2:AMN2:WPDO:NU5V:VZ5H:DIPN:57ZW:D4US:JAG4</span><br><span class="line"> Docker Root Dir: /var/lib/docker</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"> Registry: https://index.docker.io/v1/</span><br><span class="line"> Labels:</span><br><span class="line"> Experimental: <span class="literal">false</span></span><br><span class="line"> Insecure Registries:</span><br><span class="line">  127.0.0.0/8</span><br><span class="line"> Live Restore Enabled: <span class="literal">false</span></span><br><span class="line"> Product License: Community Engine</span><br><span class="line"></span><br><span class="line">[root@frankwong yunwei]<span class="comment">#</span></span><br></pre></td></tr></table></figure></div>

<h3 id="1-1-2-docker-version"><a href="#1-1-2-docker-version" class="headerlink" title="1.1.2 docker version"></a>1.1.2 docker version</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker version :显示 Docker 版本信息</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker version [OPTIONS]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-f :指定返回值的模板文件</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">显示 Docker 版本信息</span><br><span class="line"></span><br><span class="line">[root@frankwong yunwei]<span class="comment"># docker version</span></span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           19.03.11</span><br><span class="line"> API version:       1.39</span><br><span class="line"> Go version:        go1.13.10</span><br><span class="line"> Git commit:        42e35e61f3</span><br><span class="line"> Built:             Mon Jun  1 09:13:48 2020</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Experimental:      <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          18.09.1</span><br><span class="line">  API version:      1.39 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.10.6</span><br><span class="line">  Git commit:       4c52b90</span><br><span class="line">  Built:            Wed Jan  9 19:06:30 2019</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     <span class="literal">false</span></span><br><span class="line">[root@frankwong yunwei]<span class="comment">#</span></span><br></pre></td></tr></table></figure></div>


<h2 id="1-2-容器生命周期管理"><a href="#1-2-容器生命周期管理" class="headerlink" title="1.2 容器生命周期管理"></a>1.2 容器生命周期管理</h2><h3 id="1-2-1-docker-create"><a href="#1-2-1-docker-create" class="headerlink" title="1.2.1 docker create"></a>1.2.1 docker create</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker create ：创建一个新的容器但不启动它</span><br><span class="line">用法同 docker run</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker create [OPTIONS] IMAGE [COMMAND] [ARG...]</span><br><span class="line">语法同 docker run</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">使用docker镜像nginx:latest创建一个容器,并将容器命名为myrunoob</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker create  --name myrunoob  nginx:latest      </span><br><span class="line">09b93464c2f75b7b69f83d56a9cfc23ceb50a48a9db7652ee4c27e3e2cb1961f</span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-2-docker-exec"><a href="#1-2-2-docker-exec" class="headerlink" title="1.2.2 docker exec"></a>1.2.2 docker exec</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> ：在运行的容器中执行命令</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker <span class="built_in">exec</span> [OPTIONS] CONTAINER COMMAND [ARG...]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line"></span><br><span class="line">-d :分离模式: 在后台运行</span><br><span class="line">-i :即使没有附加也保持STDIN 打开</span><br><span class="line">-t :分配一个伪终端</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">在容器 mynginx 中以交互模式执行容器内 /root/runoob.sh 脚本:</span><br><span class="line"></span><br><span class="line">  runoob@runoob:~$ docker <span class="built_in">exec</span> -it mynginx /bin/sh /root/runoob.sh</span><br><span class="line">  http://www.runoob.com/</span><br><span class="line"></span><br><span class="line">在容器 mynginx 中开启一个交互模式的终端:</span><br><span class="line"></span><br><span class="line">  runoob@runoob:~$ docker <span class="built_in">exec</span> -i -t  mynginx /bin/bash</span><br><span class="line">  root@b1a0703e41e7:/<span class="comment">#</span></span><br><span class="line"></span><br><span class="line">也可以通过 docker ps -a 命令查看已经在运行的容器，然后使用容器 ID 进入容器。</span><br><span class="line">查看已经在运行的容器 ID：</span><br><span class="line"></span><br><span class="line">  <span class="comment"># docker ps -a </span></span><br><span class="line">  ...</span><br><span class="line">  9df70f9a0714        openjdk             <span class="string">"/usercode/script.sh…"</span> </span><br><span class="line">  ...</span><br><span class="line">第一列的 9df70f9a0714 就是容器 ID。</span><br><span class="line"></span><br><span class="line">通过 <span class="built_in">exec</span> 命令对指定的容器执行 bash:</span><br><span class="line">  <span class="comment"># docker exec -it 9df70f9a0714 /bin/bash</span></span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-3-docker-run"><a href="#1-2-3-docker-run" class="headerlink" title="1.2.3 docker run"></a>1.2.3 docker run</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker run ：创建一个新的容器并运行一个命令</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line"></span><br><span class="line">-a stdin</span><br><span class="line">指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项；</span><br><span class="line">-d</span><br><span class="line">后台运行容器，并返回容器ID；</span><br><span class="line">-i</span><br><span class="line">以交互模式运行容器，通常与 -t 同时使用；</span><br><span class="line">-P</span><br><span class="line">随机端口映射，容器内部端口随机映射到主机的高端口</span><br><span class="line">-p</span><br><span class="line">指定端口映射，格式为：主机(宿主)端口:容器端口</span><br><span class="line">-t</span><br><span class="line">为容器重新分配一个伪输入终端，通常与 -i 同时使用；</span><br><span class="line">--name=<span class="string">"nginx-lb"</span></span><br><span class="line">为容器指定一个名称；</span><br><span class="line">--dns 8.8.8.8</span><br><span class="line">指定容器使用的DNS服务器，默认和宿主一致；</span><br><span class="line">--dns-search example.com</span><br><span class="line">指定容器DNS搜索域名，默认和宿主一致；</span><br><span class="line">-h <span class="string">"mars"</span></span><br><span class="line">指定容器的hostname；</span><br><span class="line">-e username=<span class="string">"ritchie"</span></span><br><span class="line">设置环境变量；</span><br><span class="line">--env-file=[]</span><br><span class="line">从指定文件读入环境变量；</span><br><span class="line">--cpuset=<span class="string">"0-2"</span> or --cpuset=<span class="string">"0,1,2"</span></span><br><span class="line">绑定容器到指定CPU运行；</span><br><span class="line">-m</span><br><span class="line">设置容器使用内存最大值；</span><br><span class="line">--net=<span class="string">"bridge"</span></span><br><span class="line">指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型；</span><br><span class="line">--link=[]</span><br><span class="line">添加链接到另一个容器；</span><br><span class="line">--expose=[]</span><br><span class="line">开放一个端口或一组端口；</span><br><span class="line">--volume , -v</span><br><span class="line">绑定一个卷</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。</span><br><span class="line"></span><br><span class="line">  docker run --name mynginx -d nginx:latest</span><br><span class="line"></span><br><span class="line">使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。</span><br><span class="line"></span><br><span class="line">  docker run -P -d nginx:latest</span><br><span class="line"></span><br><span class="line">使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。</span><br><span class="line"></span><br><span class="line">  docker run -p 80:80 -v /data:/data -d nginx:latest</span><br><span class="line"></span><br><span class="line">绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。</span><br><span class="line"></span><br><span class="line">  $ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash</span><br><span class="line"></span><br><span class="line">使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker run -it nginx:latest /bin/bash</span><br><span class="line">root@b8573233d675:/<span class="comment">#</span></span><br></pre></td></tr></table></figure></div>



<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">从image启动一个container（run）</span><br><span class="line">docker run命令首先会从特定的image创之上create一层可写的container，然后通过start命令来启动它。停止的container可以重新启动并保留原来的修改。run命令启动参数有很多。</span><br><span class="line"></span><br><span class="line">当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</span><br><span class="line"></span><br><span class="line">检查本地是否存在指定的镜像，不存在就从公有仓库下载</span><br><span class="line">利用镜像创建并启动一个容器</span><br><span class="line">分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</span><br><span class="line">从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</span><br><span class="line">从地址池配置一个 ip 地址给容器</span><br><span class="line">执行用户指定的应用程序</span><br><span class="line">执行完毕后容器被终止</span><br><span class="line">Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">使用image创建container并执行相应命令，然后停止</span><br><span class="line"><span class="comment"># docker run ubuntu echo "hello world"</span></span><br><span class="line">hello word</span><br><span class="line">这是最简单的方式，跟在本地直接执行<span class="built_in">echo</span> <span class="string">'hello world'</span> 几乎感觉不出任何区别，而实际上它会从本地ubuntu:latest镜像启动到一个容器，并执行打印命令后退出（docker ps -l可查看）。需要注意的是，默认有一个--rm=<span class="literal">true</span>参数，即完成操作后停止容器并从文件系统移除。因为Docker的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器。</span><br><span class="line">容器启动后会自动随机生成一个CONTAINER ID，这个ID在后面commit命令后可以变为IMAGE ID</span><br><span class="line"></span><br><span class="line">使用image创建container并进入交互模式, login shell是/bin/bash</span><br><span class="line"><span class="comment"># docker run -i -t --name mytest centos:centos6 /bin/bash</span></span><br><span class="line">bash-4.1<span class="comment">#</span></span><br><span class="line">上面的--name参数可以指定启动后的容器名字，如果不指定则docker会帮我们取一个名字。镜像centos:centos6也可以用IMAGE ID (68edf809afe7) 代替），并且会启动一个伪终端，但通过ps或top命令我们却只能看到一两个进程，因为容器的核心是所执行的应用程序，所需要的资源都是应用程序运行所必需的，除此之外，并没有其它的资源，可见Docker对资源的利用率极高。此时使用<span class="built_in">exit</span>或Ctrl+D退出后，这个容器也就消失了（消失后的容器并没有完全删除？）</span><br><span class="line">（那么多个TAG不同而IMAGE ID相同的的镜像究竟会运行以哪一个TAG启动呢</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">运行出一个container放到后台运行</span><br><span class="line"><span class="comment"># docker run -d ubuntu /bin/sh -c "while true; do echo hello world; sleep 2; done"</span></span><br><span class="line">ae60c4b642058fefcc61ada85a610914bed9f5df0e2aa147100eab85cea785dc</span><br><span class="line">它将直接把启动的container挂起放在后台运行（这才叫saas），并且会输出一个CONTAINER ID，通过docker ps可以看到这个容器的信息，可在container外面查看它的输出docker logs ae60c4b64205，也可以通过docker attach ae60c4b64205连接到这个正在运行的终端，此时在Ctrl+C退出container就消失了，按ctrl-p ctrl-q可以退出到宿主机，而保持container仍然在运行</span><br><span class="line">另外，如果-d启动但后面的命令执行完就结束了，如/bin/bash、<span class="built_in">echo</span> <span class="built_in">test</span>，则container做完该做的时候依然会终止。而且-d不能与--rm同时使用</span><br><span class="line">可以通过这种方式来运行memcached、apache等。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">映射host到container的端口和目录</span><br><span class="line">映射主机到容器的端口是很有用的，比如在container中运行memcached，端口为11211，运行容器的host可以连接container的 internel_ip:11211 访问，如果有从其他主机访问memcached需求那就可以通过-p选项，形如-p &lt;host_port:contain_port&gt;，存在以下几种写法：</span><br><span class="line"></span><br><span class="line">-p 11211:11211 这个即是默认情况下，绑定主机所有网卡（0.0.0.0）的11211端口到容器的11211端口上</span><br><span class="line">-p 127.0.0.1:11211:11211 只绑定localhost这个接口的11211端口</span><br><span class="line">-p 127.0.0.1::5000</span><br><span class="line">-p 127.0.0.1:80:8080</span><br><span class="line">目录映射其实是“绑定挂载”host的路径到container的目录，这对于内外传送文件比较方便，在搭建私服那一节，为了避免私服container停止以后保存的images不被删除，就要把提交的images保存到挂载的主机目录下。使用比较简单，-v &lt;host_path:container_path&gt;，绑定多个目录时再加-v。</span><br><span class="line"></span><br><span class="line">-v /tmp/docker:/tmp/docker</span><br><span class="line">另外在两个container之间建立联系可用--link，详见高级部分或官方文档。</span><br><span class="line">下面是一个例子：</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker run --name nginx_test \</span></span><br><span class="line">&gt; -v /tmp/docker:/usr/share/nginx/html:ro \</span><br><span class="line">&gt; -p 80:80 -d \</span><br><span class="line">&gt; nginx:1.7.6</span><br><span class="line">在主机的/tmp/docker下建立index.html，就可以通过http://localhost:80/或http://host-ip:80访问了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">将一个container固化为一个新的image（commit）</span><br><span class="line">当我们在制作自己的镜像的时候，会在container中安装一些工具、修改配置，如果不做commit保存起来，那么container停止以后再启动，这些更改就消失了。</span><br><span class="line">docker commit &lt;container&gt; [repo:tag]</span><br><span class="line">后面的repo:tag可选</span><br><span class="line">只能提交正在运行的container，即通过docker ps可以看见的容器，</span><br><span class="line"></span><br><span class="line">查看刚运行过的容器</span><br><span class="line"><span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID   IMAGE     COMMAND      CREATED       STATUS        PORTS   NAMES</span><br><span class="line">c9fdf26326c9   nginx:1   nginx -g..   3 hours ago   Exited (0)..     nginx_test</span><br><span class="line"></span><br><span class="line">启动一个已存在的容器（run是从image新建容器后再启动），以下也可以使用docker start nginx_test代替  </span><br><span class="line">[root@hostname docker]<span class="comment"># docker start c9fdf26326c9</span></span><br><span class="line">c9fdf26326c9</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -i -t --sig-proxy=<span class="literal">false</span> 21ffe545748baf /bin/bash</span><br><span class="line">nginx服务没有启动</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># docker commit -m "some tools installed" fcbd0a5348ca seanlook/ubuntu:14.10_tutorial</span></span><br><span class="line">fe022762070b09866eaab47bc943ccb796e53f3f416abf3f2327481b446a9503</span><br><span class="line">-a <span class="string">"seanlook7@gmail.com"</span></span><br><span class="line">请注意，当你反复去commit一个容器的时候，每次都会得到一个新的IMAGE ID，假如后面的repository:tag没有变，通过docker images可以看到，之前提交的那份镜像的repository:tag就会变成&lt;none&gt;:&lt;none&gt;，所以尽量避免反复提交。</span><br><span class="line">另外，观察以下几点:</span><br><span class="line"></span><br><span class="line">commit container只会pause住容器，这是为了保证容器文件系统的一致性，但不会stop。如果你要对这个容器继续做其他修改：</span><br><span class="line"></span><br><span class="line">你可以重新提交得到新image2，删除次新的image1</span><br><span class="line">也可以关闭容器用新image1启动，继续修改，提交image2后删除image1</span><br><span class="line">当然这样会很痛苦，所以一般是采用Dockerfile来build得到最终image，参考[]</span><br><span class="line">虽然产生了一个新的image，并且你可以看到大小有100MB，但从commit过程很快就可以知道实际上它并没有独立占用100MB的硬盘空间，而只是在旧镜像的基础上修改，它们共享大部分公共的“片”。</span><br><span class="line">下</span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-4-docker-start、stop、restart"><a href="#1-2-4-docker-start、stop、restart" class="headerlink" title="1.2.4 docker start、stop、restart"></a>1.2.4 docker start、stop、restart</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">开启/停止/重启container（start/stop/restart）</span><br><span class="line">容器可以通过run新建一个来运行，也可以重新start已经停止的container，但start不能够再指定容器启动时运行的指令，因为docker只能有一个前台进程。</span><br><span class="line">容器stop（或Ctrl+D）时，会在保存当前容器的状态之后退出，下次start时保有上次关闭时更改。而且每次进入attach进去的界面是一样的，与第一次run启动或commit提交的时刻相同。</span><br><span class="line"></span><br><span class="line">CONTAINER_ID=$(docker start &lt;containner_id&gt;)</span><br><span class="line">docker stop <span class="variable">$CONTAINER_ID</span></span><br><span class="line">docker restart <span class="variable">$CONTAINER_ID</span></span><br><span class="line">关于这几个命令可以通过一个完整的实例使用：docker如何创建一个运行后台进程的容器并同时提供shell终端。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker start :启动一个或多个已经被停止的容器</span><br><span class="line">docker stop :停止一个运行中的容器</span><br><span class="line">docker restart :重启容器</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker start [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line">docker stop [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line">docker restart [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">启动已被停止的容器myrunoob</span><br><span class="line">docker start myrunoob</span><br><span class="line"></span><br><span class="line">停止运行中的容器myrunoob</span><br><span class="line">docker stop myrunoob</span><br><span class="line"></span><br><span class="line">重启容器myrunoob</span><br><span class="line">docker restart myrunoob</span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-5-docker-kill"><a href="#1-2-5-docker-kill" class="headerlink" title="1.2.5 docker kill"></a>1.2.5 docker kill</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">kill</span> :杀掉一个运行中的容器。</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker <span class="built_in">kill</span> [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-s :向容器发送一个信号</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">杀掉运行中的容器mynginx</span><br><span class="line">runoob@runoob:~$ docker <span class="built_in">kill</span> -s KILL mynginx</span><br><span class="line">mynginx</span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-6-docker-rm"><a href="#1-2-6-docker-rm" class="headerlink" title="1.2.6 docker rm"></a>1.2.6 docker rm</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker rm ：删除一个或多个容器。</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker rm [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-f :通过 SIGKILL 信号强制删除一个运行中的容器。</span><br><span class="line">-l :移除容器间的网络连接，而非容器本身。</span><br><span class="line">-v :删除与容器关联的卷。</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">强制删除容器 db01、db02：</span><br><span class="line"></span><br><span class="line">  docker rm -f db01 db02</span><br><span class="line"></span><br><span class="line">移除容器 nginx01 对容器 db01 的连接，连接名 db：</span><br><span class="line"></span><br><span class="line">  docker rm -l db </span><br><span class="line"></span><br><span class="line">删除容器 nginx01, 并删除容器挂载的数据卷：</span><br><span class="line"></span><br><span class="line">  docker rm -v nginx01</span><br><span class="line"></span><br><span class="line">删除所有已经停止的容器：</span><br><span class="line"></span><br><span class="line">  docker rm $(docker ps -a -q)</span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-7-docker-pause、unpause"><a href="#1-2-7-docker-pause、unpause" class="headerlink" title="1.2.7 docker pause、unpause"></a>1.2.7 docker pause、unpause</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker pause :暂停容器中所有的进程。</span><br><span class="line">docker unpause :恢复容器中所有的进程。</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker pause [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line">docker unpause [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">暂停数据库容器db01提供服务。</span><br><span class="line"></span><br><span class="line">  docker pause db01</span><br><span class="line"></span><br><span class="line">恢复数据库容器db01提供服务。</span><br><span class="line"></span><br><span class="line">  docker unpause db01</span><br></pre></td></tr></table></figure></div>


<h2 id="1-3-容器操作运维"><a href="#1-3-容器操作运维" class="headerlink" title="1.3 容器操作运维"></a>1.3 容器操作运维</h2><h3 id="1-3-1-docker-ps"><a href="#1-3-1-docker-ps" class="headerlink" title="1.3.1 docker ps"></a>1.3.1 docker ps</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">查看容器的信息container（ps）</span><br><span class="line"></span><br><span class="line">docker ps命令可以查看容器的CONTAINER ID、NAME、IMAGE NAME、端口开启及绑定、容器启动后执行的COMMNAD。</span><br><span class="line">最常用的功能是通过ps来找到CONTAINER_ID，以便对特定容器进行操作。</span><br><span class="line"></span><br><span class="line">docker ps    默认显示当前正在运行中的container</span><br><span class="line"></span><br><span class="line">docker ps -a 查看包括已经停止的所有容器</span><br><span class="line"></span><br><span class="line">docker ps -l 显示最新启动的一个容器（包括已停止的）</span><br><span class="line"></span><br><span class="line">查看容器的信息container（ps）</span><br><span class="line">docker ps命令可以查看容器的CONTAINER ID、NAME、IMAGE NAME、端口开启及绑定、容器启动后执行的COMMNAD。经常通过ps来找到CONTAINER_ID。</span><br><span class="line">docker ps 默认显示当前正在运行中的container</span><br><span class="line">docker ps -a 查看包括已经停止的所有容器</span><br><span class="line">docker ps -l 显示最新启动的一个容器（包括已停止的）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker ps : 列出容器</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker ps [OPTIONS]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-a :显示所有的容器，包括未运行的。</span><br><span class="line">-f :根据条件过滤显示的内容。</span><br><span class="line">--format :指定返回值的模板文件。</span><br><span class="line">-l :显示最近创建的容器。</span><br><span class="line">-n :列出最近创建的n个容器。</span><br><span class="line">--no-trunc :不截断输出。</span><br><span class="line">-q :静默模式，只显示容器编号。</span><br><span class="line">-s :显示总的文件大小。</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">列出所有在运行的容器信息。</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID   IMAGE          COMMAND                ...  PORTS                    NAMES</span><br><span class="line">09b93464c2f7   nginx:latest   <span class="string">"nginx -g 'daemon off"</span> ...  80/tcp, 443/tcp          myrunoob</span><br><span class="line">96f7f14e99ab   mysql:5.6      <span class="string">"docker-entrypoint.sh"</span> ...  0.0.0.0:3306-&gt;3306/tcp   mymysql</span><br><span class="line"></span><br><span class="line">输出详情介绍：</span><br><span class="line">  CONTAINER ID: 容器 ID。</span><br><span class="line">  IMAGE: 使用的镜像。</span><br><span class="line">  COMMAND: 启动容器时运行的命令。</span><br><span class="line">  CREATED: 容器的创建时间。</span><br><span class="line">  STATUS: 容器状态</span><br><span class="line"></span><br><span class="line">状态有7种：</span><br><span class="line">  created（已创建）</span><br><span class="line">  restarting（重启中）</span><br><span class="line">  running（运行中）</span><br><span class="line">  removing（迁移中）</span><br><span class="line">  paused（暂停）</span><br><span class="line">  exited（停止）</span><br><span class="line">  dead（死亡）</span><br><span class="line"></span><br><span class="line">PORTS: 容器的端口信息和使用的连接类型（tcp\udp）。</span><br><span class="line">NAMES: 自动分配的容器名称。</span><br><span class="line"></span><br><span class="line">列出最近创建的5个容器信息</span><br><span class="line">runoob@runoob:~$ docker ps -n 5</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                   CREATED           </span><br><span class="line">09b93464c2f7        nginx:latest        <span class="string">"nginx -g 'daemon off"</span>    2 days ago   ...     </span><br><span class="line">b8573233d675        nginx:latest        <span class="string">"/bin/bash"</span>               2 days ago   ...     </span><br><span class="line">b1a0703e41e7        nginx:latest        <span class="string">"nginx -g 'daemon off"</span>    2 days ago   ...    </span><br><span class="line">f46fb1dec520        5c6e1090e771        <span class="string">"/bin/sh -c 'set -x \t"</span>   2 days ago   ...   </span><br><span class="line">a63b4a5597de        860c279d2fec        <span class="string">"bash"</span>                    2 days ago   ...</span><br><span class="line"></span><br><span class="line">列出所有创建的容器ID</span><br><span class="line">runoob@runoob:~$ docker ps -a -q</span><br><span class="line">09b93464c2f7</span><br><span class="line">b8573233d675</span><br><span class="line">b1a0703e41e7</span><br><span class="line">f46fb1dec520</span><br><span class="line">a63b4a5597de</span><br><span class="line">6a4aa42e947b</span><br><span class="line">de7bb36e7968</span><br><span class="line">43a432b73776</span><br><span class="line">664a8ab1a585</span><br><span class="line">ba52eb632bbd</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">根据条件过滤显示的内容</span><br><span class="line">根据标签过滤</span><br><span class="line">$ docker run -d --name=<span class="built_in">test</span>-nginx --label color=blue nginx</span><br><span class="line">$ docker ps --filter <span class="string">"label=color"</span></span><br><span class="line">$ docker ps --filter <span class="string">"label=color=blue"</span></span><br><span class="line"></span><br><span class="line">根据名称过滤</span><br><span class="line">$ docker ps --filter<span class="string">"name=test-nginx"</span></span><br><span class="line"></span><br><span class="line">根据状态过滤</span><br><span class="line">$ docker ps -a --filter <span class="string">'exited=0'</span></span><br><span class="line">$ docker ps --filter status=running</span><br><span class="line">$ docker ps --filter status=paused</span><br><span class="line"></span><br><span class="line">根据镜像过滤</span><br><span class="line"><span class="comment">#镜像名称</span></span><br><span class="line">$ docker ps --filter ancestor=nginx</span><br><span class="line"><span class="comment">#镜像ID</span></span><br><span class="line">$ docker ps --filter ancestor=d0e008c6cf02</span><br><span class="line"></span><br><span class="line">根据启动顺序过滤</span><br><span class="line">$ docker ps -f before=9c3527ed70ce</span><br><span class="line">$ docker ps -f since=6e63f6ff38b0</span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-2-docker-inspect"><a href="#1-3-2-docker-inspect" class="headerlink" title="1.3.2 docker inspect"></a>1.3.2 docker inspect</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">--inspect命令</span><br><span class="line">用于查看镜像和容器的详细信息，默认会列出全部信息，可以通过--format参数来指定输出的模板格式，以便输出特定信息。</span><br><span class="line"></span><br><span class="line">查看image或container的底层信息（inspect）</span><br><span class="line">inspect的对象可以是image、运行中的container和停止的container。</span><br><span class="line"></span><br><span class="line">查看容器的内部IP</span><br><span class="line"><span class="comment"># docker inspect --format='&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;' $CONTAINER_ID</span></span><br><span class="line">172.17.42.35</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker inspect : 获取容器/镜像的元数据</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker inspect [OPTIONS] NAME|ID [NAME|ID...]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-f :指定返回值的模板文件</span><br><span class="line">-s :显示总的文件大小</span><br><span class="line">--<span class="built_in">type</span> :为指定类型返回JSON</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">获取镜像mysql:5.6的元信息</span><br><span class="line">runoob@runoob:~$ docker inspect mysql:5.6</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"Id"</span>: <span class="string">"sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec"</span>,</span><br><span class="line">        <span class="string">"RepoTags"</span>: [</span><br><span class="line">            <span class="string">"mysql:5.6"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"RepoDigests"</span>: [],</span><br><span class="line">        <span class="string">"Parent"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"Comment"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"Created"</span>: <span class="string">"2016-05-24T04:01:41.168371815Z"</span>,</span><br><span class="line">        <span class="string">"Container"</span>: <span class="string">"e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54"</span>,</span><br><span class="line">        <span class="string">"ContainerConfig"</span>: &#123;</span><br><span class="line">            <span class="string">"Hostname"</span>: <span class="string">"b0cf605c7757"</span>,</span><br><span class="line">            <span class="string">"Domainname"</span>: <span class="string">""</span>,</span><br><span class="line">            <span class="string">"User"</span>: <span class="string">""</span>,</span><br><span class="line">            <span class="string">"AttachStdin"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"AttachStdout"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"AttachStderr"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"ExposedPorts"</span>: &#123;</span><br><span class="line">                <span class="string">"3306/tcp"</span>: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">获取正在运行的容器mymysql的 IP</span><br><span class="line">runoob@runoob:~$ docker inspect --format=<span class="string">'&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;'</span> mymysql</span><br><span class="line">172.17.0.3</span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-3-docker-top"><a href="#1-3-3-docker-top" class="headerlink" title="1.3.3 docker top"></a>1.3.3 docker top</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">查看容器中正在运行的进程（top）</span><br><span class="line">容器运行时不一定有/bin/bash终端来交互执行top命令，查看container中正在运行的进程，况且还不一定有top命令，这是docker top &lt;container_id/container_name&gt;就很有用了。实际上在host上使用ps -ef|grep docker也可以看到一组类似的进程信息，把container里的进程看成是host上启动docker的子进程就对了。</span><br><span class="line"></span><br><span class="line">docker top :查看容器中运行的进程信息，支持 ps 命令参数</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker top [OPTIONS] CONTAINER [ps OPTIONS]</span><br><span class="line"></span><br><span class="line">容器运行时不一定有/bin/bash终端来交互执行top命令，而且容器还不一定有top命令，可以使用docker top来实现查看container中正在运行的进程。</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">查看容器mymysql的进程信息</span><br><span class="line">runoob@runoob:~/mysql$ docker top mymysql</span><br><span class="line">UID    PID    PPID    C      STIME   TTY  TIME       CMD</span><br><span class="line">999    40347  40331   18     00:58   ?    00:00:02   mysqld</span><br><span class="line"></span><br><span class="line">查看所有运行容器的进程信息</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span>  `docker ps |grep Up|awk <span class="string">'&#123;print $1&#125;'</span>`;<span class="keyword">do</span> <span class="built_in">echo</span> \ &amp;&amp;docker top <span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-4-docker-attach"><a href="#1-3-4-docker-attach" class="headerlink" title="1.3.4 docker attach"></a>1.3.4 docker attach</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">--attach命令</span><br><span class="line">docker attach命令对应开发者很有用，可以连接到正在运行的容器，观察容器的运行状况，或与容器的主进程进行交互。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">连接到正在运行中的container（attach）</span><br><span class="line">要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。</span><br><span class="line">官方文档中说attach后可以通过CTRL-C来detach，但实际上经过我的测试，如果container当前在运行bash，CTRL-C自然是当前行的输入，没有退出；如果container当前正在前台运行进程，如输出nginx的access.log日志，CTRL-C不仅会导致退出容器，而且还stop了。这不是我们想要的，detach的意思按理应该是脱离容器终端，但容器依然运行。好在attach是可以带上--sig-proxy=<span class="literal">false</span>来确保CTRL-D或CTRL-C不会关闭容器。</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker attach --sig-proxy=false $CONTAINER_ID</span></span><br><span class="line"></span><br><span class="line">docker attach :连接到正在运行中的容器</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker attach [OPTIONS] CONTAINER</span><br><span class="line"></span><br><span class="line">要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。</span><br><span class="line"></span><br><span class="line">官方文档中说attach后可以通过CTRL-C来detach，但实际上经过我的测试，如果container当前在运行bash，CTRL-C自然是当前行的输入，没有退出；如果container当前正在前台运行进程，如输出nginx的access.log日志，CTRL-C不仅会导致退出容器，而且还stop了。这不是我们想要的，detach的意思按理应该是脱离容器终端，但容器依然运行。好在attach是可以带上--sig-proxy=<span class="literal">false</span>来确保CTRL-D或CTRL-C不会关闭容器。</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">容器mynginx将访问日志指到标准输出，连接到容器查看访问信息</span><br><span class="line">runoob@runoob:~$ docker attach --sig-proxy=<span class="literal">false</span> mynginx</span><br><span class="line">192.168.239.1 - - [10/Jul/2016:16:54:26 +0000] <span class="string">"GET / HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36"</span> <span class="string">"-"</span></span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-5-docker-wait"><a href="#1-3-5-docker-wait" class="headerlink" title="1.3.5 docker wait"></a>1.3.5 docker wait</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">wait</span> : 阻塞运行直到容器停止，然后打印出它的退出代码</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker <span class="built_in">wait</span> [OPTIONS] CONTAINER [CONTAINER...]</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">docker <span class="built_in">wait</span> &lt;CONTAINER&gt;</span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-6-docker-export"><a href="#1-3-6-docker-export" class="headerlink" title="1.3.6 docker export"></a>1.3.6 docker export</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">export</span> :将文件系统作为一个tar归档文件导出到STDOUT</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker <span class="built_in">export</span> [OPTIONS] CONTAINER</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-o :将输入内容写到文件</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">将id为a404c6c174a2的容器按日期保存为tar文件</span><br><span class="line">runoob@runoob:~$ docker <span class="built_in">export</span> -o mysql-`date +%Y%m%d`.tar a404c6c174a2</span><br><span class="line">runoob@runoob:~$ ls mysql-`date +%Y%m%d`.tar</span><br><span class="line">mysql-20160711.tar</span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-7-docker-port"><a href="#1-3-7-docker-port" class="headerlink" title="1.3.7 docker port"></a>1.3.7 docker port</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker port :列出指定的容器的端口映射，或者查找将PRIVATE_PORT NAT到面向公众的端口</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker port [OPTIONS] CONTAINER [PRIVATE_PORT[/PROTO]]</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">查看容器mynginx的端口映射情况</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker port mymysql</span><br><span class="line">3306/tcp -&gt; 0.0.0.0:3306</span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-8-docker-rename"><a href="#1-3-8-docker-rename" class="headerlink" title="1.3.8 docker rename"></a>1.3.8 docker rename</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-9-docker-stat"><a href="#1-3-9-docker-stat" class="headerlink" title="1.3.9 docker stat"></a>1.3.9 docker stat</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>



<h2 id="1-4-容器rootfs名"><a href="#1-4-容器rootfs名" class="headerlink" title="1.4 容器rootfs名"></a>1.4 容器rootfs名</h2><h3 id="1-4-1-docker-commit"><a href="#1-4-1-docker-commit" class="headerlink" title="1.4.1 docker commit"></a>1.4.1 docker commit</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker commit :从容器创建一个新的镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-a :提交的镜像作者；</span><br><span class="line">-c :使用Dockerfile指令来创建镜像；</span><br><span class="line">-m :提交时的说明文字；</span><br><span class="line">-p :在commit时，将容器暂停</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息</span><br><span class="line">runoob@runoob:~$ docker commit -a <span class="string">"runoob.com"</span> -m <span class="string">"my apache"</span> a404c6c174a2  mymysql:v1 </span><br><span class="line">sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057</span><br><span class="line">runoob@runoob:~$ docker images mymysql:v1</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">mymysql             v1                  37af1236adef        15 seconds ago      329 MB</span><br></pre></td></tr></table></figure></div>

<h3 id="1-4-2-docker-cp"><a href="#1-4-2-docker-cp" class="headerlink" title="1.4.2 docker cp"></a>1.4.2 docker cp</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker cp :用于容器与主机之间的数据拷贝</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-</span><br><span class="line">docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-L :保持源目标中的链接</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下</span><br><span class="line">docker cp /www/runoob 96f7f14e99ab:/www/</span><br><span class="line"></span><br><span class="line">将主机/www/runoob目录拷贝到容器96f7f14e99ab中，目录重命名为www</span><br><span class="line">docker cp /www/runoob 96f7f14e99ab:/www</span><br><span class="line"></span><br><span class="line">将容器96f7f14e99ab的/www目录拷贝到主机的/tmp目录中</span><br><span class="line">docker cp  96f7f14e99ab:/www /tmp/</span><br></pre></td></tr></table></figure></div>

<h3 id="1-4-3-docker-diff"><a href="#1-4-3-docker-diff" class="headerlink" title="1.4.3 docker diff"></a>1.4.3 docker diff</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker diff : 检查容器里文件结构的更改</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker diff [OPTIONS] CONTAINER</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">查看容器mymysql的文件结构更改</span><br><span class="line">runoob@runoob:~$ docker diff mymysql</span><br><span class="line">A /logs</span><br><span class="line">A /mysql_data</span><br><span class="line">C /run</span><br><span class="line">C /run/mysqld</span><br><span class="line">A /run/mysqld/mysqld.pid</span><br><span class="line">A /run/mysqld/mysqld.sock</span><br><span class="line">C /tmp</span><br></pre></td></tr></table></figure></div>



<h2 id="1-5-镜像仓库"><a href="#1-5-镜像仓库" class="headerlink" title="1.5 镜像仓库"></a>1.5 镜像仓库</h2><h3 id="1-5-1-docker-login、logout"><a href="#1-5-1-docker-login、logout" class="headerlink" title="1.5.1 docker login、logout"></a>1.5.1 docker login、logout</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker login : 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub</span><br><span class="line">docker <span class="built_in">logout</span> : 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker login [OPTIONS] [SERVER]</span><br><span class="line">docker <span class="built_in">logout</span> [OPTIONS] [SERVER]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-u :登陆的用户名</span><br><span class="line">-p :登陆的密码</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">登陆到Docker Hub</span><br><span class="line">docker login -u 用户名 -p 密码</span><br><span class="line"></span><br><span class="line">登出Docker Hub</span><br><span class="line">docker <span class="built_in">logout</span></span><br></pre></td></tr></table></figure></div>

<h3 id="1-5-2-docker-pull"><a href="#1-5-2-docker-pull" class="headerlink" title="1.5.2 docker pull"></a>1.5.2 docker pull</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">从docker registry server 中下拉image或repository（pull）</span><br><span class="line"></span><br><span class="line">Usage: docker pull [OPTIONS] NAME[:TAG]</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker pull centos</span></span><br><span class="line">上面的命令需要注意，在docker v1.2版本以前，会下载官方镜像的centos仓库里的所有镜像，而从v.13开始官方文档里的说明变了：will pull the centos:latest image, its intermediate layers and any aliases of the same id，也就是只会下载tag为latest的镜像（以及同一images id的其他tag）。</span><br><span class="line">也可以明确指定具体的镜像：</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker pull centos:centos6</span></span><br><span class="line">当然也可以从某个人的公共仓库（包括自己是私人仓库）拉取，形如docker pull username/repository&lt;:tag_name&gt; ：</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker pull seanlook/centos:centos6</span></span><br><span class="line">如果你没有网络，或者从其他私服获取镜像，形如docker pull registry.domain.com:5000/repos:&lt;tag_name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker pull dl.dockerpool.com:5000/mongo:latest</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker pull : 从镜像仓库中拉取或者更新指定镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker pull [OPTIONS] NAME[:TAG|@DIGEST]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-a :拉取所有 tagged 镜像</span><br><span class="line">--<span class="built_in">disable</span>-content-trust :忽略镜像的校验,默认开启</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">从Docker Hub下载java最新版镜像</span><br><span class="line">docker pull java</span><br><span class="line"></span><br><span class="line">从Docker Hub下载REPOSITORY为java的所有镜像</span><br><span class="line">docker pull -a java</span><br></pre></td></tr></table></figure></div>

<h3 id="1-5-3-docker-push"><a href="#1-5-3-docker-push" class="headerlink" title="1.5.3 docker push"></a>1.5.3 docker push</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">推送一个image或repository到registry（push）</span><br><span class="line">与上面的pull对应，可以推送到Docker Hub的Public、Private以及私服，但不能推送到Top Level Repository。</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker push seanlook/mongo</span></span><br><span class="line"><span class="comment"># docker push registry.tp-link.net:5000/mongo:2014-10-27</span></span><br><span class="line">registry.tp-link.net也可以写成IP，172.29.88.222。</span><br><span class="line">在repository不存在的情况下，命令行下push上去的会为我们创建为私有库，然而通过浏览器创建的默认为公共库</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker push : 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker push [OPTIONS] NAME[:TAG]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">--<span class="built_in">disable</span>-content-trust :忽略镜像的校验,默认开启</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">上传本地镜像myapache:v1到镜像仓库中</span><br><span class="line">docker push myapache:v1</span><br></pre></td></tr></table></figure></div>

<h3 id="1-5-4-docker-search"><a href="#1-5-4-docker-search" class="headerlink" title="1.5.4 docker search"></a>1.5.4 docker search</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">在docker index中搜索image（search）</span><br><span class="line"></span><br><span class="line">Usage: docker search TERM</span><br><span class="line"></span><br><span class="line">[root@frankwong yunwei]<span class="comment"># docker search node</span></span><br><span class="line">NAME                                   DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">node                                   Node.js is a JavaScript-based platform <span class="keyword">for</span> s…   8900                [OK]                </span><br><span class="line">mongo-express                          Web-based MongoDB admin interface, written w…   707                 [OK]                </span><br><span class="line">nodered/node-red-docker                Deprecated - older Node-RED Docker images.      345                                     [OK]</span><br><span class="line">selenium/node-chrome                                                                   209                                     [OK]</span><br><span class="line">prom/node-exporter                                                                     188                                     [OK]</span><br><span class="line">nodered/node-red                       Low-code programming <span class="keyword">for</span> event-driven applic…   139                                     </span><br><span class="line">selenium/node-firefox                                                                  133                                     [OK]</span><br><span class="line">circleci/node                          Node.js is a JavaScript-based platform <span class="keyword">for</span> s…   106                                     </span><br><span class="line">digitallyseamless/nodejs-bower-grunt    Node.js w/ Bower &amp; Grunt Dockerfile <span class="keyword">for</span> tru…   48                                      [OK]</span><br><span class="line">kkarczmarczyk/node-yarn                Node docker image with yarn package manager …   48                                      [OK]</span><br><span class="line">bitnami/node                           Bitnami Node.js Docker Image                    44                                      [OK]</span><br><span class="line">iron/node                              Tiny Node image                                 29                                      </span><br><span class="line">calico/node                            Calico<span class="string">'s per-host DaemonSet container image.…   19                                      [OK]</span></span><br><span class="line"><span class="string">appsvc/node                            Azure App Service Node.js dockerfiles           14                                      [OK]</span></span><br><span class="line"><span class="string">centos/nodejs-8-centos7                Platform for building and running Node.js 8 …   11                                      </span></span><br><span class="line"><span class="string">basi/node-exporter                     Node exporter image that allows to expose th…   8                                       [OK]</span></span><br><span class="line"><span class="string">cusspvz/node                            Super small Node.js container (~15MB) bas…    7                                       [OK]</span></span><br><span class="line"><span class="string">tarampampam/node                       Docker image, based on node, with some addit…   3                                       [OK]</span></span><br><span class="line"><span class="string">ogazitt/node-env                       node app that shows environment variables       2                                       </span></span><br><span class="line"><span class="string">ppc64le/node                           Node.js is a JavaScript-based platform for s…   2                                       </span></span><br><span class="line"><span class="string">nodecg/nodecg                          Create broadcast graphics using Node.js and …   1                                       [OK]</span></span><br><span class="line"><span class="string">bitnami/node-exporter                  Bitnami Node Exporter Docker Image              1                                       [OK]</span></span><br><span class="line"><span class="string">testim/node-chrome                     Selenium Chrome Node + Testim Extension         0                                       [OK]</span></span><br><span class="line"><span class="string">camptocamp/node-collectd               rancher node monitoring agent                   0                                       [OK]</span></span><br><span class="line"><span class="string">appsvctest/node                        node build                                      0                                       [OK]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">搜索的范围是官方镜像和所有个人公共镜像。NAME列的 / 后面是仓库的名字</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">docker search : 从Docker Hub查找镜像</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">语法</span></span><br><span class="line"><span class="string">docker search [OPTIONS] TERM</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OPTIONS说明：</span></span><br><span class="line"><span class="string">--automated :只列出 automated build类型的镜像；</span></span><br><span class="line"><span class="string">--no-trunc :显示完整的镜像描述；</span></span><br><span class="line"><span class="string">-s :列出收藏数不小于指定值的镜像。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">实例</span></span><br><span class="line"><span class="string">从Docker Hub查找所有镜像名包含java，并且收藏数大于10的镜像</span></span><br><span class="line"><span class="string">runoob@runoob:~$ docker search -s 10 java</span></span><br><span class="line"><span class="string">NAME                  DESCRIPTION                           STARS   OFFICIAL   AUTOMATED</span></span><br><span class="line"><span class="string">java                  Java is a concurrent, class-based...   1037    [OK]       </span></span><br><span class="line"><span class="string">anapsix/alpine-java   Oracle Java 8 (and 7) with GLIBC ...   115                [OK]</span></span><br><span class="line"><span class="string">develar/java                                                 46                 [OK]</span></span><br><span class="line"><span class="string">isuper/java-oracle    This repository contains all java...   38                 [OK]</span></span><br><span class="line"><span class="string">lwieske/java-8        Oracle Java 8 Container - Full + ...   27                 [OK]</span></span><br><span class="line"><span class="string">nimmis/java-centos    This is docker images of CentOS 7...   13                 [OK]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数说明：</span></span><br><span class="line"><span class="string">NAME: 镜像仓库源的名称</span></span><br><span class="line"><span class="string">DESCRIPTION: 镜像的描述</span></span><br><span class="line"><span class="string">OFFICIAL: 是否 docker 官方发布</span></span><br><span class="line"><span class="string">stars: 类似 Github 里面的 star，表示点赞、喜欢的意思</span></span><br><span class="line"><span class="string">AUTOMATED: 自动构建</span></span><br></pre></td></tr></table></figure></div>



<h2 id="1-6-本地镜像管理"><a href="#1-6-本地镜像管理" class="headerlink" title="1.6 本地镜像管理"></a>1.6 本地镜像管理</h2><h3 id="1-6-1-docker-build"><a href="#1-6-1-docker-build" class="headerlink" title="1.6.1 docker build"></a>1.6.1 docker build</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker build 使用此配置生成新的image</span><br><span class="line">build命令可以从Dockerfile和上下文来创建镜像：</span><br><span class="line">docker build [OPTIONS] PATH | URL | -</span><br><span class="line">上面的PATH或URL中的文件被称作上下文，build image的过程会先把这些文件传送到docker的服务端来进行的。</span><br><span class="line">如果PATH直接就是一个单独的Dockerfile文件则可以不需要上下文；如果URL是一个Git仓库地址，那么创建image的过程中会自动git <span class="built_in">clone</span>一份到本机的临时目录，它就成为了本次build的上下文。无论指定的PATH是什么，Dockerfile是至关重要的，请参考Dockerfile Reference。</span><br><span class="line">请看下面的例子：</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat Dockerfile </span></span><br><span class="line">FROM seanlook/nginx:bash_vim</span><br><span class="line">EXPOSE 80</span><br><span class="line">ENTRYPOINT /usr/sbin/nginx -c /etc/nginx/nginx.conf &amp;&amp; /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker build -t seanlook/nginx:bash_vim_Df .</span></span><br><span class="line">Sending build context to Docker daemon 73.45 MB</span><br><span class="line">Sending build context to Docker daemon </span><br><span class="line">Step 0 : FROM seanlook/nginx:bash_vim</span><br><span class="line"> ---&gt; aa8516fa0bb7</span><br><span class="line">Step 1 : EXPOSE 80</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; fece07e2b515</span><br><span class="line">Step 2 : ENTRYPOINT /usr/sbin/nginx -c /etc/nginx/nginx.conf &amp;&amp; /bin/bash</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> e08963fd5afb</span><br><span class="line"> ---&gt; d9bbd13f5066</span><br><span class="line">Removing intermediate container e08963fd5afb</span><br><span class="line">Successfully built d9bbd13f5066</span><br><span class="line">上面的PATH为.，所以在当前目录下的所有文件（不包括.dockerignore中的）将会被tar打包并传送到docker daemon（一般在本机），从输出我们可以到Sending build context...，最后有个Removing intermediate container的过程，可以通过--rm=<span class="literal">false</span>来保留容器。</span><br><span class="line">TO-DO</span><br><span class="line">docker build github.com/creack/docker-firefox失败。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker build 命令用于使用 Dockerfile 创建镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker build [OPTIONS] PATH | URL | -</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">--build-arg=[] :设置镜像创建时的变量；</span><br><span class="line">--cpu-shares :设置 cpu 使用权重；</span><br><span class="line">--cpu-period :限制 CPU CFS周期；</span><br><span class="line">--cpu-quota :限制 CPU CFS配额；</span><br><span class="line">--cpuset-cpus :指定使用的CPU id；</span><br><span class="line">--cpuset-mems :指定使用的内存 id；</span><br><span class="line">--<span class="built_in">disable</span>-content-trust :忽略校验，默认开启；</span><br><span class="line">-f :指定要使用的Dockerfile路径；</span><br><span class="line">--force-rm :设置镜像过程中删除中间容器；</span><br><span class="line">--isolation :使用容器隔离技术；</span><br><span class="line">--label=[] :设置镜像使用的元数据；</span><br><span class="line">-m :设置内存最大值；</span><br><span class="line">--memory-swap :设置Swap的最大值为内存+swap，<span class="string">"-1"</span>表示不限swap；</span><br><span class="line">--no-cache :创建镜像的过程不使用缓存；</span><br><span class="line">--pull :尝试去更新镜像的新版本；</span><br><span class="line">--quiet, -q :安静模式，成功后只输出镜像 ID；</span><br><span class="line">--rm :设置镜像成功后删除中间容器；</span><br><span class="line">--shm-size :设置/dev/shm的大小，默认值是64M；</span><br><span class="line">--<span class="built_in">ulimit</span> :Ulimit配置。</span><br><span class="line">--tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。</span><br><span class="line">--network: 默认 default。在构建期间设置RUN指令的网络模式</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1</span><br><span class="line">docker build -t runoob/ubuntu:v1 .</span><br><span class="line"></span><br><span class="line">使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像</span><br><span class="line">docker build github.com/creack/docker-firefox</span><br><span class="line"></span><br><span class="line">也可以通过 -f Dockerfile 文件的位置：</span><br><span class="line">$ docker build -f /path/to/a/Dockerfile .</span><br><span class="line"></span><br><span class="line">在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回：</span><br><span class="line">$ docker build -t <span class="built_in">test</span>/myapp .</span><br><span class="line">Sending build context to Docker daemon 2.048 kB</span><br><span class="line">Error response from daemon: Unknown instruction: RUNCMD</span><br></pre></td></tr></table></figure></div>

<h3 id="1-6-2-docker-images"><a href="#1-6-2-docker-images" class="headerlink" title="1.6.2 docker images"></a>1.6.2 docker images</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">-- 列出机器上的镜像（images）</span><br><span class="line"><span class="comment"># docker images </span></span><br><span class="line">REPOSITORY               TAG             IMAGE ID        CREATED         VIRTUAL SIZE</span><br><span class="line">ubuntu                   14.10           2185fd50e2ca    13 days ago     236.9 MB</span><br><span class="line">…</span><br><span class="line">其中我们可以根据REPOSITORY来判断这个镜像是来自哪个服务器，如果没有 / 则表示官方镜像，类似于username/repos_name表示Github的个人公共库，类似于regsistory.example.com:5000/repos_name则表示的是私服。</span><br><span class="line">IMAGE ID列其实是缩写，要显示完整则带上--no-trunc选项</span><br><span class="line"></span><br><span class="line">各个选项说明:</span><br><span class="line">REPOSITORY：表示镜像的仓库源</span><br><span class="line">TAG：镜像的标签</span><br><span class="line">IMAGE ID：镜像ID</span><br><span class="line">CREATED：镜像创建时间</span><br><span class="line">SIZE：镜像大小</span><br><span class="line"></span><br><span class="line">同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如 ubuntu 仓库源里，有 15.10、14.04 等多个不同的版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker images : 列出本地镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker images [OPTIONS] [REPOSITORY[:TAG]]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）；</span><br><span class="line">--digests :显示镜像的摘要信息；</span><br><span class="line">-f :显示满足条件的镜像；</span><br><span class="line">--format :指定返回值的模板文件；</span><br><span class="line">--no-trunc :显示完整的镜像信息；</span><br><span class="line">-q :只显示镜像ID</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">查看本地镜像列表</span><br><span class="line">runoob@runoob:~$ docker images</span><br><span class="line">REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">mymysql                 v1                  37af1236adef        5 minutes ago       329 MB</span><br><span class="line">runoob/ubuntu           v4                  1c06aa18edee        2 days ago          142.1 MB</span><br><span class="line">&lt;none&gt;                  &lt;none&gt;              5c6e1090e771        2 days ago          165.9 MB</span><br><span class="line">httpd                   latest              ed38aaffef30        11 days ago         195.1 MB</span><br><span class="line">alpine                  latest              4e38e38c8ce0        2 weeks ago         4.799 MB</span><br><span class="line">mongo                   3.2                 282fd552add6        3 weeks ago         336.1 MB</span><br><span class="line">redis                   latest              4465e4bcad80        3 weeks ago         185.7 MB</span><br><span class="line">php                     5.6-fpm             025041cd3aa5        3 weeks ago         456.3 MB</span><br><span class="line">python                  3.5                 045767ddf24a        3 weeks ago         684.1 MB</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">列出本地镜像中REPOSITORY为ubuntu的镜像列表</span><br><span class="line">root@runoob:~<span class="comment"># docker images  ubuntu</span></span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu              14.04               90d5884b1ee0        9 weeks ago         188 MB</span><br><span class="line">ubuntu              15.10               4e3b13c8a266        3 months ago        136.3 MB</span><br></pre></td></tr></table></figure></div>

<h3 id="1-6-3-docker-rmi"><a href="#1-6-3-docker-rmi" class="headerlink" title="1.6.3 docker rmi"></a>1.6.3 docker rmi</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">删除一个或多个container、image（rm、rmi）</span><br><span class="line">你可能在使用过程中会build或commit许多镜像，无用的镜像需要删除。但删除这些镜像是有一些条件的：</span><br><span class="line"></span><br><span class="line">同一个IMAGE ID可能会有多个TAG（可能还在不同的仓库），首先你要根据这些 image names 来删除标签，当删除最后一个tag的时候就会自动删除镜像；</span><br><span class="line">承上，如果要删除的多个IMAGE NAME在同一个REPOSITORY，可以通过docker rmi &lt;image_id&gt;来同时删除剩下的TAG；若在不同Repo则还是需要手动逐个删除TAG；</span><br><span class="line">还存在由这个镜像启动的container时（即便已经停止），也无法删除镜像；</span><br><span class="line">TO-DO</span><br><span class="line">如何查看镜像与容器的依存关系</span><br><span class="line"></span><br><span class="line">删除容器</span><br><span class="line">docker rm &lt;container_id/contaner_name&gt;</span><br><span class="line"></span><br><span class="line">删除所有停止的容器</span><br><span class="line">docker rm $(docker ps -a -q)</span><br><span class="line">删除镜像</span><br><span class="line">docker rmi &lt;image_id/image_name ...&gt;</span><br><span class="line">下面是一个完整的示例：</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker images            &lt;==</span></span><br><span class="line">ubuntu            13.10        195eb90b5349       4 months ago       184.6 MB</span><br><span class="line">ubuntu            saucy        195eb90b5349       4 months ago       184.6 MB</span><br><span class="line">seanlook/ubuntu   rm_test      195eb90b5349       4 months ago       184.6 MB</span><br><span class="line"></span><br><span class="line">使用195eb90b5349启动、停止一个容器后，删除这个镜像</span><br><span class="line"><span class="comment"># docker rmi 195eb90b5349</span></span><br><span class="line">Error response from daemon: Conflict, cannot delete image 195eb90b5349 because it is </span><br><span class="line">tagged <span class="keyword">in</span> multiple repositories, use -f to force</span><br><span class="line">2014/11/04 14:19:00 Error: failed to remove one or more images</span><br><span class="line"></span><br><span class="line">删除seanlook仓库中的tag     &lt;==</span><br><span class="line"><span class="comment"># docker rmi seanlook/ubuntu:rm_test</span></span><br><span class="line">Untagged: seanlook/ubuntu:rm_test</span><br><span class="line"></span><br><span class="line">现在删除镜像，还会由于container的存在不能rmi</span><br><span class="line"><span class="comment"># docker rmi 195eb90b5349</span></span><br><span class="line">Error response from daemon: Conflict, cannot delete 195eb90b5349 because the </span><br><span class="line"> container eef3648a6e77 is using it, use -f to force</span><br><span class="line">2014/11/04 14:24:15 Error: failed to remove one or more images</span><br><span class="line"></span><br><span class="line">先删除由这个镜像启动的容器    &lt;==</span><br><span class="line"><span class="comment"># docker rm eef3648a6e77</span></span><br><span class="line"></span><br><span class="line">删除镜像                    &lt;==</span><br><span class="line"><span class="comment"># docker rmi 195eb90b5349</span></span><br><span class="line">Deleted: 195eb90b534950d334188c3627f860fbdf898e224d8a0a11ec54ff453175e081</span><br><span class="line">Deleted: 209ea56fda6dc2fb013e4d1e40cb678b2af91d1b54a71529f7df0bd867adc961</span><br><span class="line">Deleted: 0f4aac48388f5d65a725ccf8e7caada42f136026c566528a5ee9b02467dac90a</span><br><span class="line">Deleted: fae16849ebe23b48f2bedcc08aaabd45408c62b531ffd8d3088592043d5e7364</span><br><span class="line">Deleted: f127542f0b6191e99bb015b672f5cf48fa79d974784ac8090b11aeac184eaaff</span><br><span class="line">注意，上面的删除过程我所举的例子比较特殊——镜像被tag在多个仓库，也有启动过的容器。按照&lt;==指示的顺序进行即可。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker rmi : 删除本地一个或多少镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker rmi [OPTIONS] IMAGE [IMAGE...]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-f :强制删除；</span><br><span class="line">--no-prune :不移除该镜像的过程镜像，默认移除；</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">强制删除本地镜像 runoob/ubuntu:v4</span><br><span class="line"></span><br><span class="line">root@runoob:~<span class="comment"># docker rmi -f runoob/ubuntu:v4</span></span><br><span class="line">Untagged: runoob/ubuntu:v4</span><br><span class="line">Deleted: sha256:1c06aa18edee44230f93a90a7d88139235de12cd4c089d41eed8419b503072be</span><br><span class="line">Deleted: sha256:85feb446e89a28d58ee7d80ea5ce367eebb7cec70f0ec18aa4faa874cbd97c73</span><br><span class="line"></span><br><span class="line">docker prune 命令</span><br><span class="line">prune 命令用来删除不再使用的 docker 对象。</span><br><span class="line"></span><br><span class="line">删除所有未被 tag 标记和未被容器使用的镜像:</span><br><span class="line">$ docker image prune</span><br><span class="line">WARNING! This will remove all dangling images.</span><br><span class="line">Are you sure you want to <span class="built_in">continue</span>? [y/N] y</span><br><span class="line"></span><br><span class="line">删除所有未被容器使用的镜像:</span><br><span class="line">$ docker image prune -a</span><br><span class="line"></span><br><span class="line">删除所有停止运行的容器:</span><br><span class="line">$ docker container prune</span><br><span class="line"></span><br><span class="line">删除所有未被挂载的卷:</span><br><span class="line">$ docker volume prune</span><br><span class="line"></span><br><span class="line">删除所有网络:</span><br><span class="line">$ docker network prune</span><br><span class="line"></span><br><span class="line">删除 docker 所有资源:</span><br><span class="line">$ docker system prune</span><br></pre></td></tr></table></figure></div>

<h3 id="1-6-4-docker-tag"><a href="#1-6-4-docker-tag" class="headerlink" title="1.6.4 docker tag"></a>1.6.4 docker tag</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">给镜像打上标签（tag）</span><br><span class="line">tag的作用主要有两点：一是为镜像起一个容易理解的名字，二是可以通过docker tag来重新指定镜像的仓库，这样在push时自动提交到仓库。</span><br><span class="line"></span><br><span class="line">将同一IMAGE_ID的所有tag，合并为一个新的</span><br><span class="line"><span class="comment"># docker tag 195eb90b5349 seanlook/ubuntu:rm_test</span></span><br><span class="line"></span><br><span class="line">新建一个tag，保留旧的那条记录</span><br><span class="line"><span class="comment"># docker tag Registry/Repos:Tag New_Registry/New_Repos:New_Tag</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker tag : 标记本地镜像，将其归入某一仓库</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">将镜像ubuntu:15.10标记为 runoob/ubuntu:v3 镜像</span><br><span class="line"></span><br><span class="line">root@runoob:~<span class="comment"># docker tag ubuntu:15.10 runoob/ubuntu:v3</span></span><br><span class="line">root@runoob:~<span class="comment"># docker images   runoob/ubuntu:v3</span></span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">runoob/ubuntu       v3                  4e3b13c8a266        3 months ago        136.3 MB</span><br></pre></td></tr></table></figure></div>

<h3 id="1-6-5-docker-save"><a href="#1-6-5-docker-save" class="headerlink" title="1.6.5 docker save"></a>1.6.5 docker save</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker save : 将指定镜像保存成 tar 归档文件</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker save [OPTIONS] IMAGE [IMAGE...]</span><br><span class="line"></span><br><span class="line">OPTIONS 说明：</span><br><span class="line">-o :输出到的文件</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">将镜像 runoob/ubuntu:v3 生成 my_ubuntu_v3.tar 文档</span><br><span class="line">runoob@runoob:~$ docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3</span><br><span class="line">runoob@runoob:~$ ll my_ubuntu_v3.tar</span><br><span class="line">-rw------- 1 runoob runoob 142102016 Jul 11 01:37 my_ubuntu_v3.ta</span><br><span class="line"></span><br><span class="line">docker 镜像导入导出有两种方法：</span><br><span class="line">一种是使用 save 和 load 命令</span><br><span class="line"></span><br><span class="line">使用例子如下：</span><br><span class="line">docker save ubuntu:load&gt;/root/ubuntu.tar</span><br><span class="line">docker load &lt; ubuntu.tar</span><br><span class="line"></span><br><span class="line">一种是使用 <span class="built_in">export</span> 和 import 命令</span><br><span class="line">使用例子如下：</span><br><span class="line">docker <span class="built_in">export</span> 98ca36 &gt; ubuntu.tar</span><br><span class="line">cat ubuntu.tar | sudo docker import - ubuntu:import</span><br></pre></td></tr></table></figure></div>

<h3 id="1-6-6-docker-import"><a href="#1-6-6-docker-import" class="headerlink" title="1.6.6 docker import"></a>1.6.6 docker import</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker import : 从归档文件中创建镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-c :应用docker 指令创建镜像；</span><br><span class="line">-m :提交时的说明文字；</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">从镜像归档文件my_ubuntu_v3.tar创建镜像，命名为runoob/ubuntu:v4</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker import  my_ubuntu_v3.tar runoob/ubuntu:v4  </span><br><span class="line">sha256:63ce4a6d6bc3fabb95dbd6c561404a309b7bdfc4e21c1d59fe9fe4299cbfea39</span><br><span class="line">runoob@runoob:~$ docker images runoob/ubuntu:v4</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">runoob/ubuntu       v4                  63ce4a6d6bc3        20 seconds ago      142.1 MB</span><br></pre></td></tr></table></figure></div>

<h3 id="1-6-7-docker-load"><a href="#1-6-7-docker-load" class="headerlink" title="1.6.7 docker load"></a>1.6.7 docker load</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker load : 导入使用 docker save 命令导出的镜像</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker load [OPTIONS]</span><br><span class="line"></span><br><span class="line">OPTIONS 说明：</span><br><span class="line">--input , -i : 指定导入的文件，代替 STDIN。</span><br><span class="line">--quiet , -q : 精简输出信息</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">导入镜像：</span><br><span class="line">$ docker image ls</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line"></span><br><span class="line">$ docker load &lt; busybox.tar.gz</span><br><span class="line">Loaded image: busybox:latest</span><br><span class="line"></span><br><span class="line">$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">busybox             latest              769b9341d937        7 weeks ago         2.489 MB</span><br><span class="line"></span><br><span class="line">$ docker load --input fedora.tar</span><br><span class="line">Loaded image: fedora:rawhide</span><br><span class="line">Loaded image: fedora:20</span><br><span class="line"></span><br><span class="line">$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">busybox             latest              769b9341d937        7 weeks ago         2.489 MB</span><br><span class="line">fedora              rawhide             0d20aec6529d        7 weeks ago         387 MB</span><br><span class="line">fedora              20                  58394af37342        7 weeks ago         385.5 MB</span><br><span class="line">fedora              heisenbug           58394af37342        7 weeks ago         385.5 MB</span><br><span class="line">fedora              latest              58394af37342        7 weeks ago         385.5 MB</span><br></pre></td></tr></table></figure></div>



<h2 id="1-7-容器资源管理"><a href="#1-7-容器资源管理" class="headerlink" title="1.7 容器资源管理"></a>1.7 容器资源管理</h2><h3 id="1-7-1-docker-volume"><a href="#1-7-1-docker-volume" class="headerlink" title="1.7.1 docker volume"></a>1.7.1 docker volume</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h3 id="1-7-2-docker-network"><a href="#1-7-2-docker-network" class="headerlink" title="1.7.2 docker network"></a>1.7.2 docker network</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>


<h2 id="1-8-系统日志信息"><a href="#1-8-系统日志信息" class="headerlink" title="1.8 系统日志信息"></a>1.8 系统日志信息</h2><h3 id="1-8-1-docker-events"><a href="#1-8-1-docker-events" class="headerlink" title="1.8.1 docker events"></a>1.8.1 docker events</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker events : 从服务器获取实时事件</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker events [OPTIONS]</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-f ：根据条件过滤事件；</span><br><span class="line">--since ：从指定的时间戳后显示所有事件;</span><br><span class="line">--until ：流水时间显示到指定的时间为止；</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">显示docker 2016年7月1日后的所有事件</span><br><span class="line">runoob@runoob:~/mysql$ docker events  --since=<span class="string">"1467302400"</span></span><br><span class="line">2016-07-08T19:44:54.501277677+08:00 network connect 66f958fd13dc4314ad20034e576d5c5eba72e0849dcc38ad9e8436314a4149d4 (container=b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64, name=bridge, <span class="built_in">type</span>=bridge)</span><br><span class="line">2016-07-08T19:44:54.723876221+08:00 container start b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (image=nginx:latest, name=elegant_albattani)</span><br><span class="line">2016-07-08T19:44:54.726110498+08:00 container resize b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (height=39, image=nginx:latest, name=elegant_albattani, width=167)</span><br><span class="line">2016-07-08T19:46:22.137250899+08:00 container die b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (exitCode=0, image=nginx:latest, name=elegant_albattani)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">显示docker 镜像为mysql:5.6 2016年7月1日后的相关事件</span><br><span class="line">runoob@runoob:~/mysql$ docker events -f <span class="string">"image"</span>=<span class="string">"mysql:5.6"</span> --since=<span class="string">"1467302400"</span> </span><br><span class="line">2016-07-11T00:38:53.975174837+08:00 container start 96f7f14e99ab9d2f60943a50be23035eda1623782cc5f930411bbea407a2bb10 (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:51:17.022572452+08:00 container <span class="built_in">kill</span> 96f7f14e99ab9d2f60943a50be23035eda1623782cc5f930411bbea407a2bb10 (image=mysql:5.6, name=mymysql, signal=9)</span><br><span class="line">2016-07-11T00:51:17.132532080+08:00 container die 96f7f14e99ab9d2f60943a50be23035eda1623782cc5f930411bbea407a2bb10 (exitCode=137, image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:51:17.514661357+08:00 container destroy 96f7f14e99ab9d2f60943a50be23035eda1623782cc5f930411bbea407a2bb10 (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:57:18.551984549+08:00 container create c8f0a32f12f5ec061d286af0b1285601a3e33a90a08ff1706de619ac823c345c (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:57:18.557405864+08:00 container attach c8f0a32f12f5ec061d286af0b1285601a3e33a90a08ff1706de619ac823c345c (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:57:18.844134112+08:00 container start c8f0a32f12f5ec061d286af0b1285601a3e33a90a08ff1706de619ac823c345c (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:57:19.140141428+08:00 container die c8f0a32f12f5ec061d286af0b1285601a3e33a90a08ff1706de619ac823c345c (exitCode=1, image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:58:05.941019136+08:00 container destroy c8f0a32f12f5ec061d286af0b1285601a3e33a90a08ff1706de619ac823c345c (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:58:07.965128417+08:00 container create a404c6c174a21c52f199cfce476e041074ab020453c7df2a13a7869b48f2f37e (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:58:08.188734598+08:00 container start a404c6c174a21c52f199cfce476e041074ab020453c7df2a13a7869b48f2f37e (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T00:58:20.010876777+08:00 container top a404c6c174a21c52f199cfce476e041074ab020453c7df2a13a7869b48f2f37e (image=mysql:5.6, name=mymysql)</span><br><span class="line">2016-07-11T01:06:01.395365098+08:00 container top a404c6c174a21c52f199cfce476e041074ab020453c7df2a13a7869b48f2f37e (image=mysql:5.6, name=mymysql)</span><br><span class="line"></span><br><span class="line">如果指定的时间是到秒级的，需要将时间转成时间戳。如果时间为日期的话，可以直接使用，如--since=<span class="string">"2016-07-01"</span>。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[时间转换时间戳]</span><br><span class="line"></span><br><span class="line">A.将日期转换为Unix时间戳</span><br><span class="line">将当前时间以Unix时间戳表示：</span><br><span class="line">date +%s</span><br><span class="line"></span><br><span class="line">输出如下：</span><br><span class="line">1361542433</span><br><span class="line"></span><br><span class="line">转换指定日期为Unix时间戳：</span><br><span class="line">date -d <span class="string">'2013-2-22 22:14'</span> +%s</span><br><span class="line"></span><br><span class="line">输出如下：</span><br><span class="line">1361542440</span><br><span class="line"></span><br><span class="line">B.将Unix时间戳转换为日期时间</span><br><span class="line">不指定日期时间的格式：</span><br><span class="line">date -d @1361542596</span><br><span class="line"></span><br><span class="line">输出如下：</span><br><span class="line">Fri Feb 22 22:16:36 CST 2013</span><br><span class="line"></span><br><span class="line">指定日期格式的转换：</span><br><span class="line">date -d @1361542596 +<span class="string">"%Y-%m-%d %H:%M:%S"</span></span><br><span class="line"></span><br><span class="line">输出如下：</span><br><span class="line">2013-02-22 22:16:36</span><br></pre></td></tr></table></figure></div>

<h3 id="1-8-2-docker-history"><a href="#1-8-2-docker-history" class="headerlink" title="1.8.2 docker history"></a>1.8.2 docker history</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">history</span> : 查看指定镜像的创建历史</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker <span class="built_in">history</span> [OPTIONS] IMAGE</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-H :以可读的格式打印镜像大小和日期，默认为<span class="literal">true</span>；</span><br><span class="line">--no-trunc :显示完整的提交记录；</span><br><span class="line">-q :仅列出提交记录ID</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">查看本地镜像runoob/ubuntu:v3的创建历史</span><br><span class="line">root@runoob:~<span class="comment"># docker history runoob/ubuntu:v3</span></span><br><span class="line">IMAGE             CREATED           CREATED BY                                      SIZE      COMMENT</span><br><span class="line">4e3b13c8a266      3 months ago      /bin/sh -c <span class="comment">#(nop) CMD ["/bin/bash"]             0 B                 </span></span><br><span class="line">&lt;missing&gt;         3 months ago      /bin/sh -c sed -i <span class="string">'s/^#\s*\(deb.*universe\)$/   1.863 kB            </span></span><br><span class="line"><span class="string">&lt;missing&gt;         3 months ago      /bin/sh -c set -xe   &amp;&amp; echo '</span><span class="comment">#!/bin/sh' &gt; /u   701 B               </span></span><br><span class="line">&lt;missing&gt;         3 months ago      /bin/sh -c <span class="comment">#(nop) ADD file:43cb048516c6b80f22   136.3 MB</span></span><br></pre></td></tr></table></figure></div>

<h3 id="1-8-3-docker-logs"><a href="#1-8-3-docker-logs" class="headerlink" title="1.8.3 docker logs"></a>1.8.3 docker logs</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">docker logs : 获取容器的日志</span><br><span class="line"></span><br><span class="line">语法</span><br><span class="line">docker logs [OPTIONS] CONTAINER</span><br><span class="line"></span><br><span class="line">OPTIONS说明：</span><br><span class="line">-f : 跟踪日志输出</span><br><span class="line">--since :显示某个开始时间的所有日志</span><br><span class="line">-t : 显示时间戳</span><br><span class="line">--tail :仅列出最新N条容器日志</span><br><span class="line"></span><br><span class="line">实例</span><br><span class="line">跟踪查看容器mynginx的日志输出</span><br><span class="line">runoob@runoob:~$ docker logs -f mynginx</span><br><span class="line">192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] <span class="string">"GET / HTTP/1.1"</span> 200 612 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">2016/07/10 16:53:33 [error] 5<span class="comment">#5: *1 open() "/usr/share/nginx/html/favicon.ico" failed (2: No such file or directory), client: 192.168.239.1, server: localhost, request: "GET /favicon.ico HTTP/1.1", host: "192.168.239.130", referrer: "http://192.168.239.130/"</span></span><br><span class="line">192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 404 571 <span class="string">"http://192.168.239.130/"</span> <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.239.1 - - [10/Jul/2016:16:53:59 +0000] <span class="string">"GET / HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">查看容器mynginx从2016年7月1日后的最新10条日志</span><br><span class="line">docker logs --since=<span class="string">"2016-07-01"</span> --tail=10 mynginx</span><br></pre></td></tr></table></figure></div>

<h1 id="二-Docker-资源汇总"><a href="#二-Docker-资源汇总" class="headerlink" title="二. Docker 资源汇总"></a>二. Docker 资源汇总</h1><h2 id="2-1-Docker-资源"><a href="#2-1-Docker-资源" class="headerlink" title="2.1 Docker 资源"></a>2.1 Docker 资源</h2><ul>
<li>Docker 官方主页: <a href="https://www.docker.com/" target="_blank" rel="noopener">https://www.docker.com</a></li>
<li>Docker 官方博客: <a href="https://blog.docker.com/" target="_blank" rel="noopener">https://blog.docker.com/</a></li>
<li>Docker 官方文档: <a href="https://docs.docker.com/" target="_blank" rel="noopener">https://docs.docker.com/</a></li>
<li>Docker Store: <a href="https://store.docker.com/" target="_blank" rel="noopener">https://store.docker.com</a></li>
<li>Docker Cloud: <a href="https://cloud.docker.com/" target="_blank" rel="noopener">https://cloud.docker.com</a></li>
<li>Docker Hub: <a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com</a></li>
<li>Docker 的源代码仓库: <a href="https://github.com/moby/moby">https://github.com/moby/moby</a></li>
<li>Docker 发布版本历史: <a href="https://docs.docker.com/release-notes/" target="_blank" rel="noopener">https://docs.docker.com/release-notes/</a></li>
<li>Docker 常见问题: <a href="https://docs.docker.com/engine/faq/" target="_blank" rel="noopener">https://docs.docker.com/engine/faq/</a></li>
<li>Docker 远端应用 API: <a href="https://docs.docker.com/develop/sdk/" target="_blank" rel="noopener">https://docs.docker.com/develop/sdk/</a></li>
</ul>
<h2 id="2-2-Docker-国内镜像"><a href="#2-2-Docker-国内镜像" class="headerlink" title="2.2 Docker 国内镜像"></a>2.2 Docker 国内镜像</h2><ul>
<li>阿里云的加速器：<a href="https://help.aliyun.com/document_detail/60750.html" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/60750.html</a></li>
<li>网易加速器：<a href="http://hub-mirror.c.163.com" target="_blank" rel="noopener">http://hub-mirror.c.163.com</a></li>
<li>官方中国加速器：<a href="https://registry.docker-cn.com" target="_blank" rel="noopener">https://registry.docker-cn.com</a></li>
<li>ustc 的镜像：<a href="https://docker.mirrors.ustc.edu.cn" target="_blank" rel="noopener">https://docker.mirrors.ustc.edu.cn</a></li>
<li>daocloud：<a href="https://www.daocloud.io/mirror#accelerator-doc（注册后使用）" target="_blank" rel="noopener">https://www.daocloud.io/mirror#accelerator-doc（注册后使用）</a></li>
</ul>
<h2 id="2-3-Docker-常用案例"><a href="#2-3-Docker-常用案例" class="headerlink" title="2.3 Docker 常用案例"></a>2.3 Docker 常用案例</h2><h3 id="2-3-1-Hello-World"><a href="#2-3-1-Hello-World" class="headerlink" title="2.3.1 Hello World"></a>2.3.1 Hello World</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">Docker 允许你在容器内运行应用程序， 使用 docker run 命令来在容器内运行一个应用程序</span><br><span class="line"></span><br><span class="line">输出Hello world</span><br><span class="line">runoob@runoob:~$ docker run ubuntu:15.10 /bin/<span class="built_in">echo</span> <span class="string">"Hello world"</span></span><br><span class="line">Hello world</span><br><span class="line"></span><br><span class="line">各个参数解析：</span><br><span class="line">  docker: Docker 的二进制执行文件。</span><br><span class="line">  run: 与前面的 docker 组合来运行一个容器。</span><br><span class="line">  ubuntu:15.10 指定要运行的镜像，Docker 首先从本地主机上查找镜像是否存在，如果不存在，Docker 就会从镜像仓库 Docker Hub 下载公共镜像。</span><br><span class="line">  /bin/<span class="built_in">echo</span> <span class="string">"Hello world"</span>: 在启动的容器里执行的命令</span><br><span class="line"></span><br><span class="line">以上命令完整的意思可以解释为：Docker 以 ubuntu15.10 镜像创建一个新容器，然后在容器里执行 bin/<span class="built_in">echo</span> <span class="string">"Hello world"</span>，然后输出结果</span><br></pre></td></tr></table></figure></div>
<h3 id="2-3-2-运行交互式的容器"><a href="#2-3-2-运行交互式的容器" class="headerlink" title="2.3.2 运行交互式的容器"></a>2.3.2 运行交互式的容器</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">我们通过 docker 的两个参数 -i -t，让 docker 运行的容器实现<span class="string">"对话"</span>的能力</span><br><span class="line"></span><br><span class="line">$ docker run -i -t ubuntu:15.10 /bin/bash</span><br><span class="line">root@0123ce188bd8:/<span class="comment">#</span></span><br><span class="line"></span><br><span class="line">各个参数解析：</span><br><span class="line">-t: 在新容器内指定一个伪终端或终端。</span><br><span class="line">-i: 允许你对容器内的标准输入 (STDIN) 进行交互</span><br><span class="line"></span><br><span class="line">注意第二行 root@0123ce188bd8:/<span class="comment">#，此时我们已进入一个 ubuntu15.10 系统的容器</span></span><br><span class="line">我们尝试在容器中运行命令 cat /proc/version和ls分别查看当前系统的版本信息和当前目录下的文件列表</span><br><span class="line">root@0123ce188bd8:/<span class="comment">#  cat /proc/version</span></span><br><span class="line">Linux version 4.4.0-151-generic (buildd@lgw01-amd64-043) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) ) <span class="comment">#178-Ubuntu SMP Tue Jun 11 08:30:22 UTC 2019</span></span><br><span class="line">root@0123ce188bd8:/<span class="comment"># ls</span></span><br><span class="line">bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">root@0123ce188bd8:/<span class="comment"># </span></span><br><span class="line"></span><br><span class="line">我们可以通过运行 <span class="built_in">exit</span> 命令或者使用 CTRL+D 来退出容器</span><br><span class="line">root@0123ce188bd8:/<span class="comment">#  exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">root@runoob:~<span class="comment"># </span></span><br><span class="line">注意第三行中 root@runoob:~<span class="comment"># 表明我们已经退出了当期的容器，返回到当前的主机中</span></span><br></pre></td></tr></table></figure></div>
<h3 id="2-3-3-启动容器（后台模式）"><a href="#2-3-3-启动容器（后台模式）" class="headerlink" title="2.3.3 启动容器（后台模式）"></a>2.3.3 启动容器（后台模式）</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">使用以下命令创建一个以进程方式运行的容器</span><br><span class="line">runoob@runoob:~$ docker run -d ubuntu:15.10 /bin/sh -c <span class="string">"while true; do echo hello world; sleep 1; done"</span></span><br><span class="line">2b1b7a428627c51ab8810d541d759f072b4fc75487eed05812646b8534a2fe63</span><br><span class="line"></span><br><span class="line">在输出中，我们没有看到期望的 <span class="string">"hello world"</span>，而是一串长字符</span><br><span class="line">2b1b7a428627c51ab8810d541d759f072b4fc75487eed05812646b8534a2fe63</span><br><span class="line">这个长字符串叫做容器 ID，对每个容器来说都是唯一的，我们可以通过容器 ID 来查看对应的容器发生了什么。</span><br><span class="line">首先，我们需要确认容器有在运行，可以通过 docker ps 来查看：</span><br><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND              ...  </span><br><span class="line">5917eac21c36        ubuntu:15.10           <span class="string">"/bin/sh -c 'while t…"</span>    ...</span><br><span class="line"></span><br><span class="line">输出详情介绍：</span><br><span class="line"></span><br><span class="line">CONTAINER ID: 容器 ID。</span><br><span class="line">IMAGE: 使用的镜像。</span><br><span class="line">COMMAND: 启动容器时运行的命令。</span><br><span class="line">CREATED: 容器的创建时间。</span><br><span class="line">STATUS: 容器状态。</span><br><span class="line"> 状态有7种：</span><br><span class="line">   created（已创建）</span><br><span class="line">   restarting（重启中）</span><br><span class="line">   running（运行中）</span><br><span class="line">   removing（迁移中）</span><br><span class="line">   paused（暂停）</span><br><span class="line">   exited（停止）</span><br><span class="line">   dead（死亡）</span><br><span class="line">   </span><br><span class="line">PORTS: 容器的端口信息和使用的连接类型（tcp\udp）</span><br><span class="line">NAMES: 自动分配的容器名称</span><br><span class="line"></span><br><span class="line">在宿主主机内使用 docker logs 命令，查看容器内的标准输出：</span><br><span class="line">runoob@runoob:~$ docker logs 2b1b7a428627</span><br><span class="line">runoob@runoob:~$ docker logs amazing_cori</span><br></pre></td></tr></table></figure></div>
<h3 id="2-3-4-停止容器"><a href="#2-3-4-停止容器" class="headerlink" title="2.3.4 停止容器"></a>2.3.4 停止容器</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">使用 docker stop 命令来停止容器:</span><br><span class="line">$ docker stop 2bxxxxxxxx</span><br><span class="line"></span><br><span class="line">通过 docker ps 查看，容器已经停止工作</span><br><span class="line">$ docker ps</span><br><span class="line"></span><br><span class="line">也可以用下面的命令来停止:</span><br><span class="line">$ docker stop amazing_cori</span><br></pre></td></tr></table></figure></div>

<h3 id="2-3-5-Docker-容器使用"><a href="#2-3-5-Docker-容器使用" class="headerlink" title="2.3.5 Docker 容器使用"></a>2.3.5 Docker 容器使用</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">通过命令 docker <span class="built_in">command</span> --<span class="built_in">help</span> 可以了解命令使用方法</span><br><span class="line"></span><br><span class="line">查看 docker stats 指令的具体使用方法</span><br><span class="line"><span class="comment"># docker stats --help</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[容器使用]</span><br><span class="line">获取镜像，如果我们本地没有 ubuntu 镜像，我们可以使用 docker pull 命令来载入 ubuntu 镜像：</span><br><span class="line">$ docker pull ubuntu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[启动容器]</span><br><span class="line">以下命令使用 ubuntu 镜像启动一个容器，参数为以命令行模式进入该容器：</span><br><span class="line">$ docker run -it ubuntu /bin/bash</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">-i: 交互式操作</span><br><span class="line">-t: 终端</span><br><span class="line">ubuntu: ubuntu 镜像</span><br><span class="line">/bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash</span><br><span class="line"></span><br><span class="line">要退出终端，直接输入 <span class="built_in">exit</span>:</span><br><span class="line">root@ed09e4490c57:/<span class="comment"># exit</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[启动已停止运行的容器]</span><br><span class="line"></span><br><span class="line">查看所有的容器命令如下：</span><br><span class="line">$ docker ps -a</span><br><span class="line"></span><br><span class="line">使用 docker start 启动一个已停止的容器：</span><br><span class="line">$ docker start b750bbbcfd88</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[后台运行]</span><br><span class="line">在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 -d 指定容器的运行模式</span><br><span class="line"></span><br><span class="line">$ docker run -itd --name ubuntu-test ubuntu /bin/bash</span><br><span class="line"></span><br><span class="line">注：加了 -d 参数默认不会进入容器，想要进入容器需要使用指令 docker <span class="built_in">exec</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[停止一个容器]</span><br><span class="line">$ docker stop &lt;容器 ID&gt;</span><br><span class="line"></span><br><span class="line">停止的容器可以通过 docker restart 重启：</span><br><span class="line">$ docker restart &lt;容器 ID&gt;</span><br><span class="line"></span><br><span class="line">[进入容器]</span><br><span class="line">在使用 -d 参数时，容器启动后会进入后台。此时想要进入容器，可以通过以下指令进入：</span><br><span class="line">  docker attach</span><br><span class="line">  docker <span class="built_in">exec</span>：推荐大家使用 docker <span class="built_in">exec</span> 命令，因为此退出容器终端，不会导致容器的停止</span><br><span class="line"></span><br><span class="line">attach 命令</span><br><span class="line">$ docker attach 1e560fca3906 </span><br><span class="line">注意： 如果从这个容器退出，会导致容器的停止</span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> 命令</span><br><span class="line">$ docker <span class="built_in">exec</span> -it 243c32535da7 /bin/bash</span><br><span class="line"></span><br><span class="line">注意： 如果从这个容器退出，不会导致容器的停止，这就是推荐使用 docker <span class="built_in">exec</span> 的原因</span><br><span class="line">参考：docker <span class="built_in">exec</span> --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">[导出和导入容器]</span><br><span class="line">导出容器</span><br><span class="line">如果要导出本地某个容器，可以使用 docker <span class="built_in">export</span> 命令</span><br><span class="line">$ docker <span class="built_in">export</span> 1e560fca3906 &gt; ubuntu.tar</span><br><span class="line">这样将导出容器快照到本地文件</span><br><span class="line"></span><br><span class="line">导入容器快照</span><br><span class="line">可以使用 docker import 从容器快照文件中再导入为镜像，以下实例将快照文件 ubuntu.tar 导入到镜像 <span class="built_in">test</span>/ubuntu:v1:</span><br><span class="line">$ cat docker/ubuntu.tar | docker import - <span class="built_in">test</span>/ubuntu:v1</span><br><span class="line"></span><br><span class="line">此外，也可以通过指定 URL 或者某个目录来导入</span><br><span class="line">$ docker import http://example.com/exampleimage.tgz example/imagerepo</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[删除容器]</span><br><span class="line">删除容器使用 docker rm 命令</span><br><span class="line">$ docker rm -f 1e560fca3906</span><br><span class="line"></span><br><span class="line">下面的命令可以清理掉所有处于终止状态的容器</span><br><span class="line">$ docker container prune</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[运行一个 web 应用]</span><br><span class="line"></span><br><span class="line">前面我们运行的容器并没有一些什么特别的用处。</span><br><span class="line">接下来让我们尝试使用 docker 构建一个 web 应用程序。</span><br><span class="line">我们将在docker容器中运行一个 Python Flask 应用来运行一个web应用</span><br><span class="line"></span><br><span class="line">runoob@runoob:~<span class="comment"># docker pull training/webapp  # 载入镜像</span></span><br><span class="line">runoob@runoob:~<span class="comment"># docker run -d -P training/webapp python app.py</span></span><br><span class="line"></span><br><span class="line">参数说明:</span><br><span class="line">-d:让容器在后台运行。</span><br><span class="line">-P:将容器内部使用的网络端口映射到我们使用的主机上。</span><br><span class="line"></span><br><span class="line">[查看 WEB 应用容器]</span><br><span class="line">使用 docker ps 来查看我们正在运行的容器：</span><br><span class="line">runoob@runoob:~<span class="comment">#  docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             ...        PORTS                 </span><br><span class="line">d3d5e39ed9d3        training/webapp     <span class="string">"python app.py"</span>     ...        0.0.0.0:32769-&gt;5000/tcp</span><br><span class="line"></span><br><span class="line">这里多了端口信息</span><br><span class="line">PORTS</span><br><span class="line">0.0.0.0:32769-&gt;5000/tcp</span><br><span class="line"></span><br><span class="line">Docker 开放了 5000 端口（默认 Python Flask 端口）映射到主机端口 32769 上</span><br><span class="line"></span><br><span class="line">通过浏览器访问WEB应用</span><br><span class="line">http://xx.xx.xx.xx:32769</span><br><span class="line"></span><br><span class="line">我们也可以通过 -p 参数来设置不一样的端口：</span><br><span class="line">runoob@runoob:~$ docker run -d -p 5000:5000 training/webapp python app.py</span><br><span class="line"></span><br><span class="line">docker ps查看正在运行的容器</span><br><span class="line">runoob@runoob:~<span class="comment">#  docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                             PORTS                     NAMES</span><br><span class="line">bf08b7f2cd89        training/webapp     ...        0.0.0.0:5000-&gt;5000/tcp    wizardly_chandrasekhar</span><br><span class="line">d3d5e39ed9d3        training/webapp     ...        0.0.0.0:32769-&gt;5000/tcp   xenodochial_hoov</span><br><span class="line"></span><br><span class="line">容器内部的 5000 端口映射到我们本地主机的 5000 端口上</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[网络端口的快捷方式]</span><br><span class="line"></span><br><span class="line">通过 docker ps 命令可以查看到容器的端口映射，docker 还提供了另一个快捷方式 docker port，</span><br><span class="line">使用 docker port 可以查看指定 （ID 或者名字）容器的某个确定端口映射到宿主机的端口号。</span><br><span class="line"></span><br><span class="line">上面创建的 web 应用容器 ID 为 bf08b7f2cd89 名字为 wizardly_chandrasekhar</span><br><span class="line"></span><br><span class="line">我可以使用 docker port bf08b7f2cd89 或 docker port wizardly_chandrasekhar 来查看容器端口的映射情况</span><br><span class="line"></span><br><span class="line">$ docker port bf08b7f2cd89</span><br><span class="line">5000/tcp -&gt; 0.0.0.0:5000</span><br><span class="line"></span><br><span class="line">$ docker port wizardly_chandrasekhar</span><br><span class="line">5000/tcp -&gt; 0.0.0.0:5000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[查看 WEB 应用程序日志]</span><br><span class="line"></span><br><span class="line">docker logs [ID或者名字] 可以查看容器内部的标准输出</span><br><span class="line">runoob@runoob:~$ docker logs -f bf08b7f2cd89</span><br><span class="line"> * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)</span><br><span class="line">192.168.239.1 - - [09/May/2016 16:30:37] <span class="string">"GET / HTTP/1.1"</span> 200 -</span><br><span class="line">192.168.239.1 - - [09/May/2016 16:30:37] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 404 -</span><br><span class="line"></span><br><span class="line">-f: 让 docker logs 像使用 tail -f 一样来输出容器内部的标准输出。</span><br><span class="line">从上面，我们可以看到应用程序使用的是 5000 端口并且能够查看到应用程序的访问日志。</span><br><span class="line"></span><br><span class="line">[查看WEB应用程序容器的进程]</span><br><span class="line">我们还可以使用 docker top 来查看容器内部运行的进程</span><br><span class="line"></span><br><span class="line">$ docker top wizardly_chandrasekhar</span><br><span class="line">UID     PID         PPID          ...       TIME                CMD</span><br><span class="line">root    23245       23228         ...       00:00:00            python app.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[检查 WEB 应用程序]</span><br><span class="line"></span><br><span class="line">使用 docker inspect 来查看 Docker 的底层信息。它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息</span><br><span class="line">$ docker inspect wizardly_chandrasekhar</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"Id"</span>: <span class="string">"bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85"</span>,</span><br><span class="line">        <span class="string">"Created"</span>: <span class="string">"2018-09-17T01:41:26.174228707Z"</span>,</span><br><span class="line">        <span class="string">"Path"</span>: <span class="string">"python"</span>,</span><br><span class="line">        <span class="string">"Args"</span>: [</span><br><span class="line">            <span class="string">"app.py"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"State"</span>: &#123;</span><br><span class="line">            <span class="string">"Status"</span>: <span class="string">"running"</span>,</span><br><span class="line">            <span class="string">"Running"</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">"Paused"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"Restarting"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"OOMKilled"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"Dead"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"Pid"</span>: 23245,</span><br><span class="line">            <span class="string">"ExitCode"</span>: 0,</span><br><span class="line">            <span class="string">"Error"</span>: <span class="string">""</span>,</span><br><span class="line">            <span class="string">"StartedAt"</span>: <span class="string">"2018-09-17T01:41:26.494185806Z"</span>,</span><br><span class="line">            <span class="string">"FinishedAt"</span>: <span class="string">"0001-01-01T00:00:00Z"</span></span><br><span class="line">        &#125;,</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[停止 WEB 应用容器]</span><br><span class="line">$ docker stop wizardly_chandrasekhar   </span><br><span class="line">wizardly_chandrasekhar</span><br><span class="line"></span><br><span class="line">[重启WEB应用容器]</span><br><span class="line">已经停止的容器，我们可以使用命令 docker start 来启动</span><br><span class="line">$ docker start wizardly_chandrasekhar</span><br><span class="line">wizardly_chandrasekhar</span><br><span class="line"></span><br><span class="line">docker ps -l 查询最后一次创建的容器</span><br><span class="line"><span class="comment">#  docker ps -l </span></span><br><span class="line">CONTAINER ID        IMAGE                             PORTS                     NAMES</span><br><span class="line">bf08b7f2cd89        training/webapp     ...        0.0.0.0:5000-&gt;5000/tcp    wizardly_chandrasekhar</span><br><span class="line"></span><br><span class="line">正在运行的容器，我们可以使用 docker restart 命令来重启</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[移除WEB应用容器]</span><br><span class="line">使用 docker rm 命令来删除不需要的容器</span><br><span class="line">$ docker rm wizardly_chandrasekhar  </span><br><span class="line">wizardly_chandrasekhar</span><br><span class="line"></span><br><span class="line">删除容器时，容器必须是停止状态，否则会报如下错误</span><br><span class="line">$ docker rm wizardly_chandrasekhar</span><br><span class="line">Error response from daemon: You cannot remove a running container bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85. Stop the container before attempting removal or force remove</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[ 权限问题报错：]</span><br><span class="line">安装完docker后，执行docker相关命令，出现：</span><br><span class="line">”Got permission denied <span class="keyword">while</span> trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.26/images/json: dial unix /var/run/docker.sock: connect: permission denied“</span><br><span class="line"></span><br><span class="line">原因</span><br><span class="line">摘自docker mannual上的一段话：</span><br><span class="line">---</span><br><span class="line">Manage Docker as a non-root user</span><br><span class="line"></span><br><span class="line">The docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can only access it using sudo. The docker daemon always runs as the root user.</span><br><span class="line"></span><br><span class="line">If you don’t want to use sudo when you use the docker <span class="built_in">command</span>, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket <span class="built_in">read</span>/writable by the docker group</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">大概的意思就是：docker进程使用Unix Socket而不是TCP端口。而默认情况下，Unix socket属于root用户，需要root权限才能访问。</span><br><span class="line"></span><br><span class="line">解决方法1</span><br><span class="line">使用sudo获取管理员权限，运行docker命令</span><br><span class="line"></span><br><span class="line">解决方法2</span><br><span class="line">docker守护进程启动的时候，会默认赋予名字为docker的用户组读写Unix socket的权限，因此只要创建docker用户组，并将当前用户加入到docker用户组中，那么当前用户就有权限访问Unix socket了，进而也就可以执行docker相关命令</span><br><span class="line"></span><br><span class="line">sudo groupadd docker             <span class="comment"># 添加docker用户组</span></span><br><span class="line">sudo gpasswd -a <span class="variable">$USER</span> docker     <span class="comment"># 将登陆用户加入到docker用户组中</span></span><br><span class="line">newgrp docker                    <span class="comment"># 更新用户组</span></span><br><span class="line">docker ps                        <span class="comment"># 测试docker命令是否可以使用sudo正常使用</span></span><br></pre></td></tr></table></figure></div>
<h3 id="2-3-6-Docker-镜像使用"><a href="#2-3-6-Docker-镜像使用" class="headerlink" title="2.3.6 Docker 镜像使用"></a>2.3.6 Docker 镜像使用</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">当运行容器时，使用的镜像如果在本地中不存在，docker 就会自动从 docker 镜像仓库中下载，默认是从 Docker Hub 公共镜像源下载。</span><br><span class="line"></span><br><span class="line">1. 管理和使用本地 Docker 主机镜像</span><br><span class="line">2. 创建镜像</span><br><span class="line"></span><br><span class="line">[列出镜像列表]</span><br><span class="line">可以使用 docker images 来列出本地主机上的镜像</span><br><span class="line">runoob@runoob:~$ docker images           </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu              14.04               90d5884b1ee0        5 days ago          188 MB</span><br><span class="line">php                 5.6                 f40e9e0f10c8        9 days ago          444.8 MB</span><br><span class="line">nginx               latest              6f8d099c3adc        12 days ago         182.7 MB</span><br><span class="line">mysql               5.6                 f2e8d6c772c0        3 weeks ago         324.6 MB</span><br><span class="line">httpd               latest              02ef73cf1bc0        3 weeks ago         194.4 MB</span><br><span class="line">ubuntu              15.10               4e3b13c8a266        4 weeks ago         136.3 MB</span><br><span class="line">hello-world         latest              690ed74de00f        6 months ago        960 B</span><br><span class="line">training/webapp     latest              6fae60ef3446        11 months ago       348.8 MB</span><br><span class="line"></span><br><span class="line">各个选项说明:</span><br><span class="line">  REPOSITORY：表示镜像的仓库源</span><br><span class="line">  TAG：镜像的标签</span><br><span class="line">  IMAGE ID：镜像ID</span><br><span class="line">  CREATED：镜像创建时间</span><br><span class="line">  SIZE：镜像大小</span><br><span class="line"></span><br><span class="line">同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如 ubuntu 仓库源里，有 15.10、14.04 等多个不同的版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。</span><br><span class="line"></span><br><span class="line">所以，我们如果要使用版本为15.10的ubuntu系统镜像来运行容器时，命令如下：</span><br><span class="line">runoob@runoob:~$ docker run -t -i ubuntu:15.10 /bin/bash </span><br><span class="line">root@d77ccb2e5cca:/<span class="comment">#</span></span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">  -i: 交互式操作。</span><br><span class="line">  -t: 终端。</span><br><span class="line">  ubuntu:15.10: 这是指用 ubuntu 15.10 版本镜像为基础来启动容器。</span><br><span class="line">  /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash</span><br><span class="line">  </span><br><span class="line">如果要使用版本为 14.04 的 ubuntu 系统镜像来运行容器时，命令如下：</span><br><span class="line">runoob@runoob:~$ docker run -t -i ubuntu:14.04 /bin/bash </span><br><span class="line">root@39e968165990:/<span class="comment"># </span></span><br><span class="line"></span><br><span class="line">如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[获取一个新的镜像]</span><br><span class="line">当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用 docker pull 命令来下载它。</span><br><span class="line"></span><br><span class="line">Crunoob@runoob:~$ docker pull ubuntu:13.10</span><br><span class="line">13.10: Pulling from library/ubuntu</span><br><span class="line">6599cadaf950: Pull complete </span><br><span class="line">23eda618d451: Pull complete </span><br><span class="line">f0be3084efe9: Pull complete </span><br><span class="line">52de432f084b: Pull complete </span><br><span class="line">a3ed95caeb02: Pull complete </span><br><span class="line">Digest: sha256:15b79a6654811c8d992ebacdfbd5152fcf3d165e374e264076aa435214a947a3</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> ubuntu:13.10</span><br><span class="line">下载完成后，可以直接使用这个镜像来运行容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[查找镜像]</span><br><span class="line">我们可以从 Docker Hub 网站来搜索镜像，Docker Hub 网址为： https://hub.docker.com/</span><br><span class="line">我们也可以使用 docker search 命令来搜索镜像。比如我们需要一个 httpd 的镜像来作为我们的 web 服务。我们可以通过 docker search 命令搜索 httpd 来寻找适合我们的镜像。</span><br><span class="line">runoob@runoob:~$  docker search httpd</span><br><span class="line"></span><br><span class="line">NAME: 镜像仓库源的名称</span><br><span class="line">DESCRIPTION: 镜像的描述</span><br><span class="line">OFFICIAL: 是否 docker 官方发布</span><br><span class="line">stars: 类似 Github 里面的 star，表示点赞、喜欢的意思。</span><br><span class="line">AUTOMATED: 自动构建</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[拉取镜像]</span><br><span class="line">$ docker pull httpd</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/httpd</span><br><span class="line">8b87079b7a06: Pulling fs layer </span><br><span class="line">a3ed95caeb02: Download complete </span><br><span class="line">0d62ec9c6a76: Download complete </span><br><span class="line">a329d50397b9: Download complete </span><br><span class="line">ea7c1f032b5c: Waiting </span><br><span class="line">be44112b72c7: Waiting</span><br><span class="line"></span><br><span class="line">$ docker run httpd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[删除镜像]</span><br><span class="line">镜像删除使用 docker rmi 命令，比如我们删除 hello-world 镜像</span><br><span class="line">$ docker rmi hello-world</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[创建镜像]</span><br><span class="line">当我们从 docker 镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改</span><br><span class="line">1、从已经创建的容器中更新镜像，并且提交这个镜像</span><br><span class="line">2、使用 Dockerfile 指令来创建一个新的镜像</span><br><span class="line"></span><br><span class="line">更新镜像</span><br><span class="line">更新镜像之前，我们需要使用镜像来创建一个容器。</span><br><span class="line">$ docker run -t -i ubuntu:15.10 /bin/bash</span><br><span class="line">root@e218edb10161:/<span class="comment"># </span></span><br><span class="line"></span><br><span class="line">在运行的容器内使用 apt-get update 命令进行更新</span><br><span class="line">在完成操作之后，输入 <span class="built_in">exit</span> 命令来退出这个容器</span><br><span class="line"></span><br><span class="line">此时 ID 为 e218edb10161 的容器，是按我们的需求更改的容器。我们可以通过命令 docker commit 来提交容器副本</span><br><span class="line">runoob@runoob:~$ docker commit -m=<span class="string">"has update"</span> -a=<span class="string">"runoob"</span> e218edb10161 runoob/ubuntu:v2</span><br><span class="line">sha256:70bf1840fd7c0d2d8ef0a42a817eb29f854c1af8f7c59fc03ac7bdee9545aff8</span><br><span class="line"></span><br><span class="line">各个参数说明：</span><br><span class="line">  -m: 提交的描述信息</span><br><span class="line">  -a: 指定镜像作者</span><br><span class="line">  e218edb10161：容器 ID</span><br><span class="line">  runoob/ubuntu:v2: 指定要创建的目标镜像名</span><br><span class="line"></span><br><span class="line">我们可以使用 docker images 命令来查看我们的新镜像 runoob/ubuntu:v2：</span><br><span class="line">runoob@runoob:~$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">runoob/ubuntu       v2                  70bf1840fd7c        15 seconds ago      158.5 MB</span><br><span class="line">ubuntu              14.04               90d5884b1ee0        5 days ago          188 MB</span><br><span class="line">php                 5.6                 f40e9e0f10c8        9 days ago          444.8 MB</span><br><span class="line">nginx               latest              6f8d099c3adc        12 days ago         182.7 MB</span><br><span class="line">mysql               5.6                 f2e8d6c772c0        3 weeks ago         324.6 MB</span><br><span class="line">httpd               latest              02ef73cf1bc0        3 weeks ago         194.4 MB</span><br><span class="line">ubuntu              15.10               4e3b13c8a266        4 weeks ago         136.3 MB</span><br><span class="line">hello-world         latest              690ed74de00f        6 months ago        960 B</span><br><span class="line">training/webapp     latest              6fae60ef3446        12 months ago       348.8 MB</span><br><span class="line"></span><br><span class="line">使用我们的新镜像 runoob/ubuntu 来启动一个容器</span><br><span class="line">$ docker run -t -i runoob/ubuntu:v2 /bin/bash                            </span><br><span class="line">root@1a9fbdeb5da3:/<span class="comment">#</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[构建镜像]</span><br><span class="line">我们使用命令 docker build ， 从零开始来创建一个新的镜像。为此，我们需要创建一个 Dockerfile 文件，其中包含一组指令来告诉 Docker 如何构建我们的镜像。</span><br><span class="line">runoob@runoob:~$ cat Dockerfile </span><br><span class="line">FROM    centos:6.7</span><br><span class="line">MAINTAINER      Fisher <span class="string">"fisher@sudops.com"</span></span><br><span class="line"></span><br><span class="line">RUN     /bin/<span class="built_in">echo</span> <span class="string">'root:123456'</span> |chpasswd</span><br><span class="line">RUN     useradd runoob</span><br><span class="line">RUN     /bin/<span class="built_in">echo</span> <span class="string">'runoob:123456'</span> |chpasswd</span><br><span class="line">RUN     /bin/<span class="built_in">echo</span> -e <span class="string">"LANG=\"en_US.UTF-8\""</span> &gt;/etc/default/<span class="built_in">local</span></span><br><span class="line">EXPOSE  22</span><br><span class="line">EXPOSE  80</span><br><span class="line">CMD     /usr/sbin/sshd -D</span><br><span class="line"></span><br><span class="line">每一个指令都会在镜像上创建一个新的层，每一个指令的前缀都必须是大写的。</span><br><span class="line">第一条FROM，指定使用哪个镜像源</span><br><span class="line">RUN 指令告诉docker 在镜像内执行命令，安装了什么。。。</span><br><span class="line">然后，我们使用 Dockerfile 文件，通过 docker build 命令来构建一个镜像</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker build -t runoob/centos:6.7 .</span><br><span class="line">Sending build context to Docker daemon 17.92 kB</span><br><span class="line">Step 1 : FROM centos:6.7</span><br><span class="line"> ---&amp;gt; d95b5ca17cc3</span><br><span class="line">Step 2 : MAINTAINER Fisher <span class="string">"fisher@sudops.com"</span></span><br><span class="line"> ---&amp;gt; Using cache</span><br><span class="line"> ---&amp;gt; 0c92299c6f03</span><br><span class="line">Step 3 : RUN /bin/<span class="built_in">echo</span> <span class="string">'root:123456'</span> |chpasswd</span><br><span class="line"> ---&amp;gt; Using cache</span><br><span class="line"> ---&amp;gt; 0397ce2fbd0a</span><br><span class="line">Step 4 : RUN useradd runoob</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">-t ：指定要创建的目标镜像名</span><br><span class="line">. ：Dockerfile 文件所在目录，可以指定Dockerfile 的绝对路径</span><br><span class="line"></span><br><span class="line">使用docker images 查看创建的镜像已经在列表中存在,镜像ID为860c279d2fec</span><br><span class="line">runoob@runoob:~$ docker images </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line">runoob/centos       6.7                 860c279d2fec        About a minute ago   190.6 MB</span><br><span class="line">runoob/ubuntu       v2                  70bf1840fd7c        17 hours ago         158.5 MB</span><br><span class="line">ubuntu              14.04               90d5884b1ee0        6 days ago           188 MB</span><br><span class="line">php                 5.6                 f40e9e0f10c8        10 days ago          444.8 MB</span><br><span class="line">nginx               latest              6f8d099c3adc        12 days ago          182.7 MB</span><br><span class="line">mysql               5.6                 f2e8d6c772c0        3 weeks ago          324.6 MB</span><br><span class="line">httpd               latest              02ef73cf1bc0        3 weeks ago          194.4 MB</span><br><span class="line">ubuntu              15.10               4e3b13c8a266        5 weeks ago          136.3 MB</span><br><span class="line">hello-world         latest              690ed74de00f        6 months ago         960 B</span><br><span class="line">centos              6.7                 d95b5ca17cc3        6 months ago         190.6 MB</span><br><span class="line">training/webapp     latest              6fae60ef3446        12 months ago        348.8 MB</span><br><span class="line"></span><br><span class="line">我们可以使用新的镜像来创建容器</span><br><span class="line">runoob@runoob:~$ docker run -t -i runoob/centos:6.7  /bin/bash</span><br><span class="line">[root@41c28d18b5fb /]<span class="comment"># id runoob</span></span><br><span class="line">uid=500(runoob) gid=500(runoob) groups=500(runoob)</span><br><span class="line"></span><br><span class="line">从上面看到新镜像已经包含我们创建的用户 runoob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[设置镜像标签]</span><br><span class="line">我们可以使用 docker tag 命令，为镜像添加一个新的标签</span><br><span class="line">$ docker tag 860c279d2fec runoob/centos:dev</span><br><span class="line"></span><br><span class="line">docker tag 镜像ID，这里是 860c279d2fec ,用户名称、镜像源名(repository name)和新的标签名(tag)。</span><br><span class="line">使用 docker images 命令可以看到，ID为860c279d2fec的镜像多一个标签</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">runoob/centos       6.7                 860c279d2fec        5 hours ago         190.6 MB</span><br><span class="line">runoob/centos       dev                 860c279d2fec        5 hours ago         190.6 MB</span><br><span class="line">runoob/ubuntu       v2                  70bf1840fd7c        22 hours ago        158.5 MB</span><br><span class="line">ubuntu              14.04               90d5884b1ee0        6 days ago          188 MB</span><br><span class="line">php                 5.6                 f40e9e0f10c8        10 days ago         444.8 MB</span><br><span class="line">nginx               latest              6f8d099c3adc        13 days ago         182.7 MB</span><br><span class="line">mysql               5.6                 f2e8d6c772c0        3 weeks ago         324.6 MB</span><br><span class="line">httpd               latest              02ef73cf1bc0        3 weeks ago         194.4 MB</span><br><span class="line">ubuntu              15.10               4e3b13c8a266        5 weeks ago         136.3 MB</span><br><span class="line">hello-world         latest              690ed74de00f        6 months ago        960 B</span><br><span class="line">centos              6.7                 d95b5ca17cc3        6 months ago        190.6 MB</span><br><span class="line">training/webapp     latest              6fae60ef3446        12 months ago       348.8 MB</span><br></pre></td></tr></table></figure></div>
<h3 id="2-3-7-Docker-容器连接"><a href="#2-3-7-Docker-容器连接" class="headerlink" title="2.3.7 Docker 容器连接"></a>2.3.7 Docker 容器连接</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射</span><br><span class="line">下面实现通过端口连接到一个 docker 容器</span><br><span class="line"></span><br><span class="line">[网络端口映射]</span><br><span class="line">创建了一个 python 应用的容器</span><br><span class="line">$ docker run -d -P training/webapp python app.py</span><br><span class="line">fce072cc88cee71b1cdceb57c2821d054a4a59f67da6b416fceb5593f059fc6d</span><br><span class="line"></span><br><span class="line">可以指定容器绑定的网络地址，比如绑定 127.0.0.1</span><br><span class="line">我们使用 -P 参数创建一个容器，使用 docker ps 可以看到容器端口 5000 绑定主机端口 32768</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID    IMAGE               COMMAND            ...           PORTS                     NAMES</span><br><span class="line">fce072cc88ce    training/webapp     <span class="string">"python app.py"</span>    ...     0.0.0.0:32768-&gt;5000/tcp   grave_hopper</span><br><span class="line"></span><br><span class="line">我们也可以使用 -p 标识来指定容器端口绑定到主机端口</span><br><span class="line"></span><br><span class="line">两种方式的区别是:</span><br><span class="line">  -P :是容器内部端口随机映射到主机的高端口</span><br><span class="line">  -p : 是容器内部端口绑定到指定的主机端口</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker run -d -p 5000:5000 training/webapp python app.py</span><br><span class="line">33e4523d30aaf0258915c368e66e03b49535de0ef20317d3f639d40222ba6bc0</span><br><span class="line"></span><br><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND           ...           PORTS                     NAMES</span><br><span class="line">33e4523d30aa        training/webapp     <span class="string">"python app.py"</span>   ...   0.0.0.0:5000-&gt;5000/tcp    berserk_bartik</span><br><span class="line">fce072cc88ce        training/webapp     <span class="string">"python app.py"</span>   ...   0.0.0.0:32768-&gt;5000/tcp   grave_hopper</span><br><span class="line"></span><br><span class="line">另外，可以指定容器绑定的网络地址，比如绑定 127.0.0.1</span><br><span class="line">runoob@runoob:~$ docker run -d -p 127.0.0.1:5001:5000 training/webapp python app.py</span><br><span class="line">95c6ceef88ca3e71eaf303c2833fd6701d8d1b2572b5613b5a932dfdfe8a857c</span><br><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND           ...     PORTS                                NAMES</span><br><span class="line">95c6ceef88ca        training/webapp     <span class="string">"python app.py"</span>   ...  5000/tcp, 127.0.0.1:5001-&gt;5000/tcp   adoring_stonebraker</span><br><span class="line">33e4523d30aa        training/webapp     <span class="string">"python app.py"</span>   ...  0.0.0.0:5000-&gt;5000/tcp               berserk_bartik</span><br><span class="line">fce072cc88ce        training/webapp     <span class="string">"python app.py"</span>   ...    0.0.0.0:32768-&gt;5000/tcp              grave_hopper</span><br><span class="line"></span><br><span class="line">这样就可以通过访问 127.0.0.1:5001 来访问容器的 5000 端口</span><br><span class="line"></span><br><span class="line">上面的例子中，默认都是绑定 tcp 端口，如果要绑定 UDP 端口，可以在端口后面加上 /udp</span><br><span class="line">runoob@runoob:~$ docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py</span><br><span class="line">6779686f06f6204579c1d655dd8b2b31e8e809b245a97b2d3a8e35abe9dcd22a</span><br><span class="line">runoob@runoob:~$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND           ...   PORTS                                NAMES</span><br><span class="line">6779686f06f6        training/webapp     <span class="string">"python app.py"</span>   ...   5000/tcp, 127.0.0.1:5000-&gt;5000/udp   drunk_visvesvaraya</span><br><span class="line">95c6ceef88ca        training/webapp     <span class="string">"python app.py"</span>   ...    5000/tcp, 127.0.0.1:5001-&gt;5000/tcp   adoring_stonebraker</span><br><span class="line">33e4523d30aa        training/webapp     <span class="string">"python app.py"</span>   ...     0.0.0.0:5000-&gt;5000/tcp               berserk_bartik</span><br><span class="line">fce072cc88ce        training/webapp     <span class="string">"python app.py"</span>   ...    0.0.0.0:32768-&gt;5000/tcp              grave_hopper</span><br><span class="line"></span><br><span class="line">docker port 命令可以让我们快捷地查看端口的绑定情况</span><br><span class="line">runoob@runoob:~$ docker port adoring_stonebraker 5000</span><br><span class="line">127.0.0.1:5001</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Docker 容器互联]</span><br><span class="line">端口映射并不是唯一把 docker 连接到另一个容器的方法。</span><br><span class="line">docker 有一个连接系统允许将多个容器连接在一起，共享连接信息。</span><br><span class="line">docker 连接会创建一个父子关系，其中父容器可以看到子容器的信息</span><br><span class="line"></span><br><span class="line">a) 容器命名</span><br><span class="line"></span><br><span class="line">当我们创建一个容器的时候，docker 会自动对它进行命名。另外，我们也可以使用 --name 标识来命名容器，例如：</span><br><span class="line">runoob@runoob:~$  docker run -d -P --name runoob training/webapp python app.py</span><br><span class="line">43780a6eabaaf14e590b6e849235c75f3012995403f97749775e38436db9a441</span><br><span class="line"></span><br><span class="line">我们可以使用 docker ps 命令来查看容器名称</span><br><span class="line">runoob@runoob:~$ docker ps -l</span><br><span class="line">CONTAINER ID     IMAGE            COMMAND           ...    PORTS                     NAMES</span><br><span class="line">43780a6eabaa     training/webapp   <span class="string">"python app.py"</span>  ...     0.0.0.0:32769-&gt;5000/tcp   runoob</span><br><span class="line"></span><br><span class="line">b) 新建网络</span><br><span class="line"></span><br><span class="line">先创建一个新的 Docker 网络</span><br><span class="line">$ docker network create -d bridge <span class="built_in">test</span>-net</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">-d：参数指定 Docker 网络类型，有 bridge、overlay</span><br><span class="line">其中 overlay 网络类型用于 Swarm mode，在本小节中你可以忽略它</span><br><span class="line"></span><br><span class="line">c) 连接容器</span><br><span class="line">运行一个容器并连接到新建的 <span class="built_in">test</span>-net 网络</span><br><span class="line">$ docker run -itd --name test1 --network <span class="built_in">test</span>-net ubuntu /bin/bash</span><br><span class="line"></span><br><span class="line">打开新的终端，再运行一个容器并加入到 <span class="built_in">test</span>-net 网络</span><br><span class="line">$ docker run -itd --name test2 --network <span class="built_in">test</span>-net ubuntu /bin/bash</span><br><span class="line"></span><br><span class="line">下面通过 ping 来证明 test1 容器和 test2 容器建立了互联关系。</span><br><span class="line">如果 test1、test2 容器内中无 ping 命令，则在容器内执行以下命令安装 ping（即学即用：可以在一个容器里安装好，提交容器到镜像，在以新的镜像重新运行以上俩个容器）</span><br><span class="line">apt-get update</span><br><span class="line">apt install iputils-ping</span><br><span class="line"></span><br><span class="line">在 test1 容器输入以下命令：</span><br><span class="line">$ docker <span class="built_in">exec</span> -it test1 /bin/bash</span><br><span class="line"><span class="comment"># ping test2</span></span><br><span class="line"></span><br><span class="line">在 test2 容器也会成功连接到 test1</span><br><span class="line">$ docker <span class="built_in">exec</span> -it test2 /bin/bash</span><br><span class="line"><span class="comment"># ping test1</span></span><br><span class="line"></span><br><span class="line">如果你有多个容器之间需要互相连接，推荐使用 Docker Compose</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[配置 DNS]</span><br><span class="line">可以在宿主机的 /etc/docker/daemon.json 文件中增加以下内容来设置全部容器的 DNS：</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"dns"</span> : [</span><br><span class="line">    <span class="string">"114.114.114.114"</span>,</span><br><span class="line">    <span class="string">"8.8.8.8"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">设置后，启动容器的 DNS 会自动配置为 114.114.114.114 和 8.8.8.8。</span><br><span class="line">配置完，需要重启 docker 才能生效。</span><br><span class="line">查看容器的 DNS 是否生效可以使用以下命令，它会输出容器的 DNS 信息：</span><br><span class="line">$ docker run -it --rm ubuntu  cat etc/resolv.conf</span><br><span class="line"></span><br><span class="line">手动指定容器的配置</span><br><span class="line">如果只想在指定的容器设置 DNS，则可以使用以下命令:</span><br><span class="line">$ docker run -it --rm host_ubuntu  --dns=114.114.114.114 --dns-search=test.com ubuntu</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">  -h HOSTNAME 或者 --hostname=HOSTNAME： 设定容器的主机名，它会被写到容器内的 /etc/hostname 和 /etc/hosts</span><br><span class="line">  --dns=IP_ADDRESS： 添加 DNS 服务器到容器的 /etc/resolv.conf 中，让容器用这个服务器来解析所有不在 /etc/hosts 中的主机名</span><br><span class="line">  --dns-search=DOMAIN： 设定容器的搜索域，当设定搜索域为 .example.com 时，在搜索一个名为 host 的主机时，DNS 不仅搜索 host，还会搜索 host.example.com</span><br><span class="line"></span><br><span class="line">如果在容器启动时没有指定 --dns 和 --dns-search，Docker 会默认用宿主主机上的 /etc/resolv.conf 来配置容器的 DNS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[解决windows系统无法对docker容器进行端口映射的问题]</span><br><span class="line">1、问题：</span><br><span class="line">在Windows家庭版下安装了docker，并尝试在其中运行jupyter notebook等服务，但映射完毕之后，在主机的浏览器中，打开localhost:port无法访问对应的服务。</span><br><span class="line"></span><br><span class="line">2、问题出现的原因：</span><br><span class="line">The reason you’re having this, is because on Linux, the docker daemon (and your containers) run on the Linux machine itself, so “localhost” is also the host that the container is running on, and the ports are mapped to.</span><br><span class="line"></span><br><span class="line">On Windows (and OS X), the docker daemon, and your containers cannot run natively, so only the docker client is running on your Windows machine, but the daemon (and your containers) run <span class="keyword">in</span> a VirtualBox Virtual Machine, that runs Linux.</span><br><span class="line"></span><br><span class="line">因为docker是运行在Linux上的，在Windows中运行docker，实际上还是在Windows下先安装了一个Linux环境，然后在这个系统中运行的docker。也就是说，服务中使用的localhost指的是这个Linux环境的地址，而不是我们的宿主环境Windows。</span><br><span class="line"></span><br><span class="line">3、解决方法：</span><br><span class="line">通过命令</span><br><span class="line">docker-machine ip default   <span class="comment"># 其中，default 是docker-machine的name，可以通过docker-machine -ls 查看</span></span><br><span class="line"></span><br><span class="line">找到这个Linux的ip地址，一般情况下这个地址是192.168.99.100，然后在Windows的浏览器中，输入这个地址，加上服务的端口即可启用了。</span><br><span class="line"></span><br><span class="line">比如，首先运行一个docker 容器：</span><br><span class="line">docker run -it -p 8888:8888 conda:v1</span><br><span class="line"></span><br><span class="line">其中，conda:v1是我的容器名称。然后在容器中开启jupyter notebook 服务：</span><br><span class="line">jupyter notebook --no-browser --port=8888 --ip=172.17.0.2 --allow-root</span><br><span class="line"></span><br><span class="line">其中的ip参数为我的容器的ip地址，可以通过如下命令获得：</span><br><span class="line">docker inspect container_id</span><br><span class="line"></span><br><span class="line">最后在windows浏览器中测试结果：</span><br><span class="line">http://192.168.99.100:8888</span><br></pre></td></tr></table></figure></div>

<h3 id="2-3-8-Docker-仓库管理"><a href="#2-3-8-Docker-仓库管理" class="headerlink" title="2.3.8 Docker 仓库管理"></a>2.3.8 Docker 仓库管理</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">仓库（Repository）是集中存放镜像的地方。以下介绍一下 Docker Hub。当然不止 docker hub，只是远程的服务商不一样，操作都是一样的</span><br><span class="line">https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">Docker Hub</span><br><span class="line">目前 Docker 官方维护了一个公共仓库 Docker Hub https://hub.docker.com/</span><br><span class="line">大部分需求都可以通过在 Docker Hub 中直接下载镜像来实现</span><br><span class="line"></span><br><span class="line">注册</span><br><span class="line">在 https://hub.docker.com 免费注册一个 Docker 账号</span><br><span class="line"></span><br><span class="line">登录和退出</span><br><span class="line">登录需要输入用户名和密码，登录成功后，我们就可以从 docker hub 上拉取自己账号下的全部镜像</span><br><span class="line">$ docker login</span><br><span class="line"></span><br><span class="line">退出</span><br><span class="line">退出 docker hub 可以使用以下命令：</span><br><span class="line">$ docker <span class="built_in">logout</span></span><br><span class="line"></span><br><span class="line">拉取镜像</span><br><span class="line">你可以通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull 命令来将它下载到本地。</span><br><span class="line">以 ubuntu 为关键词进行搜索：</span><br><span class="line">$ docker search ubuntu</span><br><span class="line"></span><br><span class="line">使用 docker pull 将官方 ubuntu 镜像下载到本地：</span><br><span class="line">$ docker pull ubuntu </span><br><span class="line"></span><br><span class="line">推送镜像</span><br><span class="line">用户登录后，可以通过 docker push 命令将自己的镜像推送到 Docker Hub</span><br><span class="line">以下命令中的 username 请替换为你的 Docker 账号用户名</span><br><span class="line">$ docker tag ubuntu:18.04 username/ubuntu:18.04</span><br><span class="line">$ docker image ls</span><br><span class="line"></span><br><span class="line">REPOSITORY      TAG        IMAGE ID            CREATED           ...  </span><br><span class="line">ubuntu          18.04      275d79972a86        6 days ago        ...  </span><br><span class="line">username/ubuntu 18.04      275d79972a86        6 days ago        ...  </span><br><span class="line">$ docker push username/ubuntu:18.04</span><br><span class="line">$ docker search username/ubuntu</span><br><span class="line"></span><br><span class="line">NAME             DESCRIPTION       STARS         OFFICIAL    AUTOMATED</span><br><span class="line">username/ubuntu</span><br></pre></td></tr></table></figure></div>

<h3 id="2-3-9-Docker-Dockerfile"><a href="#2-3-9-Docker-Dockerfile" class="headerlink" title="2.3.9 Docker Dockerfile"></a>2.3.9 Docker Dockerfile</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">什么是 Dockerfile？</span><br><span class="line">Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[使用 Dockerfile 定制镜像]</span><br><span class="line">这里仅讲解如何运行 Dockerfile 文件来定制一个镜像，具体 Dockerfile 文件内指令详解，将在下一节中介绍，这里你只要知道构建的流程即可</span><br><span class="line"></span><br><span class="line">1. 下面以定制一个 nginx 镜像（构建好的镜像内会有一个 /usr/share/nginx/html/index.html 文件）</span><br><span class="line">在一个空目录下，新建一个名为 Dockerfile 文件，并在文件内添加以下内容：</span><br><span class="line"></span><br><span class="line">FROM nginx</span><br><span class="line">RUN <span class="built_in">echo</span> <span class="string">'这是一个本地构建的nginx镜像'</span> &gt; /usr/share/nginx/html/index.html</span><br><span class="line"></span><br><span class="line">2. FROM 和 RUN 指令的作用</span><br><span class="line">FROM：定制的镜像都是基于 FROM 的镜像，这里的 nginx 就是定制需要的基础镜像。后续的操作都是基于 nginx。</span><br><span class="line">RUN：用于执行后面跟着的命令行命令。有以下俩种格式：</span><br><span class="line">shell 格式：</span><br><span class="line">  RUN &lt;命令行命令&gt;</span><br><span class="line">  <span class="comment"># &lt;命令行命令&gt; 等同于，在终端操作的 shell 命令</span></span><br><span class="line"><span class="built_in">exec</span> 格式：</span><br><span class="line">  RUN [<span class="string">"可执行文件"</span>, <span class="string">"参数1"</span>, <span class="string">"参数2"</span>]</span><br><span class="line">  <span class="comment"># 例如：</span></span><br><span class="line">  <span class="comment"># RUN ["./test.php", "dev", "offline"] 等价于 RUN ./test.php dev offline</span></span><br><span class="line">注意：Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大。例如：</span><br><span class="line">---</span><br><span class="line">FROM centos</span><br><span class="line">RUN yum install wget</span><br><span class="line">RUN wget -O redis.tar.gz <span class="string">"http://download.redis.io/releases/redis-5.0.3.tar.gz"</span></span><br><span class="line">RUN tar -xvf redis.tar.gz</span><br><span class="line">以上执行会创建 3 层镜像。可简化为以下格式：</span><br><span class="line">FROM centos</span><br><span class="line">RUN yum install wget \</span><br><span class="line">    &amp;&amp; wget -O redis.tar.gz <span class="string">"http://download.redis.io/releases/redis-5.0.3.tar.gz"</span> \</span><br><span class="line">    &amp;&amp; tar -xvf redis.tar.gz</span><br><span class="line">---</span><br><span class="line">如上，以 &amp;&amp; 符号连接命令，这样执行后，只会创建 1 层镜像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[开始构建镜像]</span><br><span class="line">在 Dockerfile 文件的存放目录下，执行构建动作。</span><br><span class="line">以下示例，通过目录下的 Dockerfile 构建一个 nginx:<span class="built_in">test</span>（镜像名称:镜像标签）。</span><br><span class="line">注：最后的 . 代表本次执行的上下文路径，下一节会介绍</span><br><span class="line">$ docker build -t nginx:<span class="built_in">test</span> .</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[上下文路径]</span><br><span class="line">上一节中，有提到指令最后一个 . 是上下文路径，那么什么是上下文路径呢？</span><br><span class="line">$ docker build -t nginx:<span class="built_in">test</span> .</span><br><span class="line"></span><br><span class="line">上下文路径，是指 docker 在构建镜像，有时候想要使用到本机的文件（比如复制），docker build 命令得知这个路径后，会将路径下的所有内容打包。</span><br><span class="line"></span><br><span class="line">解析：由于 docker 的运行模式是 C/S。我们本机是 C，docker 引擎是 S。实际的构建过程是在 docker 引擎下完成的，所以这个时候无法用到我们本机的文件。这就需要把我们本机的指定目录下的文件一起打包提供给 docker 引擎使用。</span><br><span class="line"></span><br><span class="line">如果未说明最后一个参数，那么默认上下文路径就是 Dockerfile 所在的位置。</span><br><span class="line"></span><br><span class="line">注意：上下文路径下不要放无用的文件，因为会一起打包发送给 docker 引擎，如果文件过多会造成过程缓慢。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[指令详解]</span><br><span class="line"></span><br><span class="line">01	COPY</span><br><span class="line">复制指令，从上下文目录中复制文件或者目录到容器里指定路径。</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;...  &lt;目标路径&gt;</span><br><span class="line">COPY [--chown=&lt;user&gt;:&lt;group&gt;] [<span class="string">"&lt;源路径1&gt;"</span>,...  <span class="string">"&lt;目标路径&gt;"</span>]</span><br><span class="line"></span><br><span class="line">[--chown=&lt;user&gt;:&lt;group&gt;]：可选参数，用户改变复制到容器内文件的拥有者和属组。</span><br><span class="line">&lt;源路径&gt;：源文件或者源目录，这里可以是通配符表达式，其通配符规则要满足 Go 的 filepath.Match 规则。例如：</span><br><span class="line"></span><br><span class="line">COPY hom* /mydir/</span><br><span class="line">COPY hom?.txt /mydir/</span><br><span class="line"></span><br><span class="line">&lt;目标路径&gt;：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">02	ADD</span><br><span class="line">ADD 指令和 COPY 的使用格式一致（同样需求下，官方推荐使用 COPY）。功能也类似，不同之处如下：</span><br><span class="line">  ADD 的优点：在执行 &lt;源文件&gt; 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 &lt;目标路径&gt;。</span><br><span class="line">  ADD 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。</span><br><span class="line"></span><br><span class="line">03	CMD</span><br><span class="line">类似于 RUN 指令，用于运行程序，但二者运行的时间点不同:</span><br><span class="line">  CMD 在docker run 时运行</span><br><span class="line">  RUN 是在 docker build</span><br><span class="line">作用：为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。CMD 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖</span><br><span class="line">注意：如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">CMD &lt;shell 命令&gt; </span><br><span class="line">CMD [<span class="string">"&lt;可执行文件或命令&gt;"</span>,<span class="string">"&lt;param1&gt;"</span>,<span class="string">"&lt;param2&gt;"</span>,...] </span><br><span class="line">CMD [<span class="string">"&lt;param1&gt;"</span>,<span class="string">"&lt;param2&gt;"</span>,...]  <span class="comment"># 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数</span></span><br><span class="line"></span><br><span class="line">推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh。</span><br><span class="line"></span><br><span class="line">04	ENTRYPOINT</span><br><span class="line">类似于 CMD 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序。</span><br><span class="line">但是, 如果运行 docker run 时使用了 --entrypoint 选项，此选项的参数可当作要运行的程序覆盖 ENTRYPOINT 指令指定的程序。</span><br><span class="line"></span><br><span class="line">优点：在执行 docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。</span><br><span class="line">注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">  ENTRYPOINT [<span class="string">"&lt;executeable&gt;"</span>,<span class="string">"&lt;param1&gt;"</span>,<span class="string">"&lt;param2&gt;"</span>,...]</span><br><span class="line">可以搭配 CMD 命令使用：一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参，以下示例会提到。</span><br><span class="line"></span><br><span class="line">示例：</span><br><span class="line">假设已通过 Dockerfile 构建了 nginx:<span class="built_in">test</span> 镜像：</span><br><span class="line">FROM nginx</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [<span class="string">"nginx"</span>, <span class="string">"-c"</span>] <span class="comment"># 定参</span></span><br><span class="line">CMD [<span class="string">"/etc/nginx/nginx.conf"</span>] <span class="comment"># 变参</span></span><br><span class="line"></span><br><span class="line">1、不传参运行</span><br><span class="line">$ docker run  nginx:<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">容器内会默认运行以下命令，启动主进程</span><br><span class="line">nginx -c /etc/nginx/nginx.conf</span><br><span class="line"></span><br><span class="line">2、传参运行</span><br><span class="line">$ docker run  nginx:<span class="built_in">test</span> -c /etc/nginx/new.conf</span><br><span class="line"></span><br><span class="line">容器内会默认运行以下命令，启动主进程(/etc/nginx/new.conf:假设容器内已有此文件)</span><br><span class="line">nginx -c /etc/nginx/new.conf</span><br><span class="line"></span><br><span class="line">05	ENV</span><br><span class="line">设置环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。</span><br><span class="line">格式：</span><br><span class="line">  ENV &lt;key&gt; &lt;value&gt;</span><br><span class="line">  ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...</span><br><span class="line">以下示例设置 NODE_VERSION = 7.2.0 ， 在后续的指令中可以通过 <span class="variable">$NODE_VERSION</span> 引用：</span><br><span class="line">ENV NODE_VERSION 7.2.0</span><br><span class="line"></span><br><span class="line">RUN curl -SLO <span class="string">"https://nodejs.org/dist/v<span class="variable">$NODE_VERSION</span>/node-v<span class="variable">$NODE_VERSION</span>-linux-x64.tar.xz"</span> \</span><br><span class="line">  &amp;&amp; curl -SLO <span class="string">"https://nodejs.org/dist/v<span class="variable">$NODE_VERSION</span>/SHASUMS256.txt.asc"</span></span><br><span class="line"></span><br><span class="line">06	ARG</span><br><span class="line">构建参数，与 ENV 作用一至。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，</span><br><span class="line">也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。</span><br><span class="line">构建命令 docker build 中可以用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">ARG &lt;参数名&gt;[=&lt;默认值&gt;]</span><br><span class="line"></span><br><span class="line">07	VOLUME</span><br><span class="line">定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷</span><br><span class="line"></span><br><span class="line">作用：</span><br><span class="line">  避免重要的数据，因容器重启而丢失，这是非常致命的。</span><br><span class="line">  避免容器不断变大</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">  VOLUME [<span class="string">"&lt;路径1&gt;"</span>, <span class="string">"&lt;路径2&gt;"</span>...]</span><br><span class="line">  VOLUME &lt;路径&gt;</span><br><span class="line">  </span><br><span class="line">在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点</span><br><span class="line"></span><br><span class="line">08	EXPOSE</span><br><span class="line">仅仅只是声明端口。</span><br><span class="line">作用：</span><br><span class="line">  帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。</span><br><span class="line">  在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。</span><br><span class="line">  </span><br><span class="line">格式：</span><br><span class="line">EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...]</span><br><span class="line"></span><br><span class="line">09	WORKDIR</span><br><span class="line">指定工作目录。用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在。（WORKDIR 指定的工作目录，必须是提前创建好的）。</span><br><span class="line">docker build 构建镜像过程中的，每一个 RUN 命令都是新建的一层。只有通过 WORKDIR 创建的目录才会一直存在。</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">WORKDIR &lt;工作目录路径&gt;</span><br><span class="line"></span><br><span class="line">10	USER</span><br><span class="line">用于指定执行后续命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已经存在）。</span><br><span class="line">格式：</span><br><span class="line">USER &lt;用户名&gt;[:&lt;用户组&gt;]</span><br><span class="line"></span><br><span class="line">11	HEALTHCHECK</span><br><span class="line">用于指定某个程序或者指令来监控 docker 容器服务的运行状态。</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令</span><br><span class="line">HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令</span><br><span class="line">HEALTHCHECK [选项] CMD &lt;命令&gt; : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。</span><br><span class="line"></span><br><span class="line">12	ONBUILD</span><br><span class="line">用于延迟构建命令的执行。简单的说，就是 Dockerfile 里用 ONBUILD 指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 <span class="built_in">test</span>-build）。当有新的 Dockerfile 使用了之前构建的镜像 FROM <span class="built_in">test</span>-build ，这是执行新镜像的 Dockerfile 构建时候，会执行 <span class="built_in">test</span>-build 的 Dockerfile 里的 ONBUILD 指定的命令。</span><br><span class="line"></span><br><span class="line">格式：</span><br><span class="line">ONBUILD &lt;其它指令&gt;</span><br></pre></td></tr></table></figure></div>

<h3 id="2-3-10-Docker-Compose"><a href="#2-3-10-Docker-Compose" class="headerlink" title="2.3.10 Docker Compose"></a>2.3.10 Docker Compose</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">Compose 简介</span><br><span class="line">Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。</span><br><span class="line"></span><br><span class="line">Compose 使用的三个步骤：</span><br><span class="line">  使用 Dockerfile 定义应用程序的环境。</span><br><span class="line">  使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。</span><br><span class="line">  最后，执行 docker-compose up 命令来启动并运行整个应用程序。</span><br><span class="line">  </span><br><span class="line">docker-compose.yml 的配置案例如下（配置参数参考下文）：</span><br><span class="line">实例</span><br><span class="line">---</span><br><span class="line"><span class="comment"># yaml 配置实例</span></span><br><span class="line">version: <span class="string">'3'</span></span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    build: .</span><br><span class="line">    ports:</span><br><span class="line">   - <span class="string">"5000:5000"</span></span><br><span class="line">    volumes:</span><br><span class="line">   - .:/code</span><br><span class="line">    - logvolume01:/var/<span class="built_in">log</span></span><br><span class="line">    links:</span><br><span class="line">   - redis</span><br><span class="line">  redis:</span><br><span class="line">    image: redis</span><br><span class="line">volumes:</span><br><span class="line">  logvolume01: &#123;&#125;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">[Compose 安装]</span><br><span class="line">Linux 上我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases</span><br><span class="line">运行以下命令以下载 Docker Compose 的当前稳定版本</span><br><span class="line">$ sudo curl -L <span class="string">"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-<span class="variable">$(uname -s)</span>-<span class="variable">$(uname -m)</span>"</span> -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line">要安装其他版本的 Compose，请替换 1.24.1</span><br><span class="line">将可执行权限应用于二进制文件：</span><br><span class="line">$ sudo chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line">创建软链：</span><br><span class="line">$ sudo ln -s /usr/<span class="built_in">local</span>/bin/docker-compose /usr/bin/docker-compose</span><br><span class="line"></span><br><span class="line">测试是否安装成功：</span><br><span class="line">$ docker-compose --version</span><br><span class="line">cker-compose version 1.24.1, build 4667896b</span><br><span class="line"></span><br><span class="line">注意： 对于 alpine，需要以下依赖包： py-pip，python-dev，libffi-dev，openssl-dev，gcc，libc-dev，和 make。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[docker compose 的使用]</span><br><span class="line">1、准备</span><br><span class="line">创建一个测试目录：</span><br><span class="line"></span><br><span class="line">$ mkdir composetest</span><br><span class="line">$ <span class="built_in">cd</span> composetest</span><br><span class="line"></span><br><span class="line">在测试目录中创建一个名为 app.py 的文件，并复制粘贴以下内容：</span><br><span class="line">composetest/app.py 文件代码</span><br><span class="line">---</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">import redis</span><br><span class="line">from flask import Flask</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">cache = redis.Redis(host=<span class="string">'redis'</span>, port=6379)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hit_count():</span><br><span class="line">    retries = 5</span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        try:</span><br><span class="line">            <span class="built_in">return</span> cache.incr(<span class="string">'hits'</span>)</span><br><span class="line">        except redis.exceptions.ConnectionError as exc:</span><br><span class="line">            <span class="keyword">if</span> retries == 0:</span><br><span class="line">                raise exc</span><br><span class="line">            retries -= 1</span><br><span class="line">            time.sleep(0.5)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.route(<span class="string">'/'</span>)</span><br><span class="line">def hello():</span><br><span class="line">    count = get_hit_count()</span><br><span class="line">    <span class="built_in">return</span> <span class="string">'Hello World! I have been seen &#123;&#125; times.\n'</span>.format(count)</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">在此示例中，redis 是应用程序网络上的 redis 容器的主机名，该主机使用的端口为 6379。</span><br><span class="line">在 composetest 目录中创建另一个名为 requirements.txt 的文件，内容如下：</span><br><span class="line">flask</span><br><span class="line">redis</span><br><span class="line"></span><br><span class="line">2、创建 Dockerfile 文件</span><br><span class="line">在 composetest 目录中，创建一个名为的文件 Dockerfile，内容如下：</span><br><span class="line">FROM python:3.7-alpine</span><br><span class="line">WORKDIR /code</span><br><span class="line">ENV FLASK_APP app.py</span><br><span class="line">ENV FLASK_RUN_HOST 0.0.0.0</span><br><span class="line">RUN apk add --no-cache gcc musl-dev linux-headers</span><br><span class="line">COPY requirements.txt requirements.txt</span><br><span class="line">RUN pip install -r requirements.txt</span><br><span class="line">COPY . .</span><br><span class="line">CMD [<span class="string">"flask"</span>, <span class="string">"run"</span>]</span><br><span class="line"></span><br><span class="line">Dockerfile 内容解释：</span><br><span class="line">  FROM python:3.7-alpine: 从 Python 3.7 映像开始构建镜像。</span><br><span class="line">  WORKDIR /code: 将工作目录设置为 /code。</span><br><span class="line">    ENV FLASK_APP app.py</span><br><span class="line">    ENV FLASK_RUN_HOST 0.0.0.0</span><br><span class="line">    设置 flask 命令使用的环境变量</span><br><span class="line">  RUN apk add --no-cache gcc musl-dev linux-headers: 安装 gcc，以便诸如 MarkupSafe 和 SQLAlchemy 之类的 Python 包可以编译加速。</span><br><span class="line">    COPY requirements.txt requirements.txt</span><br><span class="line">    RUN pip install -r requirements.txt</span><br><span class="line">    复制 requirements.txt 并安装 Python 依赖项。</span><br><span class="line">  COPY . .: 将 . 项目中的当前目录复制到 . 镜像中的工作目录。</span><br><span class="line">  CMD [<span class="string">"flask"</span>, <span class="string">"run"</span>]: 容器提供默认的执行命令为：flask run。</span><br><span class="line">  </span><br><span class="line">3、创建 docker-compose.yml</span><br><span class="line">在测试目录中创建一个名为 docker-compose.yml 的文件，然后粘贴以下内容：</span><br><span class="line"></span><br><span class="line">docker-compose.yml 配置文件</span><br><span class="line">---</span><br><span class="line"><span class="comment"># yaml 配置</span></span><br><span class="line">version: <span class="string">'3'</span></span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    build: .</span><br><span class="line">    ports:</span><br><span class="line">     - <span class="string">"5000:5000"</span></span><br><span class="line">  redis:</span><br><span class="line">    image: <span class="string">"redis:alpine"</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">该 Compose 文件定义了两个服务：web 和 redis</span><br><span class="line">  web：该 web 服务使用从 Dockerfile 当前目录中构建的镜像。然后，它将容器和主机绑定到暴露的端口 5000。此示例服务使用 Flask Web 服务器的默认端口 5000 。</span><br><span class="line">  redis：该 redis 服务使用 Docker Hub 的公共 Redis 映像。</span><br><span class="line"></span><br><span class="line">4、使用 Compose 命令构建和运行您的应用</span><br><span class="line">在测试目录中，执行以下命令来启动应用程序：</span><br><span class="line">  docker-compose up</span><br><span class="line">如果你想在后台执行该服务可以加上 -d 参数：</span><br><span class="line">  docker-compose up -d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[yml 配置指令参考]</span><br><span class="line"></span><br><span class="line">version</span><br><span class="line">指定本 yml 依从的 compose 哪个版本制定的</span><br><span class="line"></span><br><span class="line">build</span><br><span class="line">指定为构建镜像上下文路径：</span><br><span class="line"></span><br><span class="line">例如 webapp 服务，指定为从上下文路径 ./dir/Dockerfile 所构建的镜像：</span><br><span class="line">---</span><br><span class="line">version: <span class="string">"3.7"</span></span><br><span class="line">services:</span><br><span class="line">  webapp:</span><br><span class="line">    build: ./dir</span><br><span class="line">---</span><br><span class="line">或者，作为具有在上下文指定的路径的对象，以及可选的 Dockerfile 和 args：</span><br><span class="line">---</span><br><span class="line">version: <span class="string">"3.7"</span></span><br><span class="line">services:</span><br><span class="line">  webapp:</span><br><span class="line">    build:</span><br><span class="line">      context: ./dir</span><br><span class="line">      dockerfile: Dockerfile-alternate</span><br><span class="line">      args:</span><br><span class="line">        buildno: 1</span><br><span class="line">      labels:</span><br><span class="line">        - <span class="string">"com.example.description=Accounting webapp"</span></span><br><span class="line">        - <span class="string">"com.example.department=Finance"</span></span><br><span class="line">        - <span class="string">"com.example.label-with-empty-value"</span></span><br><span class="line">      target: prod</span><br><span class="line">---</span><br><span class="line">context：上下文路径。</span><br><span class="line">dockerfile：指定构建镜像的 Dockerfile 文件名。</span><br><span class="line">args：添加构建参数，这是只能在构建过程中访问的环境变量。</span><br><span class="line">labels：设置构建镜像的标签。</span><br><span class="line">target：多层构建，可以指定构建哪一层。</span><br><span class="line"></span><br><span class="line">cap_add，cap_drop</span><br><span class="line">添加或删除容器拥有的宿主机的内核功能。</span><br><span class="line">---</span><br><span class="line">cap_add:</span><br><span class="line">  - ALL <span class="comment"># 开启全部权限</span></span><br><span class="line"></span><br><span class="line">cap_drop:</span><br><span class="line">  - SYS_PTRACE <span class="comment"># 关闭 ptrace权限</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">cgroup_parent</span><br><span class="line">为容器指定父 cgroup 组，意味着将继承该组的资源限制。</span><br><span class="line">  cgroup_parent: m-executor-abcd</span><br><span class="line"></span><br><span class="line"><span class="built_in">command</span></span><br><span class="line">覆盖容器启动的默认命令</span><br><span class="line">  <span class="built_in">command</span>: [<span class="string">"bundle"</span>, <span class="string">"exec"</span>, <span class="string">"thin"</span>, <span class="string">"-p"</span>, <span class="string">"3000"</span>]</span><br><span class="line">  </span><br><span class="line">container_name</span><br><span class="line">指定自定义容器名称，而不是生成的默认名称。</span><br><span class="line">  container_name: my-web-container</span><br><span class="line">  </span><br><span class="line">depends_on</span><br><span class="line">设置依赖关系</span><br><span class="line">  docker-compose up ：以依赖性顺序启动服务。在以下示例中，先启动 db 和 redis ，才会启动 web。</span><br><span class="line">  docker-compose up SERVICE ：自动包含 SERVICE 的依赖项。在以下示例中，docker-compose up web 还将创建并启动 db 和 redis。</span><br><span class="line">  docker-compose stop ：按依赖关系顺序停止服务。在以下示例中，web 在 db 和 redis 之前停止。</span><br><span class="line">---</span><br><span class="line">version: <span class="string">"3.7"</span></span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    build: .</span><br><span class="line">    depends_on:</span><br><span class="line">      - db</span><br><span class="line">      - redis</span><br><span class="line">  redis:</span><br><span class="line">    image: redis</span><br><span class="line">  db:</span><br><span class="line">    image: postgres</span><br><span class="line">---</span><br><span class="line">注意：web 服务不会等待 redis db 完全启动 之后才启动。</span><br><span class="line"></span><br><span class="line">deploy</span><br><span class="line">指定与服务的部署和运行有关的配置。只在 swarm 模式下才会有用。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">version: <span class="string">"3.7"</span></span><br><span class="line">services:</span><br><span class="line">  redis:</span><br><span class="line">    image: redis:alpine</span><br><span class="line">    deploy:</span><br><span class="line">      mode：replicated</span><br><span class="line">      replicas: 6</span><br><span class="line">      endpoint_mode: dnsrr</span><br><span class="line">      labels: </span><br><span class="line">        description: <span class="string">"This redis service label"</span></span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: <span class="string">'0.50'</span></span><br><span class="line">          memory: 50M</span><br><span class="line">        reservations:</span><br><span class="line">          cpus: <span class="string">'0.25'</span></span><br><span class="line">          memory: 20M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">        delay: 5s</span><br><span class="line">        max_attempts: 3</span><br><span class="line">        window: 120s</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">可以选参数：</span><br><span class="line">endpoint_mode：访问集群服务的方式。</span><br><span class="line">---</span><br><span class="line">endpoint_mode: vip </span><br><span class="line"><span class="comment"># Docker 集群服务一个对外的虚拟 ip。所有的请求都会通过这个虚拟 ip 到达集群服务内部的机器。</span></span><br><span class="line">endpoint_mode: dnsrr</span><br><span class="line"><span class="comment"># DNS 轮询（DNSRR）。所有的请求会自动轮询获取到集群 ip 列表中的一个 ip 地址。</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">labels：在服务上设置标签。可以用容器上的 labels（跟 deploy 同级的配置） 覆盖 deploy 下的 labels。</span><br><span class="line">mode：指定服务提供的模式。</span><br><span class="line">  replicated：复制服务，复制指定服务到集群的机器上。</span><br><span class="line">  global：全局服务，服务将部署至集群的每个节点。</span><br><span class="line">  图解：下图中黄色的方块是 replicated 模式的运行情况，灰色方块是 global 模式的运行情况。</span><br></pre></td></tr></table></figure></div>
<p><img src="docker-composex.png" alt="img"></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">replicas：mode 为 replicated 时，需要使用此参数配置具体运行的节点数量。</span><br><span class="line">resources：配置服务器资源使用的限制，例如上例子，配置 redis 集群运行需要的 cpu 的百分比 和 内存的占用。避免占用资源过高出现异常。</span><br><span class="line">restart_policy：配置如何在退出容器时重新启动容器。</span><br><span class="line">  condition：可选 none，on-failure 或者 any（默认值：any）。</span><br><span class="line">  delay：设置多久之后重启（默认值：0）。</span><br><span class="line">  max_attempts：尝试重新启动容器的次数，超出次数，则不再尝试（默认值：一直重试）。</span><br><span class="line">  window：设置容器重启超时时间（默认值：0）。</span><br><span class="line">  </span><br><span class="line">rollback_config：配置在更新失败的情况下应如何回滚服务。</span><br><span class="line">  parallelism：一次要回滚的容器数。如果设置为0，则所有容器将同时回滚。</span><br><span class="line">  delay：每个容器组回滚之间等待的时间（默认为0s）。</span><br><span class="line">  failure_action：如果回滚失败，该怎么办。其中一个 <span class="built_in">continue</span> 或者 pause（默认pause）。</span><br><span class="line">  monitor：每个容器更新后，持续观察是否失败了的时间 (ns|us|ms|s|m|h)（默认为0s）。</span><br><span class="line">  max_failure_ratio：在回滚期间可以容忍的故障率（默认为0）。</span><br><span class="line">  order：回滚期间的操作顺序。其中一个 stop-first（串行回滚），或者 start-first（并行回滚）（默认 stop-first ）。</span><br><span class="line"></span><br><span class="line">update_config：配置应如何更新服务，对于配置滚动更新很有用。</span><br><span class="line">  parallelism：一次更新的容器数。</span><br><span class="line">  delay：在更新一组容器之间等待的时间。</span><br><span class="line">  failure_action：如果更新失败，该怎么办。其中一个 <span class="built_in">continue</span>，rollback 或者pause （默认：pause）。</span><br><span class="line">  monitor：每个容器更新后，持续观察是否失败了的时间 (ns|us|ms|s|m|h)（默认为0s）。</span><br><span class="line">  max_failure_ratio：在更新过程中可以容忍的故障率。</span><br><span class="line">  order：回滚期间的操作顺序。其中一个 stop-first（串行回滚），或者 start-first（并行回滚）（默认stop-first）。</span><br><span class="line">  </span><br><span class="line">注：仅支持 V3.4 及更高版本。</span><br><span class="line"></span><br><span class="line">devices</span><br><span class="line">指定设备映射列表。</span><br><span class="line">devices:</span><br><span class="line">---</span><br><span class="line">  - <span class="string">"/dev/ttyUSB0:/dev/ttyUSB0"</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">dns</span><br><span class="line">自定义 DNS 服务器，可以是单个值或列表的多个值。</span><br><span class="line">---</span><br><span class="line">dns: 8.8.8.8</span><br><span class="line"></span><br><span class="line">dns:</span><br><span class="line">  - 8.8.8.8</span><br><span class="line">  - 9.9.9.</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">dns_search</span><br><span class="line">自定义 DNS 搜索域。可以是单个值或列表。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">dns_search: example.com</span><br><span class="line"></span><br><span class="line">dns_search:</span><br><span class="line">  - dc1.example.com</span><br><span class="line">  - dc2.example.com</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">entrypoint</span><br><span class="line">覆盖容器默认的 entrypoint。</span><br><span class="line">  entrypoint: /code/entrypoint.sh</span><br><span class="line"></span><br><span class="line">也可以是以下格式：</span><br><span class="line">---</span><br><span class="line">entrypoint:</span><br><span class="line">    - php</span><br><span class="line">    - -d</span><br><span class="line">    - zend_extension=/usr/<span class="built_in">local</span>/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so</span><br><span class="line">    - -d</span><br><span class="line">    - memory_limit=-1</span><br><span class="line">    - vendor/bin/phpunit</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">env_file</span><br><span class="line">从文件添加环境变量。可以是单个值或列表的多个值。</span><br><span class="line">---</span><br><span class="line">env_file: .env</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">也可以是列表格式：</span><br><span class="line">---</span><br><span class="line">env_file:</span><br><span class="line">  - ./common.env</span><br><span class="line">  - ./apps/web.env</span><br><span class="line">  - /opt/secrets.env</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">environment</span><br><span class="line">添加环境变量。您可以使用数组或字典、任何布尔值，布尔值需要用引号引起来，以确保 YML 解析器不会将其转换为 True 或 False。</span><br><span class="line">---</span><br><span class="line">environment:</span><br><span class="line">  RACK_ENV: development</span><br><span class="line">  SHOW: <span class="string">'true'</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">expose</span><br><span class="line">暴露端口，但不映射到宿主机，只被连接的服务访问。</span><br><span class="line">仅可以指定内部端口为参数：</span><br><span class="line">---</span><br><span class="line">expose:</span><br><span class="line"> - <span class="string">"3000"</span></span><br><span class="line"> - <span class="string">"8000"</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">extra_hosts</span><br><span class="line">添加主机名映射。类似 docker client --add-host。</span><br><span class="line">extra_hosts:</span><br><span class="line"> - <span class="string">"somehost:162.242.195.82"</span></span><br><span class="line"> - <span class="string">"otherhost:50.31.209.229"</span></span><br><span class="line"> </span><br><span class="line">以上会在此服务的内部容器中 /etc/hosts 创建一个具有 ip 地址和主机名的映射关系：</span><br><span class="line"></span><br><span class="line">162.242.195.82  somehost</span><br><span class="line">50.31.209.229   otherhost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">healthcheck</span><br><span class="line">用于检测 docker 服务是否健康运行。</span><br><span class="line"></span><br><span class="line">healthcheck:</span><br><span class="line">  <span class="built_in">test</span>: [<span class="string">"CMD"</span>, <span class="string">"curl"</span>, <span class="string">"-f"</span>, <span class="string">"http://localhost"</span>] <span class="comment"># 设置检测程序</span></span><br><span class="line">  interval: 1m30s <span class="comment"># 设置检测间隔</span></span><br><span class="line">  timeout: 10s <span class="comment"># 设置检测超时时间</span></span><br><span class="line">  retries: 3 <span class="comment"># 设置重试次数</span></span><br><span class="line">  start_period: 40s <span class="comment"># 启动后，多少秒开始启动检测程序</span></span><br><span class="line">  </span><br><span class="line">image</span><br><span class="line">指定容器运行的镜像。以下格式都可以：</span><br><span class="line"></span><br><span class="line">image: redis</span><br><span class="line">image: ubuntu:14.04</span><br><span class="line">image: tutum/influxdb</span><br><span class="line">image: example-registry.com:4000/postgresql</span><br><span class="line">image: a4bc65fd <span class="comment"># 镜像id</span></span><br><span class="line"></span><br><span class="line">logging</span><br><span class="line">服务的日志记录配置。</span><br><span class="line">driver：指定服务容器的日志记录驱动程序，默认值为json-file。有以下三个选项</span><br><span class="line"></span><br><span class="line">driver: <span class="string">"json-file"</span></span><br><span class="line">driver: <span class="string">"syslog"</span></span><br><span class="line">driver: <span class="string">"none"</span></span><br><span class="line"></span><br><span class="line">仅在 json-file 驱动程序下，可以使用以下参数，限制日志得数量和大小。</span><br><span class="line"></span><br><span class="line">logging:</span><br><span class="line">  driver: json-file</span><br><span class="line">  options:</span><br><span class="line">    max-size: <span class="string">"200k"</span> <span class="comment"># 单个文件大小为200k</span></span><br><span class="line">    max-file: <span class="string">"10"</span> <span class="comment"># 最多10个文件</span></span><br><span class="line">    </span><br><span class="line">当达到文件限制上限，会自动删除旧得文件。</span><br><span class="line">syslog 驱动程序下，可以使用 syslog-address 指定日志接收地址。</span><br><span class="line"></span><br><span class="line">logging:</span><br><span class="line">  driver: syslog</span><br><span class="line">  options:</span><br><span class="line">    syslog-address: <span class="string">"tcp://192.168.0.42:123"</span></span><br><span class="line">    </span><br><span class="line">network_mode</span><br><span class="line">设置网络模式。</span><br><span class="line"></span><br><span class="line">network_mode: <span class="string">"bridge"</span></span><br><span class="line">network_mode: <span class="string">"host"</span></span><br><span class="line">network_mode: <span class="string">"none"</span></span><br><span class="line">network_mode: <span class="string">"service:[service name]"</span></span><br><span class="line">network_mode: <span class="string">"container:[container name/id]"</span></span><br><span class="line"></span><br><span class="line">networks</span><br><span class="line">配置容器连接的网络，引用顶级 networks 下的条目</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  some-service:</span><br><span class="line">    networks:</span><br><span class="line">      some-network:</span><br><span class="line">        aliases:</span><br><span class="line">         - alias1</span><br><span class="line">      other-network:</span><br><span class="line">        aliases:</span><br><span class="line">         - alias2</span><br><span class="line">networks:</span><br><span class="line">  some-network:</span><br><span class="line">    <span class="comment"># Use a custom driver</span></span><br><span class="line">    driver: custom-driver-1</span><br><span class="line">  other-network:</span><br><span class="line">    <span class="comment"># Use a custom driver which takes special options</span></span><br><span class="line">    driver: custom-driver-2</span><br><span class="line">    </span><br><span class="line">aliases ：同一网络上的其他容器可以使用服务名称或此别名来连接到对应容器的服务。</span><br><span class="line"></span><br><span class="line">restart</span><br><span class="line">  no：是默认的重启策略，在任何情况下都不会重启容器。</span><br><span class="line">  always：容器总是重新启动。</span><br><span class="line">  on-failure：在容器非正常退出时（退出状态非0），才会重启容器。</span><br><span class="line">  unless-stopped：在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器</span><br><span class="line">  </span><br><span class="line">restart: <span class="string">"no"</span></span><br><span class="line">restart: always</span><br><span class="line">restart: on-failure</span><br><span class="line">restart: unless-stopped</span><br><span class="line"></span><br><span class="line">注：swarm 集群模式，请改用 restart_policy。</span><br><span class="line"></span><br><span class="line">secrets</span><br><span class="line">存储敏感数据，例如密码：</span><br><span class="line"></span><br><span class="line">version: <span class="string">"3.1"</span></span><br><span class="line">services:</span><br><span class="line"></span><br><span class="line">mysql:</span><br><span class="line">  image: mysql</span><br><span class="line">  environment:</span><br><span class="line">    MYSQL_ROOT_PASSWORD_FILE: /run/secrets/my_secret</span><br><span class="line">  secrets:</span><br><span class="line">    - my_secret</span><br><span class="line"></span><br><span class="line">secrets:</span><br><span class="line">  my_secret:</span><br><span class="line">    file: ./my_secret.txt</span><br><span class="line">    </span><br><span class="line">security_opt</span><br><span class="line">修改容器默认的 schema 标签。</span><br><span class="line"></span><br><span class="line">security-opt：</span><br><span class="line">  - label:user:USER   <span class="comment"># 设置容器的用户标签</span></span><br><span class="line">  - label:role:ROLE   <span class="comment"># 设置容器的角色标签</span></span><br><span class="line">  - label:<span class="built_in">type</span>:TYPE   <span class="comment"># 设置容器的安全策略标签</span></span><br><span class="line">  - label:level:LEVEL  <span class="comment"># 设置容器的安全等级标签</span></span><br><span class="line">  </span><br><span class="line">stop_grace_period</span><br><span class="line">指定在容器无法处理 SIGTERM (或者任何 stop_signal 的信号)，等待多久后发送 SIGKILL 信号关闭容器</span><br><span class="line"></span><br><span class="line">stop_grace_period: 1s <span class="comment"># 等待 1 秒</span></span><br><span class="line">stop_grace_period: 1m30s <span class="comment"># 等待 1 分 30 秒 </span></span><br><span class="line"></span><br><span class="line">默认的等待时间是 10 秒。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stop_signal</span><br><span class="line">设置停止容器的替代信号。默认情况下使用 SIGTERM 。</span><br><span class="line">以下示例，使用 SIGUSR1 替代信号 SIGTERM 来停止容器。</span><br><span class="line"></span><br><span class="line">stop_signal: SIGUSR1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sysctls</span><br><span class="line">设置容器中的内核参数，可以使用数组或字典格式。</span><br><span class="line"></span><br><span class="line">sysctls:</span><br><span class="line">  net.core.somaxconn: 1024</span><br><span class="line">  net.ipv4.tcp_syncookies: 0</span><br><span class="line"></span><br><span class="line">sysctls:</span><br><span class="line">  - net.core.somaxconn=1024</span><br><span class="line">  - net.ipv4.tcp_syncookies=0</span><br><span class="line">  </span><br><span class="line">tmpfs</span><br><span class="line">在容器内安装一个临时文件系统。可以是单个值或列表的多个值。</span><br><span class="line"></span><br><span class="line">tmpfs: /run</span><br><span class="line"></span><br><span class="line">tmpfs:</span><br><span class="line">  - /run</span><br><span class="line">  - /tmp</span><br><span class="line"></span><br><span class="line">ulimits</span><br><span class="line">覆盖容器默认的 <span class="built_in">ulimit</span>。</span><br><span class="line">ulimits:</span><br><span class="line">  nproc: 65535</span><br><span class="line">  nofile:</span><br><span class="line">    soft: 20000</span><br><span class="line">    hard: 40000</span><br><span class="line">    </span><br><span class="line">volumes</span><br><span class="line">将主机的数据卷或着文件挂载到容器里。</span><br><span class="line">version: <span class="string">"3.7"</span></span><br><span class="line">services:</span><br><span class="line">  db:</span><br><span class="line">    image: postgres:latest</span><br><span class="line">    volumes:</span><br><span class="line">      - <span class="string">"/localhost/postgres.sock:/var/run/postgres/postgres.sock"</span></span><br><span class="line">      - <span class="string">"/localhost/data:/var/lib/postgresql/data"</span></span><br></pre></td></tr></table></figure></div>

<h3 id="2-3-11-Docker-Machine"><a href="#2-3-11-Docker-Machine" class="headerlink" title="2.3.11 Docker Machine"></a>2.3.11 Docker Machine</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">简介</span><br><span class="line">Docker Machine 是一种可以让您在虚拟主机上安装 Docker 的工具，并可以使用 docker-machine 命令来管理主机。</span><br><span class="line">Docker Machine 也可以集中管理所有的 docker 主机，比如快速的给 100 台服务器安装上 docker。</span><br><span class="line"></span><br><span class="line">Docker Machine 管理的虚拟主机可以是机上的，也可以是云供应商，如阿里云，腾讯云，AWS，或 DigitalOcean。</span><br><span class="line">使用 docker-machine 命令，您可以启动，检查，停止和重新启动托管主机，也可以升级 Docker 客户端和守护程序，以及配置 Docker 客户端与您的主机进行通信。</span><br></pre></td></tr></table></figure></div>
<p><img src="machine.png" alt="img"></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">[安装]</span><br><span class="line">安装 Docker Machine 之前你需要先安装 Docker。</span><br><span class="line">Docker Mechine 可以在多种平台上安装使用，包括 Linux 、MacOS 以及 windows</span><br><span class="line"></span><br><span class="line">Linux 安装命令</span><br><span class="line">$ base=https://github.com/docker/machine/releases/download/v0.16.0 &amp;&amp;</span><br><span class="line">  curl -L <span class="variable">$base</span>/docker-machine-$(uname -s)-$(uname -m) &gt;/tmp/docker-machine &amp;&amp;</span><br><span class="line">  sudo mv /tmp/docker-machine /usr/<span class="built_in">local</span>/bin/docker-machine &amp;&amp;</span><br><span class="line">  chmod +x /usr/<span class="built_in">local</span>/bin/docker-machine</span><br><span class="line"></span><br><span class="line">macOS 安装命令</span><br><span class="line">$ base=https://github.com/docker/machine/releases/download/v0.16.0 &amp;&amp;</span><br><span class="line">  curl -L <span class="variable">$base</span>/docker-machine-$(uname -s)-$(uname -m) &gt;/usr/<span class="built_in">local</span>/bin/docker-machine &amp;&amp;</span><br><span class="line">  chmod +x /usr/<span class="built_in">local</span>/bin/docker-machine</span><br><span class="line">  </span><br><span class="line">Windows 安装命令</span><br><span class="line">如果你是 Windows 平台，可以使用 Git BASH，并输入以下命令：</span><br><span class="line">$ base=https://github.com/docker/machine/releases/download/v0.16.0 &amp;&amp;</span><br><span class="line">  mkdir -p <span class="string">"<span class="variable">$HOME</span>/bin"</span> &amp;&amp;</span><br><span class="line">  curl -L <span class="variable">$base</span>/docker-machine-Windows-x86_64.exe &gt; <span class="string">"<span class="variable">$HOME</span>/bin/docker-machine.exe"</span> &amp;&amp;</span><br><span class="line">  chmod +x <span class="string">"<span class="variable">$HOME</span>/bin/docker-machine.exe"</span></span><br><span class="line">  </span><br><span class="line">查看是否安装成功：</span><br><span class="line">$ docker-machine version</span><br><span class="line">docker-machine version 0.16.0, build 9371605</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[使用]</span><br><span class="line">通过 virtualbox 来介绍 docker-machine 的使用方法。其他云服务商操作与此基本一致。具体可以参考每家服务商的指导文档。</span><br><span class="line">1、列出可用的机器</span><br><span class="line">可以看到目前只有这里默认的 default 虚拟机。</span><br><span class="line"></span><br><span class="line">  $ docker-machine ls</span><br><span class="line">  </span><br><span class="line">2、创建机器</span><br><span class="line">创建一台名为 <span class="built_in">test</span> 的机器。</span><br><span class="line"></span><br><span class="line">  $ docker-machine create --driver virtualbox <span class="built_in">test</span></span><br><span class="line">  </span><br><span class="line">-driver：指定用来创建机器的驱动类型，这里是 virtualbox</span><br><span class="line"></span><br><span class="line">3、查看机器的 ip</span><br><span class="line"></span><br><span class="line">  $ docker-machine ip <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">4、停止机器</span><br><span class="line"></span><br><span class="line">  $ docker-machine stop <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">5、启动机器</span><br><span class="line"></span><br><span class="line">  $ docker-machine start <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">6、进入机器</span><br><span class="line"></span><br><span class="line">  $ docker-machine ssh <span class="built_in">test</span></span><br><span class="line">  </span><br><span class="line">docker-machine 命令参数说明</span><br><span class="line">  docker-machine active：查看当前激活状态的 Docker 主机。</span><br><span class="line">---</span><br><span class="line">$ docker-machine ls</span><br><span class="line"></span><br><span class="line">NAME      ACTIVE   DRIVER         STATE     URL</span><br><span class="line">dev       -        virtualbox     Running   tcp://192.168.99.103:2376</span><br><span class="line">staging   *        digitalocean   Running   tcp://203.0.113.81:2376</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$DOCKER_HOST</span></span><br><span class="line">tcp://203.0.113.81:2376</span><br><span class="line"></span><br><span class="line">$ docker-machine active</span><br><span class="line">staging</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">config：查看当前激活状态 Docker 主机的连接信息。</span><br><span class="line">creat：创建 Docker 主机</span><br><span class="line">env：显示连接到某个主机需要的环境变量</span><br><span class="line">inspect： 以 json 格式输出指定Docker的详细信息</span><br><span class="line">ip： 获取指定 Docker 主机的地址</span><br><span class="line"><span class="built_in">kill</span>： 直接杀死指定的 Docker 主机</span><br><span class="line">ls： 列出所有的管理主机</span><br><span class="line">provision： 重新配置指定主机</span><br><span class="line">regenerate-certs： 为某个主机重新生成 TLS 信息</span><br><span class="line">restart： 重启指定的主机</span><br><span class="line">rm： 删除某台 Docker 主机，对应的虚拟机也会被删除</span><br><span class="line">ssh： 通过 SSH 连接到主机上，执行命令</span><br><span class="line">scp： 在 Docker 主机之间以及 Docker 主机和本地主机之间通过 scp 远程复制数据</span><br><span class="line">mount： 使用 SSHFS 从计算机装载或卸载目录</span><br><span class="line">start： 启动一个指定的 Docker 主机，如果对象是个虚拟机，该虚拟机将被启动</span><br><span class="line">status： 获取指定 Docker 主机的状态(包括：Running、Paused、Saved、Stopped、Stopping、Starting、Error)等</span><br><span class="line">stop： 停止一个指定的 Docker 主机</span><br><span class="line">upgrade： 将一个指定主机的 Docker 版本更新为最新</span><br><span class="line">url： 获取指定 Docker 主机的监听 URL</span><br><span class="line">version： 显示 Docker Machine 的版本或者主机 Docker 版本</span><br><span class="line"><span class="built_in">help</span>： 显示帮助信息</span><br></pre></td></tr></table></figure></div>

<h1 id="三-Docker-常用服务docker化"><a href="#三-Docker-常用服务docker化" class="headerlink" title="三. Docker 常用服务docker化"></a>三. Docker 常用服务docker化</h1><h2 id="3-1-Docker-安装-Ubuntu"><a href="#3-1-Docker-安装-Ubuntu" class="headerlink" title="3.1 Docker 安装 Ubuntu"></a>3.1 Docker 安装 Ubuntu</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">1. 查看可用的 Ubuntu 版本</span><br><span class="line">访问 Ubuntu 镜像库地址： https://hub.docker.com/_/ubuntu?tab=tags&amp;page=1</span><br><span class="line">可以通过 Sort by 查看其他版本的 Ubuntu。默认是最新版本 ubuntu:latest</span><br><span class="line"></span><br><span class="line">2. 拉取最新版的 Ubuntu 镜像</span><br><span class="line">$ docker pull ubuntu</span><br><span class="line">或者：</span><br><span class="line">$ docker pull ubuntu:latest</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器，并且可以通过 <span class="built_in">exec</span> 命令进入 ubuntu 容器</span><br><span class="line">$ docker run -itd --name ubuntu-test ubuntu</span><br><span class="line"></span><br><span class="line">5. 安装成功</span><br><span class="line">最后我们可以通过 docker ps 命令查看容器的运行信息</span><br></pre></td></tr></table></figure></div>

<h2 id="3-2-Docker-安装-Centos"><a href="#3-2-Docker-安装-Centos" class="headerlink" title="3.2 Docker 安装 Centos"></a>3.2 Docker 安装 Centos</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">1. 查看可用的 CentOS 版本</span><br><span class="line">访问 CentOS 镜像库地址：https://hub.docker.com/_/centos?tab=tags&amp;page=1。</span><br><span class="line">可以通过 Sort by 查看其他版本的 CentOS 。默认是最新版本 centos:latest</span><br><span class="line"></span><br><span class="line">2. 拉取指定版本的 CentOS 镜像，这里我们安装指定版本为例(centos7):</span><br><span class="line">$ docker pull centos:centos7</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">使用以下命令来查看是否已安装了 centos7：</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器，并且可以通过 <span class="built_in">exec</span> 命令进入 CentOS 容器</span><br><span class="line">$ docker run -itd --name centos-test centos:centos7</span><br><span class="line"></span><br><span class="line">5. 安装成功</span><br><span class="line">最后我们可以通过 docker ps 命令查看容器的运行信息</span><br></pre></td></tr></table></figure></div>

<h2 id="3-3-Docker-安装-Nginx"><a href="#3-3-Docker-安装-Nginx" class="headerlink" title="3.3 Docker 安装 Nginx"></a>3.3 Docker 安装 Nginx</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">Nginx 是一个高性能的 HTTP 和反向代理 web 服务器，同时也提供了 IMAP/POP3/SMTP 服务 </span><br><span class="line"></span><br><span class="line">1. 查看可用的 Nginx 版本</span><br><span class="line">访问 Nginx 镜像库地址： https://hub.docker.com/_/nginx?tab=tags</span><br><span class="line">可以通过 Sort by 查看其他版本的 Nginx，默认是最新版本 nginx:latest</span><br><span class="line"></span><br><span class="line">此外，我们还可以用 docker search nginx 命令来查看可用版本</span><br><span class="line">$ docker search nginx</span><br><span class="line">NAME                      DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">nginx                     Official build of Nginx.                        3260      [OK]       </span><br><span class="line">jwilder/nginx-proxy       Automated Nginx reverse proxy <span class="keyword">for</span> docker c...   674                  [OK]</span><br><span class="line">richarvey/nginx-php-fpm   Container running Nginx + PHP-FPM capable ...   207                  [OK]</span><br><span class="line">million12/nginx-php       Nginx + PHP-FPM 5.5, 5.6, 7.0 (NG), CentOS...   67                   [OK]</span><br><span class="line">maxexcloo/nginx-php       Docker framework container with Nginx and ...   57                   [OK]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">2. 取最新版的 Nginx 镜像</span><br><span class="line">这里我们拉取官方的最新版本的镜像</span><br><span class="line">$ docker pull nginx:latest</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">使用以下命令来查看是否已安装了 nginx：</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器</span><br><span class="line">$ docker run --name nginx-test -p 8080:80 -d nginx</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">  --name nginx-test：容器名称</span><br><span class="line">  -p 8080:80： 端口进行映射，将本地 8080 端口映射到容器内部的 80 端口</span><br><span class="line">  -d nginx： 设置容器在在后台一直运行</span><br><span class="line">  </span><br><span class="line">5. 安装成功</span><br><span class="line">最后可以通过浏览器可以直接访问 8080 端口的 nginx 服务</span><br></pre></td></tr></table></figure></div>

<h2 id="3-4-Docker-安装-Node-js"><a href="#3-4-Docker-安装-Node-js" class="headerlink" title="3.4 Docker 安装 Node.js"></a>3.4 Docker 安装 Node.js</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境，是一个让 JavaScript 运行在服务端的开发平台</span><br><span class="line"></span><br><span class="line">1. 查看可用的 Node 版本</span><br><span class="line">访问 Node 镜像库地址： https://hub.docker.com/_/node?tab=tags</span><br><span class="line">可以通过 Sort by 查看其他版本的 Node，默认是最新版本 node:latest</span><br><span class="line"></span><br><span class="line">还可以用 docker search node 命令来查看可用版本</span><br><span class="line">$ docker search node</span><br><span class="line"></span><br><span class="line">2. 取最新版的 node 镜像</span><br><span class="line">这里我们拉取官方的最新版本的镜像：</span><br><span class="line">$ docker pull node:latest</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">使用以下命令来查看是否已安装了 node</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器</span><br><span class="line">安装完成后，我们可以使用以下命令来运行 node 容器：</span><br><span class="line">$ docker run -itd --name node-test node</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">--name node-test：容器名称</span><br><span class="line"></span><br><span class="line">5. 安装成功</span><br><span class="line">进入查看容器运行的 node 版本</span><br><span class="line">$ docker <span class="built_in">exec</span> -it node-test /bin/bash</span><br><span class="line">root@6c5d265c68a6:/<span class="comment"># node -v</span></span><br></pre></td></tr></table></figure></div>

<h2 id="3-5-Docker-安装-PHP"><a href="#3-5-Docker-安装-PHP" class="headerlink" title="3.5 Docker 安装 PHP"></a>3.5 Docker 安装 PHP</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">安装 PHP 镜像</span><br><span class="line"></span><br><span class="line">方法一、docker pull php</span><br><span class="line">查找 Docker Hub 上的 php 镜像:</span><br><span class="line">https://hub.docker.com/_/php?tab=tags</span><br><span class="line"></span><br><span class="line">还可以用 docker search php 命令来查看可用版本</span><br><span class="line">runoob@runoob:~/php-fpm$ docker search php</span><br><span class="line">NAME                      DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">php                       While designed <span class="keyword">for</span> web development, the PH...   1232      [OK]       </span><br><span class="line">richarvey/nginx-php-fpm   Container running Nginx + PHP-FPM capable ...   207                  [OK]</span><br><span class="line">phpmyadmin/phpmyadmin     A web interface <span class="keyword">for</span> MySQL and MariaDB.          123                  [OK]</span><br><span class="line">eboraas/apache-php        PHP5 on Apache (with SSL support), built o...   69                   [OK]</span><br><span class="line">php-zendserver            Zend Server - the integrated PHP applicati...   69        [OK]       </span><br><span class="line">million12/nginx-php       Nginx + PHP-FPM 5.5, 5.6, 7.0 (NG), CentOS...   67                   [OK]</span><br><span class="line">webdevops/php-nginx       Nginx with PHP-FPM                              39                   [OK]</span><br><span class="line">webdevops/php-apache      Apache with PHP-FPM (based on webdevops/php)    14                   [OK]</span><br><span class="line">phpunit/phpunit           PHPUnit is a programmer-oriented testing f...   14                   [OK]</span><br><span class="line">tetraweb/php              PHP 5.3, 5.4, 5.5, 5.6, 7.0 <span class="keyword">for</span> CI and run...   12                   [OK]</span><br><span class="line">webdevops/php             PHP (FPM and CLI) service container             10                   [OK]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">这里我们拉取官方的镜像,标签为5.6-fpm</span><br><span class="line">runoob@runoob:~/php-fpm$ docker pull php:5.6-fpm</span><br><span class="line"></span><br><span class="line">等待下载完成后，我们就可以在本地镜像列表里查到REPOSITORY为php,标签为5.6-fpm的镜像</span><br><span class="line">runoob@runoob:~/php-fpm$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">php                 5.6-fpm             025041cd3aa5        6 days ago          456.3 MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Nginx + PHP 部署</span><br><span class="line"></span><br><span class="line">Nginx 部署可以查看：Docker安装Nginx</span><br><span class="line"></span><br><span class="line">启动 PHP：</span><br><span class="line">$ docker run --name  myphp-fpm -v ~/nginx/www:/www  -d php:5.6-fpm</span><br><span class="line"></span><br><span class="line">命令说明：</span><br><span class="line">--name myphp-fpm : 将容器命名为 myphp-fpm</span><br><span class="line">-v ~/nginx/www:/www : 将主机中项目的目录 www 挂载到容器的 /www</span><br><span class="line"></span><br><span class="line">创建 ~/nginx/conf/conf.d 目录：</span><br><span class="line">mkdir ~/nginx/conf/conf.d </span><br><span class="line"></span><br><span class="line">在该目录下添加 ~/nginx/conf/conf.d/runoob-test-php.conf 文件，内容如下：</span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">        index  index.html index.htm index.php;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    error_page   500 502 503 504  /50x.html;</span><br><span class="line">    location = /50x.html &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ \.php$ &#123;</span><br><span class="line">        fastcgi_pass   php:9000;</span><br><span class="line">        fastcgi_index  index.php;</span><br><span class="line">        fastcgi_param  SCRIPT_FILENAME  /www/<span class="variable">$fastcgi_script_name</span>;</span><br><span class="line">        include        fastcgi_params;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">配置文件说明：</span><br><span class="line">php:9000: 表示 php-fpm 服务的 URL，下面我们会具体说明。</span><br><span class="line">/www/: 是 myphp-fpm 中 php 文件的存储路径，映射到本地的 ~/nginx/www 目录</span><br><span class="line"></span><br><span class="line">启动 nginx：</span><br><span class="line">docker run --name runoob-php-nginx -p 8083:80 -d \</span><br><span class="line">&gt; -v ~/nginx/www:/usr/share/nginx/html:ro \</span><br><span class="line">&gt; -v ~/nginx/conf/conf.d:/etc/nginx/conf.d:ro \</span><br><span class="line">&gt; --link myphp-fpm:php \</span><br><span class="line">&gt; nginx</span><br><span class="line"></span><br><span class="line">-p 8083:80: 端口映射，把 nginx 中的 80 映射到本地的 8083 端口。</span><br><span class="line">~/nginx/www: 是本地 html 文件的存储目录，/usr/share/nginx/html 是容器内 html 文件的存储目录。</span><br><span class="line">~/nginx/conf/conf.d: 是本地 nginx 配置文件的存储目录，/etc/nginx/conf.d 是容器内 nginx 配置文件的存储目录。</span><br><span class="line">--link myphp-fpm:php: 把 myphp-fpm 的网络并入 nginx，并通过修改 nginx 的 /etc/hosts，把域名 php 映射成 127.0.0.1，让 nginx 通过 php:9000 访问 php-fpm</span><br><span class="line"></span><br><span class="line">接下来我们在 ~/nginx/www 目录下创建 index.php，代码如下：</span><br><span class="line">&lt;?php</span><br><span class="line"><span class="built_in">echo</span> phpinfo();</span><br><span class="line">?&gt;</span><br><span class="line"></span><br><span class="line">浏览器打开 http://127.0.0.1:8083/index.php</span><br></pre></td></tr></table></figure></div>

<h2 id="3-6-Docker-安装-MySQL"><a href="#3-6-Docker-安装-MySQL" class="headerlink" title="3.6 Docker 安装 MySQL"></a>3.6 Docker 安装 MySQL</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">MySQL 是世界上最受欢迎的开源数据库。凭借其可靠性、易用性和性能，MySQL 已成为 Web 应用程序的数据库优先选择</span><br><span class="line"></span><br><span class="line">1. 查看可用的 MySQL 版本</span><br><span class="line">访问 MySQL 镜像库地址：https://hub.docker.com/_/mysql?tab=tags</span><br><span class="line">可以通过 Sort by 查看其他版本的 MySQL，默认是最新版本 mysql:latest </span><br><span class="line"></span><br><span class="line">还可以用 docker search mysql 命令来查看可用版本</span><br><span class="line">$ docker search mysql</span><br><span class="line">NAME                     DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">mysql                    MySQL is a widely used, open-source relati...   2529      [OK]       </span><br><span class="line">mysql/mysql-server       Optimized MySQL Server Docker images. Crea...   161                  [OK]</span><br><span class="line">centurylink/mysql        Image containing mysql. Optimized to be li...   45                   [OK]</span><br><span class="line">sameersbn/mysql                                                          36                   [OK]</span><br><span class="line">google/mysql             MySQL server <span class="keyword">for</span> Google Compute Engine          16                   [OK]</span><br><span class="line">appcontainers/mysql      Centos/Debian Based Customizable MySQL Con...   8                    [OK]</span><br><span class="line">marvambass/mysql         MySQL Server based on Ubuntu 14.04              6                    [OK]</span><br><span class="line">drupaldocker/mysql       MySQL <span class="keyword">for</span> Drupal                                2                    [OK]</span><br><span class="line">azukiapp/mysql           Docker image to run MySQL by Azuki - http:...   2                    [OK]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">2. 拉取 MySQL 镜像</span><br><span class="line">$ docker pull mysql:latest</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器</span><br><span class="line">$ docker run -itd --name mysql-test -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">-p 3306:3306 ：映射容器服务的 3306 端口到宿主机的 3306 端口，外部主机可以直接通过 宿主机ip:3306 访问到 MySQL 的服务。</span><br><span class="line">MYSQL_ROOT_PASSWORD=123456：设置 MySQL 服务 root 用户的密码</span><br><span class="line"></span><br><span class="line">5. 安装成功</span><br><span class="line">通过 docker ps 命令查看是否安装成功</span><br><span class="line">通过 root 和密码 123456 访问 MySQL 服务</span><br><span class="line"><span class="comment"># mysql -h localhost -u root -p</span></span><br><span class="line"></span><br><span class="line">MySQL(5.7.19)的docker镜像在创建时映射的配置文件目录有所不同</span><br><span class="line">MySQL(5.7.19)的默认配置文件是 /etc/mysql/my.cnf 文件。如果想要自定义配置，建议向 /etc/mysql/conf.d 目录中创建 .cnf 文件。新建的文件可以任意起名，只要保证后缀名是 cnf 即可。新建的文件中的配置项可以覆盖 /etc/mysql/my.cnf 中的配置项。</span><br><span class="line"></span><br><span class="line">具体操作：</span><br><span class="line">首先需要创建将要映射到容器中的目录以及.cnf文件，然后再创建容器</span><br><span class="line"><span class="comment"># pwd</span></span><br><span class="line">/opt</span><br><span class="line"><span class="comment"># mkdir -p docker_v/mysql/conf</span></span><br><span class="line"><span class="comment"># cd docker_v/mysql/conf</span></span><br><span class="line"><span class="comment"># touch my.cnf</span></span><br><span class="line"><span class="comment"># docker run -p 3306:3306 --name mysql -v /opt/docker_v/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -d imageID</span></span><br><span class="line">4ec4f56455ea2d6d7251a05b7f308e314051fdad2c26bf3d0f27a9b0c0a71414</span><br><span class="line"></span><br><span class="line">命令说明：</span><br><span class="line">-p 3306:3306：将容器的3306端口映射到主机的3306端口</span><br><span class="line">-v /opt/docker_v/mysql/conf:/etc/mysql/conf.d：将主机/opt/docker_v/mysql/conf目录挂载到容器的/etc/mysql/conf.d</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456：初始化root用户的密码</span><br><span class="line">-d: 后台运行容器，并返回容器ID</span><br><span class="line">imageID: mysql镜像ID</span><br><span class="line"></span><br><span class="line">查看容器运行情况</span><br><span class="line"><span class="comment"># docker ps</span></span><br><span class="line">CONTAINER ID IMAGE          COMMAND          ... PORTS                    NAMES</span><br><span class="line">4ec4f56455ea c73c7527c03a  <span class="string">"docker-entrypoint.sh"</span> ... 0.0.0.0:3306-&gt;3306/tcp   mysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker 安装 mysql 8 版本</span><br><span class="line"><span class="comment"># docker 中下载 mysql</span></span><br><span class="line">docker pull mysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动</span></span><br><span class="line">docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=Lzslov123! -d mysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mysql bash</span><br><span class="line"></span><br><span class="line"><span class="comment">#登录mysql</span></span><br><span class="line">mysql -u root -p</span><br><span class="line">ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'Lzslov123!'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加远程登录用户</span></span><br><span class="line">CREATE USER <span class="string">'liaozesong'</span>@<span class="string">'%'</span> IDENTIFIED WITH mysql_native_password BY <span class="string">'Lzslov123!'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO <span class="string">'liaozesong'</span>@<span class="string">'%'</span>;</span><br></pre></td></tr></table></figure></div>

<h2 id="3-7-Docker-安装-Tomcat"><a href="#3-7-Docker-安装-Tomcat" class="headerlink" title="3.7 Docker 安装 Tomcat"></a>3.7 Docker 安装 Tomcat</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">方法一、docker pull tomcat</span><br><span class="line">查找 Docker Hub 上的 Tomcat 镜像:</span><br><span class="line">https://hub.docker.com/_/tomcat?tab=tags</span><br><span class="line"></span><br><span class="line">可以用 docker search tomcat 命令来查看可用版本</span><br><span class="line">runoob@runoob:~/tomcat$ docker search tomcat</span><br><span class="line">NAME                       DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">tomcat                     Apache Tomcat is an open <span class="built_in">source</span> implementa...   744       [OK]       </span><br><span class="line">dordoka/tomcat             Ubuntu 14.04, Oracle JDK 8 and Tomcat 8 ba...   19                   [OK]</span><br><span class="line">consol/tomcat-7.0          Tomcat 7.0.57, 8080, <span class="string">"admin/admin"</span>              16                   [OK]</span><br><span class="line">consol/tomcat-8.0          Tomcat 8.0.15, 8080, <span class="string">"admin/admin"</span>              14                   [OK]</span><br><span class="line">cloudesire/tomcat          Tomcat server, 6/7/8                            8                    [OK]</span><br><span class="line">davidcaste/alpine-tomcat   Apache Tomcat 7/8 using Oracle Java 7/8 wi...   6                    [OK]</span><br><span class="line">andreptb/tomcat            Debian Jessie based image with Apache Tomc...   4                    [OK]</span><br><span class="line">kieker/tomcat                                                              2                    [OK]</span><br><span class="line">fbrx/tomcat                Minimal Tomcat image based on Alpine Linux      2                    [OK]</span><br><span class="line">jtech/tomcat               Latest Tomcat production distribution on l...   1                    [OK]</span><br><span class="line"></span><br><span class="line">拉取官方的镜像</span><br><span class="line">$ docker pull tomcat</span><br><span class="line"></span><br><span class="line">下载完成后，我们就可以在本地镜像列表里查到 REPOSITORY 为 tomcat 的镜像</span><br><span class="line">$ docker images|grep tomcat</span><br><span class="line">tomcat              latest              70f819d3d2d9        7 days ago          335.8 MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">方法二、通过 Dockerfile 构建</span><br><span class="line">创建Dockerfile</span><br><span class="line">首先，创建目录tomcat,用于存放后面的相关东西</span><br><span class="line">$ mkdir -p ~/tomcat/webapps ~/tomcat/logs ~/tomcat/conf</span><br><span class="line"></span><br><span class="line">webapps 目录将映射为 tomcat 容器配置的应用程序目录。</span><br><span class="line">logs 目录将映射为 tomcat 容器的日志目录。</span><br><span class="line">conf 目录里的配置文件将映射为 tomcat 容器的配置文件。</span><br><span class="line">进入创建的 tomcat 目录，创建 Dockerfile</span><br><span class="line">FROM openjdk:8-jre</span><br><span class="line"></span><br><span class="line">ENV CATALINA_HOME /usr/<span class="built_in">local</span>/tomcat</span><br><span class="line">ENV PATH <span class="variable">$CATALINA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">RUN mkdir -p <span class="string">"<span class="variable">$CATALINA_HOME</span>"</span></span><br><span class="line">WORKDIR <span class="variable">$CATALINA_HOME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># let "Tomcat Native" live somewhere isolated</span></span><br><span class="line">ENV TOMCAT_NATIVE_LIBDIR <span class="variable">$CATALINA_HOME</span>/native-jni-lib</span><br><span class="line">ENV LD_LIBRARY_PATH <span class="variable">$&#123;LD_LIBRARY_PATH:+$LD_LIBRARY_PATH:&#125;</span><span class="variable">$TOMCAT_NATIVE_LIBDIR</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># runtime dependencies for Tomcat Native Libraries</span></span><br><span class="line"><span class="comment"># Tomcat Native 1.2+ requires a newer version of OpenSSL than debian:jessie has available</span></span><br><span class="line"><span class="comment"># &gt; checking OpenSSL library version &gt;= 1.0.2...</span></span><br><span class="line"><span class="comment"># &gt; configure: error: Your version of OpenSSL is not compatible with this version of tcnative</span></span><br><span class="line"><span class="comment"># see http://tomcat.10.x6.nabble.com/VOTE-Release-Apache-Tomcat-8-0-32-tp5046007p5046024.html (and following discussion)</span></span><br><span class="line"><span class="comment"># and https://github.com/docker-library/tomcat/pull/31</span></span><br><span class="line">ENV OPENSSL_VERSION 1.1.0f-3+deb9u2</span><br><span class="line">RUN <span class="built_in">set</span> -ex; \</span><br><span class="line">    currentVersion=<span class="string">"<span class="variable">$(dpkg-query --show --showformat '$&#123;Version&#125;\n' openssl)</span>"</span>; \</span><br><span class="line">    <span class="keyword">if</span> dpkg --compare-versions <span class="string">"<span class="variable">$currentVersion</span>"</span> <span class="string">'&lt;&lt;'</span> <span class="string">"<span class="variable">$OPENSSL_VERSION</span>"</span>; <span class="keyword">then</span> \</span><br><span class="line">        <span class="keyword">if</span> ! grep -q stretch /etc/apt/sources.list; <span class="keyword">then</span> \</span><br><span class="line"><span class="comment"># only add stretch if we're not already building from within stretch</span></span><br><span class="line">            &#123; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'deb http://deb.debian.org/debian stretch main'</span>; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'deb http://security.debian.org stretch/updates main'</span>; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'deb http://deb.debian.org/debian stretch-updates main'</span>; \</span><br><span class="line">            &#125; &gt; /etc/apt/sources.list.d/stretch.list; \</span><br><span class="line">            &#123; \</span><br><span class="line"><span class="comment"># add a negative "Pin-Priority" so that we never ever get packages from stretch unless we explicitly request them</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'Package: *'</span>; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'Pin: release n=stretch*'</span>; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'Pin-Priority: -10'</span>; \</span><br><span class="line">                <span class="built_in">echo</span>; \</span><br><span class="line"><span class="comment"># ... except OpenSSL, which is the reason we're here</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'Package: openssl libssl*'</span>; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"Pin: version <span class="variable">$OPENSSL_VERSION</span>"</span>; \</span><br><span class="line">                <span class="built_in">echo</span> <span class="string">'Pin-Priority: 990'</span>; \</span><br><span class="line">            &#125; &gt; /etc/apt/preferences.d/stretch-openssl; \</span><br><span class="line">        <span class="keyword">fi</span>; \</span><br><span class="line">        apt-get update; \</span><br><span class="line">        apt-get install -y --no-install-recommends openssl=<span class="string">"<span class="variable">$OPENSSL_VERSION</span>"</span>; \</span><br><span class="line">        rm -rf /var/lib/apt/lists/*; \</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">        libapr1 \</span><br><span class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># see https://www.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/KEYS</span></span><br><span class="line"><span class="comment"># see also "update.sh" (https://github.com/docker-library/tomcat/blob/master/update.sh)</span></span><br><span class="line">ENV GPG_KEYS 05AB33110949707C93A279E3D3EFE6B686867BA6 07E48665A34DCAFAE522E5E6266191C37C037D42 47309207D818FFD8DCD3F83F1931D684307A10A5 541FBE7D8F78B25E055DDEE13C370389288584E7 61B832AC2F1C5A90F0F9B00A1C506407564C17A3 713DA88BE50911535FE716F5208B0AB1D63011C7 79F7026C690BAA50B92CD8B66A3AD3F4F22C4FED 9BA44C2621385CB966EBA586F72C284D731FABEE A27677289986DB50844682F8ACB77FC2E86E29AC A9C5DF4D22E99998D9875A5110C01C5A2F6059E7 DCFD35E0BF8CA7344752DE8B6FB21E8933C60243 F3A04C595DB5B6A5F1ECA43E3B7BBB100D811BBE F7DA48BB64BCB84ECBA7EE6935CD23C10D498E23</span><br><span class="line"></span><br><span class="line">ENV TOMCAT_MAJOR 8</span><br><span class="line">ENV TOMCAT_VERSION 8.5.32</span><br><span class="line">ENV TOMCAT_SHA512 fc010f4643cb9996cad3812594190564d0a30be717f659110211414faf8063c61fad1f18134154084ad3ddfbbbdb352fa6686a28fbb6402d3207d4e0a88fa9ce</span><br><span class="line"></span><br><span class="line">ENV TOMCAT_TGZ_URLS \</span><br><span class="line"><span class="comment"># https://issues.apache.org/jira/browse/INFRA-8753?focusedCommentId=14735394#comment-14735394</span></span><br><span class="line">    https://www.apache.org/dyn/closer.cgi?action=download&amp;filename=tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz \</span><br><span class="line"><span class="comment"># if the version is outdated, we might have to pull from the dist/archive :/</span></span><br><span class="line">    https://www-us.apache.org/dist/tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz \</span><br><span class="line">    https://www.apache.org/dist/tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz \</span><br><span class="line">    https://archive.apache.org/dist/tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz</span><br><span class="line"></span><br><span class="line">ENV TOMCAT_ASC_URLS \</span><br><span class="line">    https://www.apache.org/dyn/closer.cgi?action=download&amp;filename=tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz.asc \</span><br><span class="line"><span class="comment"># not all the mirrors actually carry the .asc files :'(</span></span><br><span class="line">    https://www-us.apache.org/dist/tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz.asc \</span><br><span class="line">    https://www.apache.org/dist/tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz.asc \</span><br><span class="line">    https://archive.apache.org/dist/tomcat/tomcat-<span class="variable">$TOMCAT_MAJOR</span>/v<span class="variable">$TOMCAT_VERSION</span>/bin/apache-tomcat-<span class="variable">$TOMCAT_VERSION</span>.tar.gz.asc</span><br><span class="line"></span><br><span class="line">RUN <span class="built_in">set</span> -eux; \</span><br><span class="line">    \</span><br><span class="line">    savedAptMark=<span class="string">"<span class="variable">$(apt-mark showmanual)</span>"</span>; \</span><br><span class="line">    apt-get update; \</span><br><span class="line">    \</span><br><span class="line">    apt-get install -y --no-install-recommends gnupg dirmngr; \</span><br><span class="line">    \</span><br><span class="line">    <span class="built_in">export</span> GNUPGHOME=<span class="string">"<span class="variable">$(mktemp -d)</span>"</span>; \</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> <span class="variable">$GPG_KEYS</span>; <span class="keyword">do</span> \</span><br><span class="line">        gpg --keyserver ha.pool.sks-keyservers.net --recv-keys <span class="string">"<span class="variable">$key</span>"</span>; \</span><br><span class="line">    <span class="keyword">done</span>; \</span><br><span class="line">    \</span><br><span class="line">    apt-get install -y --no-install-recommends wget ca-certificates; \</span><br><span class="line">    \</span><br><span class="line">    success=; \</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> <span class="variable">$TOMCAT_TGZ_URLS</span>; <span class="keyword">do</span> \</span><br><span class="line">        <span class="keyword">if</span> wget -O tomcat.tar.gz <span class="string">"<span class="variable">$url</span>"</span>; <span class="keyword">then</span> \</span><br><span class="line">            success=1; \</span><br><span class="line">            <span class="built_in">break</span>; \</span><br><span class="line">        <span class="keyword">fi</span>; \</span><br><span class="line">    <span class="keyword">done</span>; \</span><br><span class="line">    [ -n <span class="string">"<span class="variable">$success</span>"</span> ]; \</span><br><span class="line">    \</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$TOMCAT_SHA512</span> *tomcat.tar.gz"</span> | sha512sum -c -; \</span><br><span class="line">    \</span><br><span class="line">    success=; \</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> <span class="variable">$TOMCAT_ASC_URLS</span>; <span class="keyword">do</span> \</span><br><span class="line">        <span class="keyword">if</span> wget -O tomcat.tar.gz.asc <span class="string">"<span class="variable">$url</span>"</span>; <span class="keyword">then</span> \</span><br><span class="line">            success=1; \</span><br><span class="line">            <span class="built_in">break</span>; \</span><br><span class="line">        <span class="keyword">fi</span>; \</span><br><span class="line">    <span class="keyword">done</span>; \</span><br><span class="line">    [ -n <span class="string">"<span class="variable">$success</span>"</span> ]; \</span><br><span class="line">    \</span><br><span class="line">    gpg --batch --verify tomcat.tar.gz.asc tomcat.tar.gz; \</span><br><span class="line">    tar -xvf tomcat.tar.gz --strip-components=1; \</span><br><span class="line">    rm bin/*.bat; \</span><br><span class="line">    rm tomcat.tar.gz*; \</span><br><span class="line">    rm -rf <span class="string">"<span class="variable">$GNUPGHOME</span>"</span>; \</span><br><span class="line">    \</span><br><span class="line">    nativeBuildDir=<span class="string">"<span class="variable">$(mktemp -d)</span>"</span>; \</span><br><span class="line">    tar -xvf bin/tomcat-native.tar.gz -C <span class="string">"<span class="variable">$nativeBuildDir</span>"</span> --strip-components=1; \</span><br><span class="line">    apt-get install -y --no-install-recommends \</span><br><span class="line">        dpkg-dev \</span><br><span class="line">        gcc \</span><br><span class="line">        libapr1-dev \</span><br><span class="line">        libssl-dev \</span><br><span class="line">        make \</span><br><span class="line">        <span class="string">"openjdk-<span class="variable">$&#123;JAVA_VERSION%%[.~bu-]*&#125;</span>-jdk=<span class="variable">$JAVA_DEBIAN_VERSION</span>"</span> \</span><br><span class="line">    ; \</span><br><span class="line">    ( \</span><br><span class="line">        <span class="built_in">export</span> CATALINA_HOME=<span class="string">"<span class="variable">$PWD</span>"</span>; \</span><br><span class="line">        <span class="built_in">cd</span> <span class="string">"<span class="variable">$nativeBuildDir</span>/native"</span>; \</span><br><span class="line">        gnuArch=<span class="string">"<span class="variable">$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)</span>"</span>; \</span><br><span class="line">        ./configure \</span><br><span class="line">            --build=<span class="string">"<span class="variable">$gnuArch</span>"</span> \</span><br><span class="line">            --libdir=<span class="string">"<span class="variable">$TOMCAT_NATIVE_LIBDIR</span>"</span> \</span><br><span class="line">            --prefix=<span class="string">"<span class="variable">$CATALINA_HOME</span>"</span> \</span><br><span class="line">            --with-apr=<span class="string">"<span class="variable">$(which apr-1-config)</span>"</span> \</span><br><span class="line">            --with-java-home=<span class="string">"<span class="variable">$(docker-java-home)</span>"</span> \</span><br><span class="line">            --with-ssl=yes; \</span><br><span class="line">        make -j <span class="string">"<span class="variable">$(nproc)</span>"</span>; \</span><br><span class="line">        make install; \</span><br><span class="line">    ); \</span><br><span class="line">    rm -rf <span class="string">"<span class="variable">$nativeBuildDir</span>"</span>; \</span><br><span class="line">    rm bin/tomcat-native.tar.gz; \</span><br><span class="line">    \</span><br><span class="line"><span class="comment"># reset apt-mark's "manual" list so that "purge --auto-remove" will remove all build dependencies</span></span><br><span class="line">    apt-mark auto <span class="string">'.*'</span> &gt; /dev/null; \</span><br><span class="line">    [ -z <span class="string">"<span class="variable">$savedAptMark</span>"</span> ] || apt-mark manual <span class="variable">$savedAptMark</span>; \</span><br><span class="line">    apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=<span class="literal">false</span>; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*; \</span><br><span class="line">    \</span><br><span class="line"><span class="comment"># sh removes env vars it doesn't support (ones with periods)</span></span><br><span class="line"><span class="comment"># https://github.com/docker-library/tomcat/issues/77</span></span><br><span class="line">    find ./bin/ -name <span class="string">'*.sh'</span> -<span class="built_in">exec</span> sed -ri <span class="string">'s|^#!/bin/sh$|#!/usr/bin/env bash|'</span> <span class="string">'&#123;&#125;'</span> +</span><br><span class="line"></span><br><span class="line"><span class="comment"># verify Tomcat Native is working properly</span></span><br><span class="line">RUN <span class="built_in">set</span> -e \</span><br><span class="line">    &amp;&amp; nativeLines=<span class="string">"<span class="variable">$(catalina.sh configtest 2&gt;&amp;1)</span>"</span> \</span><br><span class="line">    &amp;&amp; nativeLines=<span class="string">"<span class="variable">$(echo "$nativeLines" | grep 'Apache Tomcat Native')</span>"</span> \</span><br><span class="line">    &amp;&amp; nativeLines=<span class="string">"<span class="variable">$(echo "$nativeLines" | sort -u)</span>"</span> \</span><br><span class="line">    &amp;&amp; <span class="keyword">if</span> ! <span class="built_in">echo</span> <span class="string">"<span class="variable">$nativeLines</span>"</span> | grep <span class="string">'INFO: Loaded APR based Apache Tomcat Native library'</span> &gt;&amp;2; <span class="keyword">then</span> \</span><br><span class="line">        <span class="built_in">echo</span> &gt;&amp;2 <span class="string">"<span class="variable">$nativeLines</span>"</span>; \</span><br><span class="line">        <span class="built_in">exit</span> 1; \</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">EXPOSE 8080</span><br><span class="line">CMD [<span class="string">"catalina.sh"</span>, <span class="string">"run"</span>]</span><br><span class="line"></span><br><span class="line">通过 Dockerfile 创建一个镜像，替换成你自己的名字</span><br><span class="line">$ docker build -t tomcat .</span><br><span class="line"></span><br><span class="line">创建完成后，我们可以在本地的镜像列表里查找到刚刚创建的镜像</span><br><span class="line">$ docker images|grep tomcat</span><br><span class="line">tomcat              latest              70f819d3d2d9        7 days ago          335.8 MB</span><br><span class="line"></span><br><span class="line">使用 tomcat 镜像</span><br><span class="line">运行容器</span><br><span class="line">$ docker run --name tomcat -p 8080:8080 -v <span class="variable">$PWD</span>/<span class="built_in">test</span>:/usr/<span class="built_in">local</span>/tomcat/webapps/<span class="built_in">test</span> -d tomcat  </span><br><span class="line">acb33fcb4beb8d7f1ebace6f50f5fc204b1dbe9d524881267aa715c61cf75320</span><br><span class="line"></span><br><span class="line">命令说明：</span><br><span class="line">-p 8080:8080：将容器的 8080 端口映射到主机的 8080 端口</span><br><span class="line">-v <span class="variable">$PWD</span>/<span class="built_in">test</span>:/usr/<span class="built_in">local</span>/tomcat/webapps/<span class="built_in">test</span>：将主机中当前目录下的 <span class="built_in">test</span> 挂载到容器的 /<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">查看容器启动情况</span><br><span class="line">$ docker ps </span><br><span class="line">CONTAINER ID    IMAGE     COMMAND               ... PORTS                    NAMES</span><br><span class="line">acb33fcb4beb    tomcat    <span class="string">"catalina.sh run"</span>     ... 0.0.0.0:8080-&gt;8080/tcp   tomcat</span><br><span class="line"></span><br><span class="line">通过浏览器访问</span><br><span class="line">http://xx.xx.xx.xx:8080</span><br></pre></td></tr></table></figure></div>

<h2 id="3-8-Docker-安装-Python"><a href="#3-8-Docker-安装-Python" class="headerlink" title="3.8 Docker 安装 Python"></a>3.8 Docker 安装 Python</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">方法一、docker pull python:3.5</span><br><span class="line">查找 Docker Hub 上的 Python 镜像:</span><br><span class="line">https://hub.docker.com/_/python?tab=tags</span><br><span class="line"></span><br><span class="line">可以用 docker search python 命令来查看可用版本：</span><br><span class="line">$ docker search python</span><br><span class="line">NAME                           DESCRIPTION                        STARS     OFFICIAL   AUTOMATED</span><br><span class="line">python                         Python is an interpreted,...       982       [OK]       </span><br><span class="line">kaggle/python                  Docker image <span class="keyword">for</span> Python...         33                   [OK]</span><br><span class="line">azukiapp/python                Docker image to run Python ...     3                    [OK]</span><br><span class="line">vimagick/python                mini python                                  2          [OK]</span><br><span class="line">tsuru/python                   Image <span class="keyword">for</span> the Python ...           2                    [OK]</span><br><span class="line">pandada8/alpine-python         An alpine based python image                 1          [OK]</span><br><span class="line">1science/python                Python Docker images based on ...  1                    [OK]</span><br><span class="line">lucidfrontier45/python-uwsgi   Python with uWSGI                  1                    [OK]</span><br><span class="line">orbweb/python                  Python image                       1                    [OK]</span><br><span class="line">pathwar/python                 Python template <span class="keyword">for</span> Pathwar levels 1                    [OK]</span><br><span class="line">rounds/10m-python              Python, setuptools and pip.        0                    [OK]</span><br><span class="line">ruimashita/python              ubuntu 14.04 python                0                    [OK]</span><br><span class="line">tnanba/python                  Python on CentOS-7 image.          0                    [OK]</span><br><span class="line"></span><br><span class="line">拉取官方的镜像,标签为3.5</span><br><span class="line">$ docker pull python:3.5</span><br><span class="line"></span><br><span class="line">下载完成后，在本地镜像列表里查到 REPOSITORY 为python, 标签为 3.5 的镜像</span><br><span class="line">$ docker images python:3.5 </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">python              3.5              045767ddf24a        9 days ago          684.1 MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">方法二、通过 Dockerfile 构建</span><br><span class="line">创建 Dockerfile</span><br><span class="line">首先，创建目录 python，用于存放后面的相关东西</span><br><span class="line"></span><br><span class="line">$ mkdir -p ~/python ~/python/myapp</span><br><span class="line"></span><br><span class="line">myapp 目录将映射为 python 容器配置的应用目录</span><br><span class="line">进入创建的 python 目录，创建 Dockerfile</span><br><span class="line"></span><br><span class="line">FROM buildpack-deps:jessie</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove several traces of debian python</span></span><br><span class="line">RUN apt-get purge -y python.*</span><br><span class="line"></span><br><span class="line"><span class="comment"># http://bugs.python.org/issue19846</span></span><br><span class="line"><span class="comment"># &gt; At the moment, setting "LANG=C" on a Linux system *fundamentally breaks Python 3*, and that's not OK.</span></span><br><span class="line">ENV LANG C.UTF-8</span><br><span class="line"></span><br><span class="line"><span class="comment"># gpg: key F73C700D: public key "Larry Hastings &lt;larry@hastings.org&gt;" imported</span></span><br><span class="line">ENV GPG_KEY 97FC712E4C024BBEA48A61ED3A5CA953F73C700D</span><br><span class="line"></span><br><span class="line">ENV PYTHON_VERSION 3.5.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># if this is called "PIP_VERSION", pip explodes with "ValueError: invalid truth value '&lt;VERSION&gt;'"</span></span><br><span class="line">ENV PYTHON_PIP_VERSION 8.1.2</span><br><span class="line"></span><br><span class="line">RUN <span class="built_in">set</span> -ex \</span><br><span class="line">        &amp;&amp; curl -fSL <span class="string">"https://www.python.org/ftp/python/<span class="variable">$&#123;PYTHON_VERSION%%[a-z]*&#125;</span>/Python-<span class="variable">$PYTHON_VERSION</span>.tar.xz"</span> -o python.tar.xz \</span><br><span class="line">        &amp;&amp; curl -fSL <span class="string">"https://www.python.org/ftp/python/<span class="variable">$&#123;PYTHON_VERSION%%[a-z]*&#125;</span>/Python-<span class="variable">$PYTHON_VERSION</span>.tar.xz.asc"</span> -o python.tar.xz.asc \</span><br><span class="line">        &amp;&amp; <span class="built_in">export</span> GNUPGHOME=<span class="string">"<span class="variable">$(mktemp -d)</span>"</span> \</span><br><span class="line">        &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys <span class="string">"<span class="variable">$GPG_KEY</span>"</span> \</span><br><span class="line">        &amp;&amp; gpg --batch --verify python.tar.xz.asc python.tar.xz \</span><br><span class="line">        &amp;&amp; rm -r <span class="string">"<span class="variable">$GNUPGHOME</span>"</span> python.tar.xz.asc \</span><br><span class="line">        &amp;&amp; mkdir -p /usr/src/python \</span><br><span class="line">        &amp;&amp; tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \</span><br><span class="line">        &amp;&amp; rm python.tar.xz \</span><br><span class="line">        \</span><br><span class="line">        &amp;&amp; <span class="built_in">cd</span> /usr/src/python \</span><br><span class="line">        &amp;&amp; ./configure --<span class="built_in">enable</span>-shared --<span class="built_in">enable</span>-unicode=ucs4 \</span><br><span class="line">        &amp;&amp; make -j$(nproc) \</span><br><span class="line">        &amp;&amp; make install \</span><br><span class="line">        &amp;&amp; ldconfig \</span><br><span class="line">        &amp;&amp; pip3 install --no-cache-dir --upgrade --ignore-installed pip==<span class="variable">$PYTHON_PIP_VERSION</span> \</span><br><span class="line">        &amp;&amp; find /usr/<span class="built_in">local</span> -depth \</span><br><span class="line">                \( \</span><br><span class="line">                    \( -<span class="built_in">type</span> d -a -name <span class="built_in">test</span> -o -name tests \) \</span><br><span class="line">                    -o \</span><br><span class="line">                    \( -<span class="built_in">type</span> f -a -name <span class="string">'*.pyc'</span> -o -name <span class="string">'*.pyo'</span> \) \</span><br><span class="line">                \) -<span class="built_in">exec</span> rm -rf <span class="string">'&#123;&#125;'</span> + \</span><br><span class="line">        &amp;&amp; rm -rf /usr/src/python ~/.cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># make some useful symlinks that are expected to exist</span></span><br><span class="line">RUN <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/bin \</span><br><span class="line">        &amp;&amp; ln -s easy_install-3.5 easy_install \</span><br><span class="line">        &amp;&amp; ln -s idle3 idle \</span><br><span class="line">        &amp;&amp; ln -s pydoc3 pydoc \</span><br><span class="line">        &amp;&amp; ln -s python3 python \</span><br><span class="line">        &amp;&amp; ln -s python3-config python-config</span><br><span class="line"></span><br><span class="line">CMD [<span class="string">"python3"</span>]</span><br><span class="line"></span><br><span class="line">通过 Dockerfile 创建一个镜像，替换成你自己的名字</span><br><span class="line">$ docker build -t python:3.5 .</span><br><span class="line"></span><br><span class="line">创建完成后，在本地的镜像列表里查找到刚刚创建的镜像：</span><br><span class="line">runoob@runoob:~/python$ docker images python:3.5 </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">python              3.5              045767ddf24a        9 days ago          684.1 MB</span><br><span class="line"></span><br><span class="line">使用 python 镜像</span><br><span class="line">在 ~/python/myapp 目录下创建一个 helloworld.py 文件，代码如下：</span><br><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Hello, World!"</span>);</span><br><span class="line"></span><br><span class="line">运行容器</span><br><span class="line">runoob@runoob:~/python$ docker run  -v <span class="variable">$PWD</span>/myapp:/usr/src/myapp  -w /usr/src/myapp python:3.5 python helloworld.py</span><br><span class="line"></span><br><span class="line">命令说明：</span><br><span class="line">-v <span class="variable">$PWD</span>/myapp:/usr/src/myapp: 将主机中当前目录下的 myapp 挂载到容器的 /usr/src/myapp</span><br><span class="line">-w /usr/src/myapp: 指定容器的 /usr/src/myapp 目录为工作目录</span><br><span class="line">python helloworld.py: 使用容器的 python 命令来执行工作目录中的 helloworld.py 文件</span><br><span class="line"></span><br><span class="line">输出结果：</span><br><span class="line">Hello, World!</span><br></pre></td></tr></table></figure></div>

<h2 id="3-9-Docker-安装-Redis"><a href="#3-9-Docker-安装-Redis" class="headerlink" title="3.9 Docker 安装 Redis"></a>3.9 Docker 安装 Redis</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">Redis 是一个开源的使用 ANSI C 语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value 的 NoSQL 数据库，并提供多种语言的 API</span><br><span class="line"></span><br><span class="line">1. 查看可用的 Redis 版本</span><br><span class="line">访问 Redis 镜像库地址： https://hub.docker.com/_/redis?tab=tags</span><br><span class="line">可以通过 Sort by 查看其他版本的 Redis，默认是最新版本 redis:latest</span><br><span class="line"></span><br><span class="line">可以用 docker search redis 命令来查看可用版本</span><br><span class="line">$ docker search  redis</span><br><span class="line">NAME                      DESCRIPTION                   STARS  OFFICIAL  AUTOMATED</span><br><span class="line">redis                     Redis is an open <span class="built_in">source</span> ...   2321   [OK]       </span><br><span class="line">sameersbn/redis                                         32                   [OK]</span><br><span class="line">torusware/speedus-redis   Always updated official ...   29             [OK]</span><br><span class="line">bitnami/redis             Bitnami Redis Docker Image    22                   [OK]</span><br><span class="line">anapsix/redis             11MB Redis server image ...   6                    [OK]</span><br><span class="line">webhippie/redis           Docker images <span class="keyword">for</span> redis       4                    [OK]</span><br><span class="line">clue/redis-benchmark      A minimal docker image t...   3                    [OK]</span><br><span class="line">williamyeh/redis          Redis image <span class="keyword">for</span> Docker        3                    [OK]</span><br><span class="line">unblibraries/redis        Leverages phusion/baseim...   2                    [OK]</span><br><span class="line">greytip/redis             redis 3.0.3                   1                    [OK]</span><br><span class="line">servivum/redis            Redis Docker Image            1                    [OK]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">2. 取最新版的 Redis 镜像</span><br><span class="line">拉取官方的最新版本的镜像：</span><br><span class="line">$ docker pull redis:latest</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">使用以下命令来查看是否已下载了 redis：</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器</span><br><span class="line">$ docker run -itd --name redis-test -p 6379:6379 redis</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">-p 6379:6379：映射容器服务的 6379 端口到宿主机的 6379 端口。外部可以直接通过宿主机ip:6379 访问到 Redis 的服务</span><br><span class="line"></span><br><span class="line">5. 安装成功</span><br><span class="line">可以通过 docker ps 命令查看容器的运行信息</span><br><span class="line">通过 redis-cli 连接测试使用 redis 服务</span><br><span class="line"></span><br><span class="line">$ docker <span class="built_in">exec</span> -it redis-test /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-cli</span></span><br><span class="line">127.0.0.1：6379&gt; <span class="built_in">set</span> <span class="built_in">test</span> 1</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></div>

<h2 id="3-10-Docker-安装-MongoDB"><a href="#3-10-Docker-安装-MongoDB" class="headerlink" title="3.10 Docker 安装 MongoDB"></a>3.10 Docker 安装 MongoDB</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">MongoDB 是一个免费的开源跨平台面向文档的 NoSQL 数据库程序</span><br><span class="line"></span><br><span class="line">1. 查看可用的 MongoDB 版本</span><br><span class="line">访问 MongoDB 镜像库地址： https://hub.docker.com/_/mongo?tab=tags&amp;page=1</span><br><span class="line">可以通过 Sort by 查看其他版本的 MongoDB，默认是最新版本 mongo:latest</span><br><span class="line"></span><br><span class="line">还可以用 docker search mongo 命令来查看可用版本</span><br><span class="line">$ docker search mongo</span><br><span class="line">NAME                              DESCRIPTION                      STARS     OFFICIAL   AUTOMATED</span><br><span class="line">mongo                             MongoDB document databases ...   1989      [OK]       </span><br><span class="line">mongo-express                     Web-based MongoDB admin int...   22        [OK]       </span><br><span class="line">mvertes/alpine-mongo              light MongoDB container          19                   [OK]</span><br><span class="line">mongooseim/mongooseim-docker      MongooseIM server the lates...   9                    [OK]</span><br><span class="line">torusware/speedus-mongo           Always updated official Mon...   9                    [OK]</span><br><span class="line">jacksoncage/mongo                 Instant MongoDB sharded cluster  6                    [OK]</span><br><span class="line">mongoclient/mongoclient           Official docker image <span class="keyword">for</span> M...   4                    [OK]</span><br><span class="line">jadsonlourenco/mongo-rocks        Percona Mongodb with Rocksd...   4                    [OK]</span><br><span class="line">asteris/apache-php-mongo          Apache2.4 + PHP + Mongo + m...   2                    [OK]</span><br><span class="line">19hz/mongo-container              Mongodb replicaset <span class="keyword">for</span> coreos    1                    [OK]</span><br><span class="line">nitra/mongo                       Mongo3 centos7                   1                    [OK]</span><br><span class="line">ackee/mongo                       MongoDB with fixed Bluemix p...  1                    [OK]</span><br><span class="line">kobotoolbox/mongo                 https://github.com/kobotoolb...  1                    [OK]</span><br><span class="line">valtlfelipe/mongo                 Docker Image based on the la...  1                    [OK]</span><br><span class="line"></span><br><span class="line">2. 取最新版的 MongoDB 镜像</span><br><span class="line">$ docker pull mongo:latest</span><br><span class="line"></span><br><span class="line">3. 查看本地镜像</span><br><span class="line">使用以下命令来查看是否已安装了 mongo</span><br><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">4. 运行容器</span><br><span class="line">安装完成后，我们可以使用以下命令来运行 mongo 容器</span><br><span class="line">$ docker run -itd --name mongo -p 27017:27017 mongo --auth</span><br><span class="line"></span><br><span class="line">参数说明：</span><br><span class="line">-p 27017:27017 ：映射容器服务的 27017 端口到宿主机的 27017 端口。外部可以直接通过 宿主机 ip:27017 访问到 mongo 的服务。</span><br><span class="line">--auth：需要密码才能访问容器服务</span><br><span class="line"></span><br><span class="line">5. 安装成功</span><br><span class="line">通过 docker ps 命令查看容器的运行信息</span><br><span class="line"></span><br><span class="line">使用以下命令添加用户和设置密码，并且尝试连接</span><br><span class="line">$ docker <span class="built_in">exec</span> -it mongo mongo admin</span><br><span class="line"><span class="comment"># 创建一个名为 admin，密码为 123456 的用户。</span></span><br><span class="line">&gt;  db.createUser(&#123; user:<span class="string">'admin'</span>,<span class="built_in">pwd</span>:<span class="string">'123456'</span>,roles:[ &#123; role:<span class="string">'userAdminAnyDatabase'</span>, db: <span class="string">'admin'</span>&#125;]&#125;);</span><br><span class="line"><span class="comment"># 尝试使用上面创建的用户信息进行连接。</span></span><br><span class="line">&gt; db.auth(<span class="string">'admin'</span>, <span class="string">'123456'</span>)</span><br></pre></td></tr></table></figure></div>

<h2 id="3-11-Docker-安装-Apache"><a href="#3-11-Docker-安装-Apache" class="headerlink" title="3.11 Docker 安装 Apache"></a>3.11 Docker 安装 Apache</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="code"><pre><span class="line">方法一、docker pull httpd</span><br><span class="line">查找 Docker Hub 上的 httpd 镜像:</span><br><span class="line">https://hub.docker.com/_/httpd?tab=tags</span><br><span class="line"></span><br><span class="line">可以用 docker search httpd 命令来查看可用版本</span><br><span class="line">runoob@runoob:~/apache$ docker search httpd</span><br><span class="line">NAME                           DESCRIPTION                  STARS  OFFICIAL AUTOMATED</span><br><span class="line">httpd                          The Apache HTTP Server ..    524     [OK]       </span><br><span class="line">centos/httpd                                                7                [OK]</span><br><span class="line">rgielen/httpd-image-php5       Docker image <span class="keyword">for</span> Apache...   1                [OK]</span><br><span class="line">microwebapps/httpd-frontend    Httpd frontend allowing...   1                [OK]</span><br><span class="line">lolhens/httpd                  Apache httpd 2 Server        1                [OK]</span><br><span class="line">publici/httpd                  httpd:latest                 0                [OK]</span><br><span class="line">publicisworldwide/httpd        The Apache httpd webser...   0                [OK]</span><br><span class="line">rgielen/httpd-image-simple     Docker image <span class="keyword">for</span> simple...   0                [OK]</span><br><span class="line">solsson/httpd                  Derivatives of the offi...   0                [OK]</span><br><span class="line">rgielen/httpd-image-drush      Apache HTTPD + Drupal S...   0                [OK]</span><br><span class="line">learninglayers/httpd                                        0                [OK]</span><br><span class="line">sohrabkhan/httpd               Docker httpd + php5.6 (...   0                [OK]</span><br><span class="line">aintohvri/docker-httpd         Apache HTTPD Docker ext...   0                [OK]</span><br><span class="line">alizarion/httpd                httpd on centos with mo...   0                [OK]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">拉取官方的镜像</span><br><span class="line">runoob@runoob:~/apache$ docker pull httpd</span><br><span class="line"></span><br><span class="line">可以在本地镜像列表里查到REPOSITORY为httpd的镜像</span><br><span class="line">runoob@runoob:~/apache$ docker images httpd</span><br><span class="line">REPOSITORY     TAG        IMAGE ID        CREATED           SIZE</span><br><span class="line">httpd          latest     da1536b4ef14    23 seconds ago    195.1 MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">方法二、通过 Dockerfile 构建</span><br><span class="line"></span><br><span class="line">创建 Dockerfile</span><br><span class="line">首先，创建目录apache,用于存放后面的相关东西。</span><br><span class="line">runoob@runoob:~$ mkdir -p  ~/apache/www ~/apache/logs ~/apache/conf </span><br><span class="line"></span><br><span class="line">www 目录将映射为 apache 容器配置的应用程序目录。</span><br><span class="line">logs 目录将映射为 apache 容器的日志目录。</span><br><span class="line">conf 目录里的配置文件将映射为 apache 容器的配置文件。</span><br><span class="line">进入创建的 apache 目录，创建 Dockerfile。</span><br><span class="line"></span><br><span class="line">FROM debian:jessie</span><br><span class="line"></span><br><span class="line"><span class="comment"># add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added</span></span><br><span class="line"><span class="comment">#RUN groupadd -r www-data &amp;&amp; useradd -r --create-home -g www-data www-data</span></span><br><span class="line"></span><br><span class="line">ENV HTTPD_PREFIX /usr/<span class="built_in">local</span>/apache2</span><br><span class="line">ENV PATH <span class="variable">$PATH</span>:<span class="variable">$HTTPD_PREFIX</span>/bin</span><br><span class="line">RUN mkdir -p <span class="string">"<span class="variable">$HTTPD_PREFIX</span>"</span> \</span><br><span class="line">    &amp;&amp; chown www-data:www-data <span class="string">"<span class="variable">$HTTPD_PREFIX</span>"</span></span><br><span class="line">WORKDIR <span class="variable">$HTTPD_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># install httpd runtime dependencies</span></span><br><span class="line"><span class="comment"># https://httpd.apache.org/docs/2.4/install.html#requirements</span></span><br><span class="line">RUN apt-get update \</span><br><span class="line">    &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">        libapr1 \</span><br><span class="line">        libaprutil1 \</span><br><span class="line">        libaprutil1-ldap \</span><br><span class="line">        libapr1-dev \</span><br><span class="line">        libaprutil1-dev \</span><br><span class="line">        libpcre++0 \</span><br><span class="line">        libssl1.0.0 \</span><br><span class="line">    &amp;&amp; rm -r /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line">ENV HTTPD_VERSION 2.4.20</span><br><span class="line">ENV HTTPD_BZ2_URL https://www.apache.org/dist/httpd/httpd-<span class="variable">$HTTPD_VERSION</span>.tar.bz2</span><br><span class="line"></span><br><span class="line">RUN buildDeps=<span class="string">' \</span></span><br><span class="line"><span class="string">        ca-certificates \</span></span><br><span class="line"><span class="string">        curl \</span></span><br><span class="line"><span class="string">        bzip2 \</span></span><br><span class="line"><span class="string">        gcc \</span></span><br><span class="line"><span class="string">        libpcre++-dev \</span></span><br><span class="line"><span class="string">        libssl-dev \</span></span><br><span class="line"><span class="string">        make \</span></span><br><span class="line"><span class="string">    '</span> \</span><br><span class="line">    <span class="built_in">set</span> -x \</span><br><span class="line">    &amp;&amp; apt-get update \</span><br><span class="line">    &amp;&amp; apt-get install -y --no-install-recommends <span class="variable">$buildDeps</span> \</span><br><span class="line">    &amp;&amp; rm -r /var/lib/apt/lists/* \</span><br><span class="line">    \</span><br><span class="line">    &amp;&amp; curl -fSL <span class="string">"<span class="variable">$HTTPD_BZ2_URL</span>"</span> -o httpd.tar.bz2 \</span><br><span class="line">    &amp;&amp; curl -fSL <span class="string">"<span class="variable">$HTTPD_BZ2_URL</span>.asc"</span> -o httpd.tar.bz2.asc \</span><br><span class="line"><span class="comment"># see https://httpd.apache.org/download.cgi#verify</span></span><br><span class="line">    &amp;&amp; <span class="built_in">export</span> GNUPGHOME=<span class="string">"<span class="variable">$(mktemp -d)</span>"</span> \</span><br><span class="line">    &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys A93D62ECC3C8EA12DB220EC934EA76E6791485A8 \</span><br><span class="line">    &amp;&amp; gpg --batch --verify httpd.tar.bz2.asc httpd.tar.bz2 \</span><br><span class="line">    &amp;&amp; rm -r <span class="string">"<span class="variable">$GNUPGHOME</span>"</span> httpd.tar.bz2.asc \</span><br><span class="line">    \</span><br><span class="line">    &amp;&amp; mkdir -p src \</span><br><span class="line">    &amp;&amp; tar -xvf httpd.tar.bz2 -C src --strip-components=1 \</span><br><span class="line">    &amp;&amp; rm httpd.tar.bz2 \</span><br><span class="line">    &amp;&amp; <span class="built_in">cd</span> src \</span><br><span class="line">    \</span><br><span class="line">    &amp;&amp; ./configure \</span><br><span class="line">        --prefix=<span class="string">"<span class="variable">$HTTPD_PREFIX</span>"</span> \</span><br><span class="line">        --<span class="built_in">enable</span>-mods-shared=reallyall \</span><br><span class="line">    &amp;&amp; make -j<span class="string">"<span class="variable">$(nproc)</span>"</span> \</span><br><span class="line">    &amp;&amp; make install \</span><br><span class="line">    \</span><br><span class="line">    &amp;&amp; <span class="built_in">cd</span> .. \</span><br><span class="line">    &amp;&amp; rm -r src \</span><br><span class="line">    \</span><br><span class="line">    &amp;&amp; sed -ri \</span><br><span class="line">        -e <span class="string">'s!^(\s*CustomLog)\s+\S+!\1 /proc/self/fd/1!g'</span> \</span><br><span class="line">        -e <span class="string">'s!^(\s*ErrorLog)\s+\S+!\1 /proc/self/fd/2!g'</span> \</span><br><span class="line">        <span class="string">"<span class="variable">$HTTPD_PREFIX</span>/conf/httpd.conf"</span> \</span><br><span class="line">    \</span><br><span class="line">    &amp;&amp; apt-get purge -y --auto-remove <span class="variable">$buildDeps</span></span><br><span class="line"></span><br><span class="line">COPY httpd-foreground /usr/<span class="built_in">local</span>/bin/</span><br><span class="line"></span><br><span class="line">EXPOSE 80</span><br><span class="line">CMD [<span class="string">"httpd-foreground"</span>]</span><br><span class="line"></span><br><span class="line">Dockerfile文件中 COPY httpd-foreground /usr/<span class="built_in">local</span>/bin/ 是将当前目录下的httpd-foreground拷贝到镜像里，作为httpd服务的启动脚本，所以要在本地创建一个脚本文件httpd-foreground</span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"><span class="comment"># Apache gets grumpy about PID files pre-existing</span></span><br><span class="line">rm -f /usr/<span class="built_in">local</span>/apache2/logs/httpd.pid</span><br><span class="line"><span class="built_in">exec</span> httpd -DFOREGROUND</span><br><span class="line"></span><br><span class="line">赋予 httpd-foreground 文件可执行权限</span><br><span class="line">runoob@runoob:~/apache$ chmod +x httpd-foreground</span><br><span class="line"></span><br><span class="line">通过 Dockerfile 创建一个镜像，替换成你自己的名字。</span><br><span class="line">runoob@runoob:~/apache$ docker build -t httpd .</span><br><span class="line"></span><br><span class="line">创建完成后，我们可以在本地的镜像列表里查找到刚刚创建的镜像</span><br><span class="line">runoob@runoob:~/apache$ docker images httpd</span><br><span class="line">REPOSITORY     TAG        IMAGE ID        CREATED           SIZE</span><br><span class="line">httpd          latest     da1536b4ef14    23 seconds ago    195.1 MB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">使用 apache 镜像</span><br><span class="line">运行容器</span><br><span class="line"></span><br><span class="line">docker run -p 80:80 -v <span class="variable">$PWD</span>/www/:/usr/<span class="built_in">local</span>/apache2/htdocs/ -v <span class="variable">$PWD</span>/conf/httpd.conf:/usr/<span class="built_in">local</span>/apache2/conf/httpd.conf -v <span class="variable">$PWD</span>/logs/:/usr/<span class="built_in">local</span>/apache2/logs/ -d httpd</span><br><span class="line"></span><br><span class="line">命令说明：</span><br><span class="line">-p 80:80: 将容器的 80 端口映射到主机的 80 端口。</span><br><span class="line">-v <span class="variable">$PWD</span>/www/:/usr/<span class="built_in">local</span>/apache2/htdocs/: 将主机中当前目录下的 www 目录挂载到容器的 /usr/<span class="built_in">local</span>/apache2/htdocs/。</span><br><span class="line">-v <span class="variable">$PWD</span>/conf/httpd.conf:/usr/<span class="built_in">local</span>/apache2/conf/httpd.conf: 将主机中当前目录下的 conf/httpd.conf 文件挂载到容器的 /usr/<span class="built_in">local</span>/apache2/conf/httpd.conf。</span><br><span class="line">-v <span class="variable">$PWD</span>/logs/:/usr/<span class="built_in">local</span>/apache2/logs/: 将主机中当前目录下的 logs 目录挂载到容器的 /usr/<span class="built_in">local</span>/apache2/logs/</span><br><span class="line"></span><br><span class="line">查看容器启动情况</span><br><span class="line">runoob@runoob:~/apache$ docker ps</span><br><span class="line">CONTAINER ID  IMAGE   COMMAND             ... PORTS               NAMES</span><br><span class="line">79a97f2aac37  httpd   <span class="string">"httpd-foreground"</span>  ... 0.0.0.0:80-&gt;80/tcp  sharp_swanson</span><br><span class="line"></span><br><span class="line">通过浏览器访问</span><br></pre></td></tr></table></figure></div>




]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title>青云 Kubernetes</title>
    <url>/frank-wong.github.io/posts/f32b05cc/</url>
    <content><![CDATA[<h1 id="开篇：-Kubernetes-是什么以及为什么需要它"><a href="#开篇：-Kubernetes-是什么以及为什么需要它" class="headerlink" title="开篇： Kubernetes 是什么以及为什么需要它"></a>开篇： Kubernetes 是什么以及为什么需要它</h1><p>Kubernetes 是一个可扩展的，用于容器化应用程序编排，管理的平台。由 Google 于 2014 年基于其大规模生产实践经验而开源出来的。Kubernetes 目前在容器编排领域已经成为事实上的标准，社区也非常活跃。</p>
<p>Kubernetes 在国内外都已经得到广泛的应用，无论是Google, Amazon, GitHub 等还是国内的阿里，腾讯，百度，华为，京东或其他中小公司等也都已全力推进 Kubernetes 在生产中的使用。</p>
<p>现在无论是运维，后端，DBA，亦或者是前端，机器学习工程师等都需要在工作中或多或少的用到 Docker， 而在生产中大量使用的话 Kubernetes 也将会成为趋势，所以了解或掌握 Kubernetes 也成为了工程师必不可少的技能之一。</p>
<h2 id="Kubernetes-是什么"><a href="#Kubernetes-是什么" class="headerlink" title="Kubernetes 是什么?"></a>Kubernetes 是什么?</h2><p>当提到 Kubernetes 的时候，大多数人的可能会想到它可以容器编排，想到它是 PaaS (Platform as a Service) 系统，但其实不然，Kubernetes 并不是 PasS 系统，因为它工作在容器层而不是硬件层，它只不过提供了一些与 PasS 类似或者共同的功能，类似部署，扩容，监控，负载均衡，日志记录等。然而它并不是个完全一体化的平台，这些功能基本都是可选可配置的。</p>
<p>Kubernetes 可支持公有云，私有云及混合云等，具备良好的可移植性。我们可直接使用它或在其之上构建自己的容器/云平台，以达到快速部署，快速扩展，及优化资源使用等。</p>
<p>它致力于提供通用接口类似 CNI( Container Network Interface ), CSI（Container Storage Interface）, CRI（Container Runtime Interface）等规范，以便有更多可能, 让更多的厂商共同加入其生态体系内。它的目标是希望在以后，任何团队都可以在不修改 Kubernetes 核心代码的前提下，可以方便的扩展和构建符合自己需求的平台。</p>
<h2 id="为什么需要-Kubernetes"><a href="#为什么需要-Kubernetes" class="headerlink" title="为什么需要 Kubernetes"></a>为什么需要 Kubernetes</h2><p>我们回到实际的工作环境中。</p>
<ul>
<li>如果你是个前端，你是否遇到过 npm 依赖安装极慢，或是 node sass 安装不了或者版本不对的情况？</li>
<li>如果你是个后端，是否遇到过服务器与本地环境不一致的情况，导致部分功能出现非预期的情况？</li>
<li>如果你是个运维，是否遇到过频繁部署环境，但中间可能出现各种安装不了或者版本不对的问题？</li>
</ul>
<p>目前来看，对于这些问题，最好的解决方案便是标准化，容器化，现在用到最多的也就是 Docker。 Docker 通过 Dockerfile 来对环境进行描述，通过镜像进行交付，使用时不再需要关注环境不一致相关的问题。</p>
<p>现在面试的时候，无论前后端，我们总会多问下是否了解或者使用过 Docker 。如果使用过，那自然会问如果规模变大或者在生产中如何进行容器编排，部署扩容机制如何。</p>
<p>多数人在这个时候都已经回答不上来了，一方面是因为非运维相关岗位的同学，可能在实际工作中并不了解整体的架构体系，没有相关的知识积累。另一方面，对于运维同学可能尚未接触到这部分。</p>
<p>作为一个技术人员，我们应该对整体的体系架构有所了解, 掌握更多的技能，了解软件的完整生命周期，包括开发，交付，部署，以及当流量变大时的扩容等。</p>
<p>在容器编排领域，比较著名的主要有三个：Kubernetes, Mesos, 及 Docker 自家的 Swarm . 对这三者而言，较为简单的是 Swarm, 因为它本身只专注于容器编排，并且是官方团队所作，从各方面来看，对于新手都相对友好一些。但如果是用于生产中大规模使用，反而就略有不及。</p>
<p>而 Mesos 也并不仅限于容器编排，它的创建本身是为了将数据中心的所有资源进行抽象，比如 CPU，内存，网络，存储等，将整个 Mesos 集群当作是一个大的资源池，允许各种 Framework 来进行调度。比如，可以使用 Marathon 来实现 PaaS，可以运行 Spark，Hadoop 等进行计算等。同时因为它支持比如 Docker 或者 LXC 等作资源隔离，所以前几年也被大量用于容器编排。</p>
<p>随着 Kubernetes 在目前的认可度已经超过 Mesos， Docker Swarm 等，无疑它是生产环境中容器应用管理的不二之选。</p>
<p>本小册的目标是帮助更多开发者（不局限于运维，后端，前端等）认识并掌握 Kubernetes 的基础技能，了解其基础架构等。但是 Kubernetes 涉及知识点很多，且更新迭代很快，本小册集中于使用轻快的文字帮助大家掌握 K8S 的通用基础技能，对于其中需掌握的关于 Docker，及 Linux 内核相关的知识不会过于深入解释。主要以最常见 case 入手，帮助大家更快的掌握相关知识并将其用于生产实践中。</p>
<h1 id="初步认识：Kubernetes-基础概念"><a href="#初步认识：Kubernetes-基础概念" class="headerlink" title="初步认识：Kubernetes 基础概念"></a>初步认识：Kubernetes 基础概念</h1><p>好了，总算开始进入正题，抛弃掉死板的说教模式，我们以一个虚构的新成立的项目组为例开始我们的 Kubernetes 探索。(以下统一将 Kubernetes 简写为 K8S) 项目组目前就只有一个成员，我们称他为小张。项目组刚成立的时候，小张也没想好，具体要做什么，但肯定要对外提供服务的，所以先向公司申请了一台服务器。</p>
<h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><p>这台服务器可以用来做什么呢？跑服务，跑数据库，跑测试之类的都可以，我们将它所做的事情统称为工作(work) 那么，它便是工作节点 (worker Node) 对应于 K8S 中，这就是我们首先要认识的 Node 。</p>
<p>Node 可以是一台物理机，也可以是虚拟机，对于我们此处的项目来讲，这台服务器便是 K8S 中的 Node 。</p>
<h3 id="Node-状态"><a href="#Node-状态" class="headerlink" title="Node 状态"></a>Node 状态</h3><p>当我们拿到这台服务器后，首先我们登录服务器查看下服务器的基本配置和信息。其实对于一个新加入 K8S 集群的 Node 也是一样，需要先检查它的状态，并将状态上报至集群的 master 。我们来看看服务器有哪些信息是我们所关心的。</p>
<h4 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h4><p>首先，我们所关心的是我们服务器的 IP 地址，包括内网 IP 和外网 IP。对应于 K8S 集群的话这个概念是类似的，内部 IP 可在 K8S 集群内访问，外部 IP 可在集群外访问。</p>
<p>其次，我们也会关心一下我们的主机名，比如在服务器上执行 <code>hostname</code> 命令，便可得到主机名。K8S 集群中，每个 Node 的主机名也会被记录下来。当然，我们可以通过给 Kubelet 传递一个 <code>--hostname-override</code> 的参数来覆盖默认的主机名。 (Kubelet 是什么，我们后面会解释)</p>
<h4 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h4><p>再之后，我们需要看下服务器的基本信息，比如看看系统版本信息， <code>cat /etc/issue</code> 或者 <code>cat /etc/os-release</code> 等方法均可查看。对于 K8S 集群会将每个 Node 的这些基础信息都记录下来。</p>
<h4 id="容量"><a href="#容量" class="headerlink" title="容量"></a>容量</h4><p>我们通常也都会关注下，我们有几个核心的 CPU ，可通过 <code>cat /proc/cpuinfo</code> 查看，有多大的内存 通过 <code>cat /proc/meminfo</code> 或 <code>free</code> 等查看。对于 K8S 集群，会默认统计这些信息，并计算在此 Node 上可调度的 Pod 数量。（Pod 后面做解释）</p>
<h4 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h4><p>对于我们拿到的服务器，我们关心上述的一些基本信息，并根据这些信息进行判断，这台机器是否能满足我们的需要。对 K8S 集群也同样，当判断上述信息均满足要求时候，便将集群内记录的该 Node 信息标记为 <code>Ready</code> （<code>Ready = True</code>），这样我们的服务器便正式的完成交付。我们来看下其他的部分。</p>
<h2 id="Deployment-和-Pod"><a href="#Deployment-和-Pod" class="headerlink" title="Deployment 和 Pod"></a>Deployment 和 Pod</h2><p>现在小张拿到的服务器已经是可用状态，虽然此时尚不知要具体做什么，但姑且先部署一个主页来宣布下项目组的成立。</p>
<p>我们来看下一般情况下的做法，先写一个静态页面，比如叫 index.html 然后在服务器上启动一个 Nginx 或者其他任何 Web 服务器，来提供对 index.html 的访问。</p>
<p>Nginx 的安装及配置可参考 <a href="https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open-source/" target="_blank" rel="noopener">Nginx 的官方文档</a>。最简单的配置大概类似下面这样（仅保留关键部分）：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">location &#x2F; &#123;</span><br><span class="line">    root   &#x2F;www;</span><br><span class="line">    index  index.html;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>对于 K8S 而言，我们想要的，能提供对 index.html 访问的服务便可理解为 Deployment 的概念，表明一种我们预期的目标状态。</p>
<p>而对于 Nginx 和 index.html 这个组合可以理解为其中的 Pod 概念，作为最小的调度单元。</p>
<h2 id="Container-Runtime"><a href="#Container-Runtime" class="headerlink" title="Container Runtime"></a>Container Runtime</h2><p>虽然此刻部署的内容只有官网，但是为了避免单点故障，于是小张又申请了两台服务器（虽然看起来可能是浪费了点），现在要对原有的服务进行扩容，其实在新的服务器上我们所做的事情也还保持原样，部署 Nginx，提供对 index.html 的访问，甚至配置文件都完全是一样的。可以看到在这种情况下，增加一台服务器，我们需要做一件完全重复的事情。</p>
<p>本着不浪费时间做重复的工作的想法，小张想，要不然用 <a href="https://www.ansible.com/" target="_blank" rel="noopener">Ansible</a> 来统一管理服务器操作和配置吧，但考虑到后续服务器上还需要部署其他的服务，常规的这样部署，容易干扰彼此的环境。</p>
<p>所以我们想到了用虚拟化的技术，但是根据一般的经验，类似 <a href="https://www.linux-kvm.org/page/Main_Page" target="_blank" rel="noopener">KVM</a> 这样的虚拟化技术，可能在资源消耗上较多, 不够轻量级。而容器化相对来看，比较轻量级，也比较符合我们的预期，一次构建，随处执行。我们选择当前最热门的 Docker .</p>
<p>既然技术选型确定了，那很简单，在我们现在三台服务器上安装 Docker ，安装过程不再赘述，可以参考 <a href="https://docs.docker.com/install/linux/docker-ce/centos/" target="_blank" rel="noopener">Docker 的官方安装文档</a> 。</p>
<p>此时，我们需要做的事情，也便只是将我们的服务构建成一个镜像，需要编写一个 Dockerfile，构建一个镜像并部署到每台服务器上便可。</p>
<p>聊了这么多，我们现在已经将我们的服务运行到了容器中，而此处的 Docker 便是我们选择的容器运行时。选择它的最主要原因，便是为了环境隔离和避免重复工作。</p>
<p>而 Docker 如果对应于 K8S 集群中的概念，便是 Container Runtime，这里还有其他的选择，比如 rkt，runc 和其他实现了 OCI 规范的运行时。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这节里面，我们了解到了 Node 其实就是用于工作的服务器，它有一些状态和信息，当这些条件都满足一些条件判断时，Node 便处于 Ready 状态，可用于执行后续的工作。</p>
<p>Deployment 可理解为一种对期望状态的描述， Pod 作为集群中可调度的最小单元，我们会在后面详细讲解其细节。</p>
<p>Docker 是我们选择的容器运行时，可运行我们构建的服务镜像，减少在环境方面所做的重复工作，并且也非常便于部署。除了 Docker 外还存在其他的容器运行时。</p>
<p>了解到这些基本概念后，下节我们从宏观的角度上来认识 K8S 的整体架构，以便我们后续的学习和实践。</p>
<h1 id="宏观认识：整体架构"><a href="#宏观认识：整体架构" class="headerlink" title="宏观认识：整体架构"></a>宏观认识：整体架构</h1><p>工欲善其事，必先利其器。本节我们来从宏观上认识下 K8S 的整体架构，以便于后续在此基础上进行探索和实践。</p>
<h2 id="C-S-架构"><a href="#C-S-架构" class="headerlink" title="C/S 架构"></a>C/S 架构</h2><p>从更高层来看，K8S 整体上遵循 C/S 架构，从这个角度来看，可用下面的图来表示其结构：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">                               +-------------+                              </span><br><span class="line">                               |             |                              </span><br><span class="line">                               |             |               +---------------+</span><br><span class="line">                               |             |       +-----&gt; |     Node 1    |</span><br><span class="line">                               | Kubernetes  |       |       +---------------+</span><br><span class="line">+-----------------+            |   Server    |       |                      </span><br><span class="line">|       CLI       |            |             |       |       +---------------+</span><br><span class="line">|    (Kubectl)    |-----------&gt;| ( Master )  |&lt;------+-----&gt; |     Node 2    |</span><br><span class="line">|                 |            |             |       |       +---------------+</span><br><span class="line">+-----------------+            |             |       |       </span><br><span class="line">                               |             |       |       +---------------+</span><br><span class="line">                               |             |       +-----&gt; |     Node 3    |</span><br><span class="line">                               |             |               +---------------+</span><br><span class="line">                               +-------------+</span><br></pre></td></tr></table></figure></div>

<p>左侧是一个官方提供的名为 <code>kubectl</code> 的 CLI （Command Line Interface）工具，用于使用 K8S 开放的 API 来管理集群和操作对象等。</p>
<p>右侧则是 K8S 集群的后端服务及开放出的 API 等。根据上一节的内容，我们知道 Node 是用于工作的机器，而 Master 是一种角色（Role），表示在这个 Node 上包含着管理集群的一些必要组件。具体组件的详细介绍参考第 11 小节对各组件的详细剖析。</p>
<p>当然在这里，只画出了一个 Master，在生产环境中，为了保障集群的高可用，我们通常会部署多个 Master 。</p>
<h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2><p>下面我们来逐层分解， 首先是 Master ，这里我们只介绍其管理集群的相关组件。Master 是整个 K8S 集群的“大脑”，与大脑类似，它有几个重要的功能：</p>
<ul>
<li>接收：外部的请求和集群内部的通知反馈</li>
<li>发布：对集群整体的调度和管理</li>
<li>存储：存储</li>
</ul>
<p>这些功能，也通过一些组件来共同完成，通常情况下，我们将其称为 control plane 。如下图所示：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>它主要包含以下几个重要的组成部分。</p>
<h3 id="Cluster-state-store"><a href="#Cluster-state-store" class="headerlink" title="Cluster state store"></a>Cluster state store</h3><p>存储集群所有需持久化的状态，并且提供 watch 的功能支持，可以快速的通知各组件的变更等操作。</p>
<p>因为目前 Kubernetes 的存储层选择是 etcd ，所以一般情况下，大家都直接以 etcd 来代表集群状态存储服务。即：将所有状态存储到 etcd 实例中。</p>
<p>刚才我们说 Master 相当于是 K8S 集群的大脑，更细化来看，etcd 则是大脑中的核心，为什么这么说？可以参考后面详细剖析的章节，本章我们先从更高的层次来看集群的整体架构。</p>
<p>你可能会问， etcd 是必须的吗？就目前而言，etcd 是必须的，这主要是 Kubernetes 的内部实现。</p>
<p>而早在 2014 年左右，社区就一直在提议将存储层抽象出来，后端的实际存储作为一种插件化的存在。<a href="https://github.com/kubernetes/kubernetes/issues/1957">呼声</a>比较大的是另一种提供 k/v 存储功能的 <a href="https://www.consul.io/" target="_blank" rel="noopener">Consul</a> 。</p>
<p>不过得益于 etcd 的开发团队较为活跃，而且根据 K8S 社区的反馈做了相当大的改进，并且当时 K8S 团队主要的关注点也不在此，所以直到现在 etcd 仍不是一个可选项。</p>
<p>如果现在去看下 Kubernetes 的源代码，你会发现存储层的代码还比较简洁清晰，后续如果有精力也许将此处插件化也不是不可能。</p>
<h3 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h3><p>这是整个集群的入口，类似于人体的感官，接收外部的信号和请求，并将一些信息写入到 etcd 中。</p>
<p>实际处理逻辑比三次握手简单的多：</p>
<ul>
<li>请求 API Server ：“嗨，我有些东西要放到 etcd 里面”</li>
<li>API Server 收到请求：“你是谁？我为啥要听你的”</li>
<li>从请求中，拿出自己的身份凭证（一般是证书）：“是我啊，你的master，给我把这些东西放进去”</li>
<li>这时候就要看是些什么内容了，如果这些内容 API Server 能理解，那就放入 etcd 中 “好的 master 我放进去了”；如果不能理解，“抱歉 master 我理解不了”</li>
</ul>
<p>可以看到，它提供了认证相关的功能，用于判断是否有权限进行操作。当然 API Server 支持多种认证方法，不过一般情况下，我们都使用 x509 证书进行认证。</p>
<p>API Server 的目标是成为一个极简的 server，只提供 REST 操作，更新 etcd ，并充当着集群的网关。至于其他的业务逻辑之类的，通过插件或者在其他组件中完成。关于这部分的详细实现，可以参考后面的 API Server 剖析相关章节。</p>
<h3 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h3><p>Controller Manager 大概是 K8S 集群中最繁忙的部分，它在后台运行着许多不同的控制器进程，用来调节集群的状态。</p>
<p>当集群的配置发生变更，控制器就会朝着预期的状态开始工作。</p>
<h3 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h3><p>顾名思义，Scheduler 是集群的调度器，它会持续的关注集群中未被调度的 Pod ，并根据各种条件，比如资源的可用性，节点的亲和性或者其他的一些限制条件，通过绑定的 API 将 Pod 调度/绑定到 Node 上。</p>
<p>在这个过程中，调度程序一般只考虑调度开始时， Node 的状态，而不考虑在调度过程中 Node 的状态变化 (比如节点亲和性等，截至到目前 v1.11.2 也暂未加入相关功能的稳定特性)</p>
<h2 id="Node-1"><a href="#Node-1" class="headerlink" title="Node"></a>Node</h2><p>Node 的概念我们在上节已经提过了，这里不再过多赘述，简单点理解为加入集群中的机器即可。</p>
<p>那 Node 是如何加入集群接受调度，并运行服务的呢？这都要归功于运行在 Node 上的几个核心组件。我们先来看下整体结构：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+--------------------------------------------------------+       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| |      kubelet        |        |     kube-proxy      | |       </span><br><span class="line">| |                     |        |                     | |       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">| | Container Runtime (Docker)                         | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| | |Pod                  |    |Pod                  | | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | | |C1   | |C2   |     |    ||C1   ||C2   ||C3   || | |       </span><br><span class="line">| | | |     | |     |     |    ||     ||     ||     || | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">+--------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<h3 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h3><p>Kubelet 实现了集群中最重要的关于 Node 和 Pod 的控制功能，如果没有 Kubelet 的存在，那 Kubernetes 很可能就只是一个纯粹的通过 API Server CRUD 的应用程序。</p>
<p>K8S 原生的执行模式是操作应用程序的容器，而不像传统模式那样，直接操作某个包或者是操作某个进程。基于这种模式，可以让应用程序之间相互隔离，互不影响。此外，由于是操作容器，所以应用程序可以说和主机也是相互隔离的，毕竟它不依赖于主机，在任何的容器运行时（比如 Docker）上都可以部署和运行。</p>
<p>我们在上节介绍过 Pod，Pod 可以是一组容器（也可以包含存储卷），K8S 将 Pod 作为可调度的基本单位， 分离开了构建时和部署时的关注点：</p>
<ul>
<li>构建时，重点关注某个容器是否能正确构建，如何快速构建</li>
<li>部署时，关心某个应用程序的服务是否可用，是否符合预期，依赖的相关资源是否都能访问到</li>
</ul>
<p>这种隔离的模式，可以很方便的将应用程序与底层的基础设施解耦，极大的提高集群扩/缩容，迁移的灵活性。</p>
<p>在前面，我们提到了 Master 节点的 <code>Scheduler</code> 组件，它会调度未绑定的 Pod 到符合条件的 Node 上，而至于最终该 Pod 是否能运行于 Node 上，则是由 <code>Kubelet</code> 来裁定的。关于 Kubelet 的具体原理，后面有详细剖析的章节。</p>
<h3 id="Container-runtime"><a href="#Container-runtime" class="headerlink" title="Container runtime"></a>Container runtime</h3><p>容器运行时最主要的功能是下载镜像和运行容器，我们最常见的实现可能是 <a href="https://www.docker.com/" target="_blank" rel="noopener">Docker</a> , 目前还有其他的一些实现，比如 <a href="https://github.com/rkt/rkt">rkt</a>, <a href="https://github.com/kubernetes-sigs/cri-o">cri-o</a>。</p>
<p>K8S 提供了一套通用的容器运行时接口 CRI (Container Runtime Interface), 凡是符合这套标准的容器运行时实现，均可在 K8S 上使用。</p>
<h3 id="Kube-Proxy"><a href="#Kube-Proxy" class="headerlink" title="Kube Proxy"></a>Kube Proxy</h3><p>我们都知道，想要访问某个服务，那要么通过域名，要么通过 IP。而每个 Pod 在创建后都会有一个虚拟 IP，K8S 中有一个抽象的概念，叫做 <code>Service</code> ，<code>kube-proxy</code> 便是提供一种代理的服务，让你可以通过 <code>Service</code> 访问到 Pod。</p>
<p>实际的工作原理是在每个 Node 上启动一个 <code>kube-proxy</code> 的进程，通过编排 <code>iptables</code> 规则来达到此效果。深入的解析，在后面有对应的章节。</p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>本节中，我们了解到了 K8S 的整体遵循 C/S 架构，集群的 Master 包含着几个重要的组成部分，比如 <code>API Server</code>, <code>Controller Manager</code> 等。</p>
<p>而 Node 上，则运行着三个必要的组件 <code>kubelet</code>, <code>container runtime</code> (一般是 Docker), <code>kube-proxy</code> 。</p>
<p>通过所有组件的分工协作，最终实现了 K8S 对容器的编排和调度。</p>
<p>完成了这节的学习，那我们就开始着手搭建一个属于我们自己的集群吧。</p>
<h1 id="搭建-Kubernetes-集群-本地快速搭建"><a href="#搭建-Kubernetes-集群-本地快速搭建" class="headerlink" title="搭建 Kubernetes 集群 - 本地快速搭建"></a>搭建 Kubernetes 集群 - 本地快速搭建</h1><p>通过之前的学习，我们已经知道了 K8S 中有一些组件是必须的，集群中有不同的角色。本节，我们在本地快速搭建一个集群，以加深我们学习到的东西。</p>
<h2 id="方案选择"><a href="#方案选择" class="headerlink" title="方案选择"></a>方案选择</h2><p>在上一节中，我们知道 K8S 中有多种功能组件，而这些组件要在本地全部搭建好，需要一些基础知识，以及在搭建过程中会浪费不少的时间，从而可能会影响我们正常的搭建集群的目标。</p>
<p>所以，我们这里提供两个最简单，最容易实现我们目标的工具</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kind/">KIND</a> 。</li>
<li><a href="https://github.com/kubernetes/minikube">Minikube</a> 。</li>
</ul>
<h2 id="KIND"><a href="#KIND" class="headerlink" title="KIND"></a>KIND</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>KIND（Kubernetes in Docker）是为了能提供更加简单，高效的方式来启动 K8S 集群，目前主要用于比如 <code>Kubernetes</code> 自身的 CI 环境中。</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ul>
<li>可以直接在项目的 <a href="https://github.com/kubernetes-sigs/kind/releases">Release 页面</a> 下载已经编译好的二进制文件。(下文中使用的是 v0.1.0 版本的二进制包)</li>
</ul>
<p>注意：如果不直接使用二进制包，而是使用 <code>go get sigs.k8s.io/kind</code> 的方式下载，则与下文中的配置文件不兼容。<strong>请参考<a href="https://zhuanlan.zhihu.com/p/60464867" target="_blank" rel="noopener">使用 Kind 搭建你的本地 Kubernetes 集群</a></strong> 这篇文章。</p>
<p>更新（2020年2月5日）：KIND 已经发布了 v0.7.0 版本，如果你想使用新版本，建议参考 <a href="https://zhuanlan.zhihu.com/p/105173589" target="_blank" rel="noopener">使用 Kind 在离线环境创建 K8S 集群</a> ，这篇文章使用了最新版本的 KIND。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/1/29/16898b9e3d57fcab?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><p><strong>在使用 KIND 之前，你需要本地先安装好 Docker 的环境</strong> ，此处暂不做展开。</p>
<p>由于网络问题，我们此处也需要写一个配置文件，以便让 <code>kind</code> 可以使用国内的镜像源。（KIND 最新版本中已经内置了所有需要的镜像，无需此操作）</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: kind.sigs.k8s.io&#x2F;v1alpha1</span><br><span class="line">kind: Config</span><br><span class="line"></span><br><span class="line">kubeadmConfigPatches:</span><br><span class="line">- |</span><br><span class="line">  apiVersion: kubeadm.k8s.io&#x2F;v1alpha3</span><br><span class="line">  kind: InitConfiguration</span><br><span class="line">  nodeRegistration:</span><br><span class="line">  kubeletExtraArgs:</span><br><span class="line">    pod-infra-container-image: registry.aliyuncs.com&#x2F;google_containers&#x2F;pause-amd64:3.1</span><br><span class="line">- |</span><br><span class="line">  apiVersion: kubeadm.k8s.io&#x2F;v1alpha3</span><br><span class="line">  kind: ClusterConfiguration</span><br><span class="line">  imageRepository: registry.aliyuncs.com&#x2F;google_containers</span><br><span class="line">  kubernetesVersion: v1.12.2</span><br><span class="line">  networking:</span><br><span class="line">    serviceSubnet: 10.0.0.0&#x2F;16</span><br></pre></td></tr></table></figure></div>

<p>将上面的内容保存成 <code>kind-config.yaml</code> 文件，执行以下命令即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">kind create cluster --image kindest&#x2F;node:v1.12.2 --config kind-config.yaml --name moelove</span><br></pre></td></tr></table></figure></div>

<p>下面为在我机器上执行的程序输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">(MoeLove) ➜  kind ✗ kind create cluster --image kindest&#x2F;node:v1.12.2 --config kind-config.yaml --name moelove                  </span><br><span class="line">Creating cluster &#39;kind-moelove&#39; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.12.2) 🖼</span><br><span class="line"> ✓ [kind-moelove-control-plane] Creating node container 📦                                                         </span><br><span class="line"> ✓ [kind-moelove-control-plane] Fixing mounts 🗻</span><br><span class="line"> ✓ [kind-moelove-control-plane] Starting systemd 🖥</span><br><span class="line"> ✓ [kind-moelove-control-plane] Waiting for docker to be ready 🐋                                                  </span><br><span class="line"> ✓ [kind-moelove-control-plane] Starting Kubernetes (this may take a minute) ☸                                     </span><br><span class="line">Cluster creation complete. You can now use the cluster with:</span><br><span class="line"></span><br><span class="line">export KUBECONFIG&#x3D;&quot;$(kind get kubeconfig-path --name&#x3D;&quot;moelove&quot;)&quot;</span><br><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure></div>

<p>这里，通过传递上面的 <code>kind-config.yaml</code> 文件给 <code>kind create cluster</code>, 并传递了一个名字通过 <code>--name</code> 参数。</p>
<p>我们按照程序输出的提示进行操作：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">export KUBECONFIG&#x3D;&quot;$(kind get kubeconfig-path --name&#x3D;&quot;moelove&quot;)&quot;</span><br><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure></div>

<p>下面为在我机器上执行的程序输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">(MoeLove) ➜  kind ✗ export KUBECONFIG&#x3D;&quot;$(kind get kubeconfig-path --name&#x3D;&quot;moelove&quot;)&quot;</span><br><span class="line">(MoeLove) ➜  kind ✗ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;localhost:35911</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;localhost:35911&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br><span class="line">(MoeLove) ➜  kind ✗ kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;12&quot;, GitVersion:&quot;v1.12.2&quot;, GitCommit:&quot;17c77c7898218073f14c8d573582e8d2313dc740&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-10-24T06:43:59Z&quot;, GoVersion:&quot;go1.10.4&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>注意，这里需要安装 <code>kubectl</code>。 <code>kubectl</code> 的安装可参考下面的内容。</p>
<p>当你执行 <code>kubectl cluster-info</code> 如果可以看到类似我上面的输出，那你本地的 K8S 集群就已经部署好了。你可以直接阅读第 5 节或者第 6 节的内容。</p>
<p>如果你已经对 K8S 有所了解，并且对 Dashboard 有比较强烈需求的话, 可直接参考第 20 节的内容。</p>
<h2 id="Minikube"><a href="#Minikube" class="headerlink" title="Minikube"></a>Minikube</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>Minikube 是 K8S 官方为了开发者能在个人电脑上运行 K8S 而提供的一套工具。实现上是通过 Go 语言编写，通过调用虚拟化管理程序，创建出一个运行在虚拟机内的单节点集群。</p>
<p>注：从这里也可以看出，对于 K8S 集群的基本功能而言，节点数并没有什么限制。只有一个节点同样可以创建集群。</p>
<h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><ul>
<li>首先需要确认 BIOS 已经开启了 <code>VT-x</code> 或者 <code>AMD-v</code> 虚拟化的支持。具体方式可参考 <a href="https://www.shaileshjha.com/how-to-find-out-if-intel-vt-x-or-amd-v-virtualization-technology-is-supported-in-windows-10-windows-8-windows-vista-or-windows-7-machine/" target="_blank" rel="noopener">确认是否已开启 BIOS 虚拟化</a>, <a href="https://www.howtogeek.com/213795/how-to-enable-intel-vt-x-in-your-computers-bios-or-uefi-firmware/" target="_blank" rel="noopener">开启 BIOS 虚拟化支持</a> 这两篇文章。</li>
<li>其次我们需要安装一个虚拟化管理程序，这里的选择可根据你实际在用的操作系统来决定。官方推荐如下:<ul>
<li>macOS: <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://www.vmware.com/products/fusion" target="_blank" rel="noopener">VMware Fusion</a> 或 <a href="https://github.com/moby/hyperkit">HyperKit</a>。如果使用 Hyperkit 需要注意系统必须是 <code>OS X 10.10.3 Yosemite</code> 及之后版本的。</li>
<li>Linux: <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="http://www.linux-kvm.org/" target="_blank" rel="noopener">KVM</a>。</li>
<li>Windows: <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://msdn.microsoft.com/en-us/virtualization/hyperv_on_windows/quick_start/walkthrough_install" target="_blank" rel="noopener">Hyper-V</a>。</li>
</ul>
</li>
</ul>
<p>我个人推荐无论你在以上哪种操作系统中使用 Minikube 都选择用 <code>Virtualbox</code> 作为虚拟化管理程序，1. Virtualbox 无论操作体验还是安装都比较简单 2. Minikube 对其支持更完备，并且也已经经过大量用户测试，相关问题均已基本修复。</p>
<p><em>如果你是在 Linux 系统上面，其实还有一个选择，便是将 Minikube 的 <code>--vm-driver</code> 参数设置为 <code>none</code> ，并且在本机已经正确安装 Docker。 这种方式是无需虚拟化支持的。</em></p>
<h3 id="安装-kubectl"><a href="#安装-kubectl" class="headerlink" title="安装 kubectl"></a>安装 kubectl</h3><p>上一节我们已经学到 K8S 集群是典型的 C/S 架构，有一个官方提供的名为 <code>kubectl</code> 的 CLI 工具。在这里，我们要安装 <code>kubectl</code> 以便后续我们可以对搭建好的集群进行管理。</p>
<p><strong>注：由于 API 版本兼容的问题，尽量保持 <code>kubectl</code> 版本与 K8S 集群版本保持一致，或版本相差在在一个小版本内。</strong></p>
<p>官方文档提供了 <code>macOS</code>, <code>Linux</code>, <code>Windows</code> 等操作系统上的安装方式，且描述很详细，这里不过多赘述，<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl" target="_blank" rel="noopener">文档地址</a>。</p>
<p><strong>此处提供一个不同于官方文档中的安装方式。</strong></p>
<ul>
<li>访问 K8S 主仓库的 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md">CHANGELOG 文件</a> 找到你所需要的版本。 由于我们将要使用的 Minikube 是官方最新的稳定版 v0.28.2，而它内置的 Kubernetes 的版本是 v1.10 所以，我们选择使用对应的 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md">1.10 版本</a>的 <code>kubectl</code>。当然，我们也可以通过传递参数的方式来创建不同版本的集群。如 <code>minikube start --kubernetes-version v1.11.3</code> 用此命令创建 <code>v1.11.3</code> 版本的集群，当然 <code>kubectl</code> 的版本也需要相应升级。</li>
</ul>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="687" height="538"></svg>)</p>
<p>点击<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#client-binaries">Client Binaries</a> 找到你符合所需系统架构的对应包下载即可。这里我以 <a href="https://dl.k8s.io/v1.10.7/kubernetes-client-linux-amd64.tar.gz" target="_blank" rel="noopener">Linux 下 64 位的包</a>为例。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  wget https:&#x2F;&#x2F;dl.k8s.io&#x2F;v1.10.7&#x2F;kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">➜  echo &#39;169b57c6707ed8d8be9643b0088631e5c0c6a37a5e99205f03c1199cd32bc61e  kubernetes-client-linux-amd64.tar.gz&#39; | sha256sum -c -</span><br><span class="line">kubernetes-client-linux-amd64.tar.gz: 成功</span><br><span class="line">➜  tar zxf kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">➜  sudo mv kubernetes&#x2F;client&#x2F;bin&#x2F;kubectl &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubectl</span><br><span class="line">➜  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubectl version --client</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;10&quot;, GitVersion:&quot;v1.10.7&quot;, GitCommit:&quot;0c38c362511b20a098d7cd855f1314dad92c2780&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-08-20T10:09:03Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>执行以上命令即可完成 <code>kubectl</code> 的安装，最后一步会看到当前安装的版本信息等。</p>
<h3 id="安装-Minikube"><a href="#安装-Minikube" class="headerlink" title="安装 Minikube"></a>安装 Minikube</h3><p>先查看 Minikube 的 <a href="https://github.com/kubernetes/minikube/releases">Release 页面</a>，当前最新的稳定版本是 v0.28.2，找到你所需系统的版本，点击下载，并将下载好的可执行文件加入你的 PATH 中。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="948" height="637"></svg>)</p>
<p><strong>注：当前 Windows 系统下的安装包还处于实验性质，如果你是在 Windows 环境下，同样是可以下载安装使用的</strong></p>
<p>以 Linux 下的安装为例：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  wget -O minikube https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;minikube&#x2F;releases&#x2F;download&#x2F;v0.28.2&#x2F;minikube-linux-amd64</span><br><span class="line">➜  chmod +x minikube</span><br><span class="line">➜  sudo mv minikube &#x2F;usr&#x2F;local&#x2F;bin&#x2F;minikube</span><br><span class="line"></span><br><span class="line">➜  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;minikube version</span><br><span class="line">minikube version: v0.28.2</span><br></pre></td></tr></table></figure></div>

<p>最后一步可查看当前已安装好的 Minikube 的版本信息。如果安装成功将会看到和我上面内容相同的结果。</p>
<h3 id="创建第一个-K8S-集群"><a href="#创建第一个-K8S-集群" class="headerlink" title="创建第一个 K8S 集群"></a>创建第一个 K8S 集群</h3><p>使用 Minikube 创建集群，只要简单的执行 <code>minikube start</code> 即可。正常情况下，你会看到和我类似的输出。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ minikube start</span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br><span class="line"></span><br><span class="line">➜  ~ minikube status</span><br><span class="line">minikube: Running</span><br><span class="line">cluster: Running</span><br><span class="line">kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100</span><br></pre></td></tr></table></figure></div>

<p>为了验证我们的集群目前是否均已配置正确，可以执行以下命令查看。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl cluster-info </span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;192.168.99.100:8443</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;192.168.99.100:8443&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br><span class="line"></span><br><span class="line">➜  ~ kubectl get nodes</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    1d       v1.10.0</span><br></pre></td></tr></table></figure></div>

<p>如果出现类似拒绝连接之类的提示，那可能是因为你的 <code>kubectl</code> 配置不正确，可查看 <code>$HOME/.kube/config</code> 文件检查配置。示例输出如下：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ cat .kube&#x2F;config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;ca.crt</span><br><span class="line">    server: https:&#x2F;&#x2F;192.168.99.100:8443</span><br><span class="line">  name: minikube</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: minikube</span><br><span class="line">    user: minikube</span><br><span class="line">  name: minikube</span><br><span class="line">current-context: minikube</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: minikube</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.crt</span><br><span class="line">    client-key: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.key</span><br></pre></td></tr></table></figure></div>

<p>如果没有该文件，可按上面示例内容进行创建，替换掉其中的路径及 <code>server</code> 地址配置。 <code>server</code> 地址可通过 <code>minikube status</code> 或者 <code>minikube ip</code> 查看或检查。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">(Tao) ➜  ~ minikube ip</span><br><span class="line">192.168.99.100</span><br><span class="line"></span><br><span class="line">(Tao) ➜  ~ minikube status</span><br><span class="line">minikube: Running</span><br><span class="line">cluster: Running</span><br><span class="line">kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100</span><br></pre></td></tr></table></figure></div>

<h3 id="通过-Dashboard-查看集群当前状态"><a href="#通过-Dashboard-查看集群当前状态" class="headerlink" title="通过 Dashboard 查看集群当前状态"></a>通过 Dashboard 查看集群当前状态</h3><p>使用 <code>Minikube</code> 的另一个好处在于，你可以不用关注太多安装方面的过程，直接在终端下输入 <code>minikube dashboard</code> 打开系统 Dashboard 并通过此来查看集群相关状态。</p>
<p>执行 <code>minikube dashboard</code> 后会自动打开浏览器，默认情况下监听在通过 <code>minikube ip</code> 所获得 IP 的 3000 端口。如下图所示：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/9/16/165de25c7c517d78?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接:"></a>相关链接:</h3><ul>
<li><a href="https://websiteforstudents.com/installing-virtualbox-ubuntu-17-04/" target="_blank" rel="noopener">安装 VirtualBox</a></li>
<li><a href="https://juejin.im/post/5c99ed6c6fb9a0710e47ebeb" target="_blank" rel="noopener">使用 Kind 搭建你的本地 Kubernetes 集群</a></li>
</ul>
<h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>本节中，我们为了能更快的体验到 K8S 集群，避免很多繁琐的安装步骤，我们选择了使用官方提供的 <code>Minikube</code> 工具来搭建一个本地集群。</p>
<p>Minikube 的本质其实是将一套 “定制化” 的 K8S 集群打包成 ISO 镜像，当执行 <code>minikube start</code> 的时候，便通过此镜像启动一个虚拟机，在此虚拟机上通过 <code>kubeadm</code> 工具来搭建一套只有一个节点的 K8S 集群。关于 <code>kubeadm</code> 工具，我们在下节进行讲解。</p>
<p>同时，会通过虚拟机的相关配置接口拿到刚才所启动虚拟机的地址信息等，并完成本地的 <code>kubectl</code> 工具的配置，以便于让用户通过 <code>kubectl</code> 工具对集群进行操作。</p>
<p>事实上，当前 Docker for Mac 17.12 CE Edge 和 Docker for Windows 18.02 CE Edge ，以及这两种平台更高的 Edge 版本, 均已内置了对 K8S 的支持，但均为 Edge 版本，此处暂不做过多介绍。</p>
<h1 id="动手实践：搭建一个-Kubernetes-集群-生产可用"><a href="#动手实践：搭建一个-Kubernetes-集群-生产可用" class="headerlink" title="动手实践：搭建一个 Kubernetes 集群 - 生产可用"></a>动手实践：搭建一个 Kubernetes 集群 - 生产可用</h1><p>通过上一节的学习，我们快速的使用 <code>Minikube</code> 搭建了一个本地可用的 K8S 集群。默认情况下，节点是一个虚拟机实例，我们可以在上面体验一些基本的功能。</p>
<p>大多数人的需求并不只是包含一个虚拟机节点的本地测试集群，而是一个可在服务器运行，可自行扩/缩容，具备全部功能的，达到生产可用的集群。</p>
<p>K8S 集群的搭建，一直让很多人头疼，本节我们来搭建一个生产可用的集群，便于后续的学习或使用。</p>
<h2 id="方案选择-1"><a href="#方案选择-1" class="headerlink" title="方案选择"></a>方案选择</h2><p>K8S 生产环境可用的集群方案有很多，本节我们选择一个 Kubernetes 官方推荐的方案 <code>kubeadm</code> 进行搭建。</p>
<p><code>kubeadm</code> 是 Kubernetes 官方提供的一个 CLI 工具，可以很方便的搭建一套符合官方最佳实践的最小化可用集群。当我们使用 <code>kubeadm</code> 搭建集群时，集群可以通过 K8S 的一致性测试，并且 <code>kubeadm</code> 还支持其他的集群生命周期功能，比如升级/降级等。</p>
<p>我们在此处选择 <code>kubeadm</code> ，因为我们可以不用过于关注集群的内部细节，便可以快速的搭建出生产可用的集群。我们可以通过后续章节的学习，快速上手 K8S ，并学习到 K8S 的内部原理。在此基础上，想要在物理机上完全一步步搭建集群，便轻而易举。</p>
<h2 id="安装基础组件"><a href="#安装基础组件" class="headerlink" title="安装基础组件"></a>安装基础组件</h2><h3 id="前期准备-1"><a href="#前期准备-1" class="headerlink" title="前期准备"></a>前期准备</h3><p>使用 <code>kubeadm</code> 前，我们需要提前做一些准备。</p>
<ul>
<li><p><strong>我们需要禁用 <code>swap</code></strong>。通过之前的学习，我们知道每个节点上都有个必须的组件，名为 <code>kubelet</code>，自 K8S 1.8 开始，启动 <code>kubelet</code> 时，需要禁用 <code>swap</code> 。或者需要更改 <code>kubelet</code> 的启动参数 <code>--fail-swap-on=false</code>。</p>
<p>虽说可以更改参数让其可用，但是我建议还是禁用 <code>swap</code> 除非你的集群有特殊的需求，比如：有大内存使用的需求，但又想节约成本；或者你知道你将要做什么，否则可能会出现一些非预期的情况，尤其是做了内存限制的时候，当某个 Pod 达到内存限制的时候，它可能会溢出到 swap 中，这会导致 K8S 无法正常进行调度。</p>
<p>如何禁用：</p>
<ol>
<li>使用 <code>sudo cat /proc/swaps</code> 验证 swap 配置的设备和文件。</li>
<li>通过 <code>swapoff -a</code> 关闭 swap 。</li>
<li>使用 <code>sudo blkid</code> 或者 <code>sudo lsblk</code> 可查看到我们的设备属性，请注意输出结果中带有 <code>swap</code> 字样的信息。</li>
<li>将 <code>/etc/fstab</code> 中和上一条命令中输出的，和 swap 相关的挂载点都删掉，以免在机器重启或重挂载时，再挂载 <code>swap</code> 分区。</li>
</ol>
<p>执行完上述操作，<code>swap</code> 便会被禁用，当然你也可以再次通过上述命令，或者 <code>free</code> 命令来确认是否还有 <code>swap</code> 存在。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# free </span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:        1882748       85608     1614836       16808      182304     1630476</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>通过 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 可查看机器的 <code>product_uuid</code> 请确保要搭建集群的所有节点的 <code>product_uuid</code> 均不相同。同时所有节点的 Mac 地址也不能相同，通过 <code>ip a</code> 或者 <code>ifconfig -a</code> 可进行查看。</p>
<p>我们在第二章提到过，每个 Node 都有一些信息会被记录进集群内，而此处我们需要保证的这些唯一的信息，便会记录在集群的 <code>nodeInfo</code> 中，比如 <code>product_uuid</code> 在集群内以 <code>systemUUID</code> 来表示。具体信息我们可以通过集群的 <code>API Server</code> 获取到，在后面的章节会详细讲述。</p>
</li>
<li><p>第三章中，我们已经谈过 K8S 是 C/S 架构，在启动后，会固定监听一些端口用于提供服务。可以通过 <code>sudo netstat -ntlp |grep -E &#39;6443|23[79,80]|1025[0,1,2]&#39;</code> 查看这些端口是否被占用，如果被占用，请手动释放。</p>
<p>如果你执行上述命令时，提示 <code>command not found</code>，则表明你需要先安装 <code>netstat</code>，在 CentOS 系统中需要通过 <code>sudo yum install net-tools</code> 安装，而在 Debian/Ubuntu 系统中，则需要通过 <code>sudo apt install net-tools</code> 进行安装。</p>
</li>
<li><p>前面我们也提到了，我们需要一个容器运行时，通常情况下是 <code>Docker</code>，我们可以通过<a href="https://docs.docker.com/install/overview/" target="_blank" rel="noopener">官方的 Docker 文档</a> 进行安装，安装完成后记得启动服务。</p>
<p>官方推荐使用 <code>17.03</code> ，但我建议你可以直接安装 <code>18.03</code> 或者更新的版本，因为 <code>17.03</code> 版本的 Docker 已经在 2018 年 3 月 <code>EOL</code>（End Of Life）了。对于更新版本的 Docker，虽然 K8S 尚未在新版本中经过大量测试，但毕竟新版本有很多 Bugfix 和新特性的增加，也能规避一些可能遇到的问题（比如个别情况下 container 不会自动删除的情况 (17.09) ）。</p>
<p>另外，由于 Docker 的 API 都是带有版本的，且有良好的兼容性，当使用低版本 API 请求时会自动降级，所以一般情况下也不会有什么问题。</p>
</li>
</ul>
<h3 id="安装-kubectl-1"><a href="#安装-kubectl-1" class="headerlink" title="安装 kubectl"></a>安装 kubectl</h3><p>第三章中，我们已经说过 <code>kubectl</code> 是集群的客户端，我们现在搭建集群时，也必须要安装它，用于验证集群功能。</p>
<p>安装步骤在第 4 章已经详细说明了，此处不做赘述，可查阅第 4 章或参考下面的内容。</p>
<h3 id="安装-kubeadm-和-kubelet"><a href="#安装-kubeadm-和-kubelet" class="headerlink" title="安装 kubeadm 和 kubelet"></a>安装 kubeadm 和 kubelet</h3><p>首先是版本的选择，我们可以通过下面的命令获取到当前的 stable 版本号。要访问这个地址，需要自行处理网络问题或使用我后面提供的解决办法。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# curl -sSL https:&#x2F;&#x2F;dl.k8s.io&#x2F;release&#x2F;stable.txt</span><br><span class="line">v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>下载二进制包，并通过 <code>kubeadm version</code> 验证版本是否正确。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# curl -sSL https:&#x2F;&#x2F;dl.k8s.io&#x2F;release&#x2F;v1.11.3&#x2F;bin&#x2F;linux&#x2F;amd64&#x2F;kubeadm &gt; &#x2F;usr&#x2F;bin&#x2F;kubeadm</span><br><span class="line">[root@master ~]# chmod a+rx &#x2F;usr&#x2F;bin&#x2F;kubeadm</span><br><span class="line">[root@master ~]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:59:42Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>当然，我们其实可以使用如同上一章的方式，直接进入到 <code>kubernetes</code> 的<a href="https://github.com/kubernetes/kubernetes">官方仓库</a>，找到我们所需版本 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#v1113">v1.11.3</a> 下载 <code>Server Binaries</code>，如下图：</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1019" height="670"></svg>)</p>
<p>终端下可使用如下方式下载：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master tmp]# wget -q https:&#x2F;&#x2F;dl.k8s.io&#x2F;v1.11.3&#x2F;kubernetes-server-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure></div>

<p><strong>对于国内用户，我已经准备了下面的方式，方便使用。</strong></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1FSEcEUplQQGsjyBIZ6j2fA 提取码: cu4s</span><br></pre></td></tr></table></figure></div>

<p>下载完成后，验证文件是否正确无误，验证通过后进行解压。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master tmp]# echo &#39;e49d0db1791555d73add107d2110d54487df538b35b9dde0c5590ac4c5e9e039 kubernetes-server-linux-amd64.tar.gz&#39; | sha256sum -c -</span><br><span class="line">kubernetes-server-linux-amd64.tar.gz: 确定</span><br><span class="line">[root@master tmp]# tar -zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">[root@master tmp]# ls kubernetes</span><br><span class="line">addons  kubernetes-src.tar.gz  LICENSES  server</span><br><span class="line">[root@master tmp]# ls kubernetes&#x2F;server&#x2F;bin&#x2F; | grep -E &#39;kubeadm|kubelet|kubectl&#39;</span><br><span class="line">kubeadm</span><br><span class="line">kubectl</span><br><span class="line">kubelet</span><br></pre></td></tr></table></figure></div>

<p>可以看到在 <code>server/bin/</code> 目录下有我们所需要的全部内容，将我们所需要的 <code>kubeadm</code> <code>kubectl</code> <code>kubelet</code> 等都移动至 <code>/usr/bin</code> 目录下。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master tmp]# mv kubernetes&#x2F;server&#x2F;bin&#x2F;kube&#123;adm,ctl,let&#125; &#x2F;usr&#x2F;bin&#x2F;</span><br><span class="line">[root@master tmp]# ls &#x2F;usr&#x2F;bin&#x2F;kube*</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;kubeadm  &#x2F;usr&#x2F;bin&#x2F;kubectl  &#x2F;usr&#x2F;bin&#x2F;kubelet</span><br><span class="line">[root@master tmp]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:59:42Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">[root@master tmp]# kubectl version --client</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">[root@master tmp]# kubelet --version</span><br><span class="line">Kubernetes v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到我们所需的组件，版本均为 <code>v1.11.3</code> 。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>为了在生产环境中保障各组件的稳定运行，同时也为了便于管理，我们增加对 <code>kubelet</code> 的 <code>systemd</code> 的配置，由 <code>systemd</code> 对服务进行管理。</p>
<h3 id="配置-kubelet"><a href="#配置-kubelet" class="headerlink" title="配置 kubelet"></a>配置 kubelet</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master tmp]# cat &lt;&lt;&#39;EOF&#39; &gt; &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;kubelet: The Kubernetes Agent</span><br><span class="line">Documentation&#x3D;http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;kubelet</span><br><span class="line">Restart&#x3D;always</span><br><span class="line">StartLimitInterval&#x3D;0</span><br><span class="line">RestartSec&#x3D;10</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br><span class="line">EOF</span><br><span class="line">[root@master tmp]# mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d</span><br><span class="line">[root@master tmp]# cat &lt;&lt;&#39;EOF&#39; &gt; &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment&#x3D;&quot;KUBELET_KUBECONFIG_ARGS&#x3D;--bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf&quot;</span><br><span class="line">Environment&#x3D;&quot;KUBELET_CONFIG_ARGS&#x3D;--config&#x3D;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">EnvironmentFile&#x3D;-&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env</span><br><span class="line">EnvironmentFile&#x3D;-&#x2F;etc&#x2F;default&#x2F;kubelet</span><br><span class="line">ExecStart&#x3D;</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS</span><br><span class="line">EOF</span><br><span class="line">[root@master tmp]# systemctl enable kubelet</span><br><span class="line">Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;kubelet.service to &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.</span><br></pre></td></tr></table></figure></div>

<p>在这里我们添加了 <code>kubelet</code> 的 systemd 配置，然后添加了它的 <code>Drop-in</code> 文件，我们增加的这个 <code>kubeadm.conf</code> 文件，会被 systemd 自动解析，用于复写 <code>kubelet</code> 的基础 systemd 配置，可以看到我们增加了一系列的配置参数。在第 17 章中，我们会对 <code>kubelet</code> 做详细剖析，到时再进行解释。</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>此时，我们的前期准备已经基本完成，可以使用 <code>kubeadm</code> 来创建集群了。别着急，在此之前，我们还需要安装两个工具，名为<code>crictl</code> 和 <code>socat</code>。</p>
<h3 id="安装前置依赖-crictl"><a href="#安装前置依赖-crictl" class="headerlink" title="安装前置依赖 crictl"></a>安装前置依赖 crictl</h3><p><code>crictl</code> 包含在 <a href="https://github.com/kubernetes-sigs/cri-tools.git"><code>cri-tools</code></a> 项目中，这个项目中包含两个工具：</p>
<ul>
<li><code>crictl</code> 是 <code>kubelet</code> CRI (Container Runtime Interface) 的 CLI 。</li>
<li><code>critest</code> 是 <code>kubelet</code> CRI 的测试工具集。</li>
</ul>
<p>安装可以通过进入 <code>cri-tools</code> 项目的 <a href="https://github.com/kubernetes-sigs/cri-tools/releases">Release 页面</a> ，根据项目 <a href="https://github.com/kubernetes-sigs/cri-tools#current-status">README</a> 文件中的版本兼容关系，选择自己所需的安装包，下载即可，由于我们安装 K8S 1.11.3 所以选择最新的 v1.11.x 的安装包。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="800" height="600"></svg>)</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# wget https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;cri-tools&#x2F;releases&#x2F;download&#x2F;v1.11.1&#x2F;crictl-v1.11.1-linux-amd64.tar.gz</span><br><span class="line">[root@master ~]# echo &#39;ccf83574556793ceb01717dc91c66b70f183c60c2bbec70283939aae8fdef768 crictl-v1.11.1-linux-amd64.tar.gz&#39; | sha256sum -c -</span><br><span class="line">crictl-v1.11.1-linux-amd64.tar.gz: 确定</span><br><span class="line">[root@master ~]# tar zxvf crictl-v1.11.1-linux-amd64.tar.gz</span><br><span class="line">[root@master ~]# mv crictl &#x2F;usr&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure></div>

<h3 id="安装前置依赖-socat"><a href="#安装前置依赖-socat" class="headerlink" title="安装前置依赖 socat"></a>安装前置依赖 socat</h3><p><code>socat</code> 是一款很强大的命令行工具，可以建立两个双向字节流并在其中传输数据。这么说你也许不太理解，简单点说，它其中的一个功能是可以实现端口转发。</p>
<p>无论在 K8S 中，还是在 Docker 中，如果我们需要在外部访问服务，端口转发是个必不可少的部分。当然，你可能会问基本上没有任何地方提到说 <code>socat</code> 是一个依赖项啊，别急，我们来看下<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockershim/docker_streaming.go#L189-L192"> K8S 的源码</a>便知道了。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func portForward(client libdocker.Interface, podSandboxID string, port int32, stream io.ReadWriteCloser) error &#123;</span><br><span class="line">    &#x2F;&#x2F; 省略了和 socat 无关的代码</span><br><span class="line"></span><br><span class="line">	socatPath, lookupErr :&#x3D; exec.LookPath(&quot;socat&quot;)</span><br><span class="line">	if lookupErr !&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;unable to do port forwarding: socat not found.&quot;)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	args :&#x3D; []string&#123;&quot;-t&quot;, fmt.Sprintf(&quot;%d&quot;, containerPid), &quot;-n&quot;, socatPath, &quot;-&quot;, fmt.Sprintf(&quot;TCP4:localhost:%d&quot;, port)&#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><code>socat</code> 的安装很简单 CentOS 下执行 <code>sudo yum install -y socat</code> ，Debian/Ubuntu 下执行 <code>sudo apt-get install -y socat</code> 即可完成安装。</p>
<h3 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h3><p>所有的准备工作已经完成，我们开始真正创建一个 K8S 集群。 <strong>注意：如果需要配置 Pod 网络方案，请先阅读本章最后的部分 <a href="https://juejin.im/book/5b9b2dc86fb9a05d0f16c8ac/section/5b9b8346f265da0af03375ed#配置集群网络" target="_blank" rel="noopener">配置集群网络</a></strong></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubeadm init                                                                                             </span><br><span class="line">[init] using Kubernetes version: v1.11.3               </span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">...</span><br><span class="line">I0920 01:09:09.602908   17966 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0920 01:09:09.603001   17966 kernel_validator.go:96] Validating kernel config</span><br><span class="line">        [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03</span><br><span class="line">[preflight&#x2F;images] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight&#x2F;images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight&#x2F;images] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">...</span><br><span class="line">[markmaster] Marking the node master as master by adding the label &quot;node-role.kubernetes.io&#x2F;master&#x3D;&#39;&#39;&quot;</span><br><span class="line">[markmaster] Marking the node master as master by adding the taints [node-role.kubernetes.io&#x2F;master:NoSchedule]</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 202.182.112.120:6443 --token t14kzc.vjurhx5k98dpzqdc --discovery-token-ca-cert-hash sha256:d64f7ce1af9f9c0c73d2d737fd0095456ad98a2816cb5527d55f984c8aa8a762</span><br></pre></td></tr></table></figure></div>

<p>以上省略了部分输出。</p>
<p>我们从以上日志中可以看到，创建集群时会检查内核版本，Docker 版本等信息，这里提示 Docker 版本较高，我们忽略这个提示。</p>
<p>然后会下载一些镜像，当然这里提示我们可以通过执行 <code>kubeadm config images pull</code> 提前去下载镜像。我们来看下</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubeadm config images pull</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-apiserver-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-controller-manager-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-scheduler-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-proxy-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;pause:3.1</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;etcd-amd64:3.2.18</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;coredns:1.1.3</span><br></pre></td></tr></table></figure></div>

<p>对于国内用户使用 <code>kubeadm</code> 创建集群时，可能遇到的问题便是这些镜像下载不下来，最终导致创建失败。所以我在国内的代码托管平台提供了一个<a href="https://gitee.com/K8S-release/kubeadm" target="_blank" rel="noopener">仓库</a> 可以 clone 该项目，进入 <code>v1.11.3</code> 目录，对每个 <code>tar</code> 文件执行 <code>sudo docker load -i xx.tar</code> 即可将镜像导入。</p>
<p>或者可使用<a href="https://dev.aliyun.com/list.html?namePrefix=google-containers" target="_blank" rel="noopener">阿里云提供的镜像</a>，只需要将 <code>k8s.gcr.io</code> 替换为 <code>registry.aliyuncs.com/google_containers</code> ，执行 <code>docker pull</code> 后再 <code>docker tag</code> 重 tag 即可。</p>
<p>继续看上面的日志，<code>kubeadm init</code> 执行起见生成了一些文件，而这些文件我们先前在 kubelet server 的 <code>Drop-in</code> 的配置中配置过。</p>
<p>生成这些配置文件后，将启动 <code>kubelet</code> 服务，生成一系列的证书和相关的配置之类的，并增加一些扩展。</p>
<p>最终集群创建成功，并提示可在任意机器上使用指定命令加入集群。</p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>在上面的步骤中，我们已经安装了 K8S 的 CLI 工具 <code>kubectl</code>，我们使用此工具查看集群信息：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl cluster-info</span><br><span class="line">Kubernetes master is running at http:&#x2F;&#x2F;localhost:8080</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure></div>

<p>使用 <code>kubectl cluster-info</code> 可查看集群 master 和集群服务的地址，但我们也注意到最后有一句报错 <code>connection ... refused</code> 很显然这里存在错误。</p>
<p><code>kubectl get nodes</code> 可查看集群中 <code>Node</code> 信息，同样报错。</p>
<p>在上面我们提到过，K8S 默认会监听一些端口，但并不是 <code>8080</code> 端口，由此可知，我们的 <code>kubectl</code> 配置有误。</p>
<h3 id="配置-kubectl"><a href="#配置-kubectl" class="headerlink" title="配置 kubectl"></a>配置 kubectl</h3><ul>
<li><p>使用 <code>kubectl</code> 的参数 <code>--kubeconfig</code> 或者环境变量 <code>KUBECONFIG</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl --kubeconfig &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf get nodes                                                </span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    13h       v1.11.3</span><br><span class="line">[root@master ~]#</span><br><span class="line">[root@master ~]# KUBECONFIG&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf kubectl get nodes                                                  </span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    13h       v1.11.3</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>使用传参的方式未免太繁琐，我们也可以更改默认配置文件</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# mkdir -p $HOME&#x2F;.kube</span><br><span class="line">[root@master ~]# sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@master ~]# sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@master ~]# kubectl get nodes                                                  </span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    13h       v1.11.3</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<h3 id="配置集群网络"><a href="#配置集群网络" class="headerlink" title="配置集群网络"></a>配置集群网络</h3><p>通过上面的配置，我们已经可以正常获取 <code>Node</code> 信息。但通过第 2 章，我们了解到 <code>Node</code> 都有 <code>status</code>，而此时我们唯一的 <code>Node</code> 是 <code>NotReady</code>。我们通过给 <code>kubectl</code> 传递 <code>-o</code> 参数更改输出格式，查看更详细的情况。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes -o yaml</span><br><span class="line">apiVersion: v1       </span><br><span class="line">items:                                       </span><br><span class="line">- apiVersion: v1                              </span><br><span class="line">  kind: Node</span><br><span class="line">  ...</span><br><span class="line">  status:</span><br><span class="line">    addresses:</span><br><span class="line">    - address: master</span><br><span class="line">      type: Hostname</span><br><span class="line">    ...</span><br><span class="line">    - lastHeartbeatTime: 2018-09-20T14:45:45Z</span><br><span class="line">      lastTransitionTime: 2018-09-20T01:09:48Z</span><br><span class="line">      message: &#39;runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady</span><br><span class="line">        message:docker: network plugin is not ready: cni config uninitialized&#39;</span><br><span class="line">      reason: KubeletNotReady</span><br><span class="line">      status: &quot;False&quot;</span><br><span class="line">      type: Ready</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></div>

<p>从以上输出中，我们可以看到 master 处于 <code>NotReady</code> 的原因是 <code>network plugin is not ready: cni config uninitialized</code> 那么 <code>CNI</code> 是什么呢？<code>CNI</code> 是 Container Network Interface 的缩写，是 K8S 用于配置 Linux 容器网络的接口规范。</p>
<p>关于网络的选择，我们此处不做过多介绍，我们暂时选择一个被广泛使用的方案 <code>flannel</code>。 但注意，如果要使用 <code>flannel</code> 需要在 <code>kubeadm init</code> 的时候，传递 <code>--pod-network-cidr=10.244.0.0/16</code> 参数。另外需要查看 <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> 是否已设置为 <code>1</code>。 可以通过 <code>sysctl net.bridge.bridge-nf-call-iptables=1</code> 更改配置。</p>
<p>我们在前面创建集群时，并没有传递任何参数。为了能使用 <code>flannel</code> , 所以我们需要先将集群重置。使用 <code>kubeadm reset</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubeadm reset </span><br><span class="line">[reset] WARNING: changes made to this host by &#39;kubeadm init&#39; or &#39;kubeadm join&#39; will be reverted.</span><br><span class="line">[reset] are you sure you want to proceed? [y&#x2F;N]: y</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">[reset] stopping the kubelet service</span><br><span class="line">[reset] unmounting mounted directories in &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&quot;</span><br><span class="line">[reset] removing kubernetes-managed containers</span><br><span class="line">[reset] cleaning up running containers using crictl with socket &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">[reset] failed to list running pods using crictl: exit status 1. Trying to use docker instead[reset] deleting contents of stateful directories: [&#x2F;var&#x2F;lib&#x2F;kubelet &#x2F;etc&#x2F;cni&#x2F;net.d &#x2F;var&#x2F;lib&#x2F;dockershim &#x2F;var&#x2F;run&#x2F;kubernetes &#x2F;var&#x2F;lib&#x2F;etcd]</span><br><span class="line">[reset] deleting contents of config directories: [&#x2F;etc&#x2F;kubernetes&#x2F;manifests &#x2F;etc&#x2F;kubernetes&#x2F;pki]</span><br><span class="line">[reset] deleting files: [&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf &#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf &#x2F;etc&#x2F;kubernetes&#x2F;scheduler.conf]</span><br></pre></td></tr></table></figure></div>

<p>重新初始化集群，并传递参数。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubeadm init --pod-network-cidr&#x3D;10.244.0.0&#x2F;16</span><br><span class="line">[init] using Kubernetes version: v1.11.3</span><br><span class="line">...</span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br></pre></td></tr></table></figure></div>

<p><strong>注意：这里会重新生成相应证书等配置，需要按上面的内容重新配置 kubectl。</strong></p>
<p>此时，<code>CNI</code> 也尚未初始化完成，我们还需完成以下的步骤。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># 注意，这里的 flannel 配置仅适用于 1.11 版本的 K8S，若安装其他版本的 K8S 需要替换掉此链接</span><br><span class="line">[root@master ~]# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;v0.10.0&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;flannel created</span><br><span class="line">serviceaccount&#x2F;flannel created</span><br><span class="line">configmap&#x2F;kube-flannel-cfg created</span><br><span class="line">daemonset.extensions&#x2F;kube-flannel-ds created</span><br></pre></td></tr></table></figure></div>

<p>稍等片刻，再次查看 Node 状态：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    12m       v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到 status 已经是 <code>Ready</code> 状态。根据第 3 章的内容，我们知道 K8S 中最小的调度单元是 <code>Pod</code> 我们来看下集群中现有 <code>Pod</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods --all-namespaces</span><br><span class="line">NAMESPACE     NAME                             READY     STATUS              RESTARTS   AGE </span><br><span class="line">kube-system   coredns-78fcdf6894-h7pkc         0&#x2F;1       ContainerCreating   0          12m</span><br><span class="line">kube-system   coredns-78fcdf6894-lhlks         0&#x2F;1       ContainerCreating   0          12m</span><br><span class="line">kube-system   etcd-master                      1&#x2F;1       Running             0          5m</span><br><span class="line">kube-system   kube-apiserver-master            1&#x2F;1       Running             0          5m</span><br><span class="line">kube-system   kube-controller-manager-master   1&#x2F;1       Running             0          5m</span><br><span class="line">kube-system   kube-flannel-ds-tqvck            1&#x2F;1       Running             0          6m</span><br><span class="line">kube-system   kube-proxy-25tk2                 1&#x2F;1       Running             0          12m</span><br><span class="line">kube-system   kube-scheduler-master            1&#x2F;1       Running             0          5m</span><br></pre></td></tr></table></figure></div>

<p>我们发现有两个 <code>coredns</code> 的 <code>Pod</code> 是 <code>ContainerCreating</code> 的状态，但并未就绪。根据第 3 章的内容，我们知道 <code>Pod</code> 实际会有一个调度过程，此处我们暂且不论，后续章节再对此进行解释。</p>
<h3 id="新增-Node"><a href="#新增-Node" class="headerlink" title="新增 Node"></a>新增 Node</h3><p>我们按照刚才执行完 <code>kubeadm init</code> 后，给出的信息，在新的机器上执行 <code>kubeadm join</code> 命令。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# kubeadm join 202.182.112.120:6443 --token t14kzc.vjurhx5k98dpzqdc --discovery-token-ca-cert-hash sha256:d64f7ce1af9f9c0c73d2d737fd0095456ad98a2816cb5527d55f984c8aa8a762</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line"> 1. Run &#39;modprobe -- &#39; to load missing kernel modules;</span><br><span class="line">2. Provide the missing builtin kernel ipvs support</span><br><span class="line"></span><br><span class="line">I0921 04:00:54.805439   10677 kernel_validator.go:81] Validating kernel version                                                  </span><br><span class="line">I0921 04:00:54.805604   10677 kernel_validator.go:96] Validating kernel config                                                   </span><br><span class="line">        [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03</span><br><span class="line">[discovery] Trying to connect to API Server &quot;202.182.112.120:6443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https:&#x2F;&#x2F;202.182.112.120:6443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https:&#x2F;&#x2F;202.182.112.120:6443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;202.182.112.120:6443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;202.182.112.120:6443&quot;</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.11&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;&#x2F;var&#x2F;run&#x2F;dockershim.sock&quot; to the Node API object &quot;node1&quot; as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</span><br></pre></td></tr></table></figure></div>

<p>上面的命令执行完成，提示已经成功加入集群。 此时，我们在 master 上查看下当前集群状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    12m       v1.11.3</span><br><span class="line">node1     Ready     &lt;none&gt;    7m        v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到 node1 已经加入了集群。</p>
<h2 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们选择官方推荐的 <code>kubeadm</code> 工具在服务器上搭建了一套有两个节点的集群。</p>
<p><code>kubeadm</code> 可以自动的拉取相关组件的 Docker 镜像，并将其“组织”起来，免去了我们逐个部署相关组件的麻烦。</p>
<p>我们首先学习到了部署 K8S 时需要对系统做的基础配置，其次安装了一些必要的工具，以便 K8S 的功能可正常运行。</p>
<p>其次，我们学习到了 CNI 相关的知识，并在集群中部署了 <code>flannel</code> 网络方案。</p>
<p>最后，我们学习了增加 Node 的方法，以便后续扩展集群。</p>
<p>集群搭建方面的学习暂时告一段落，但这并不是结束，这才是真正的开始，从下一章开始，我们要学习集群管理相关的内容，学习如何真正使用 K8S 。</p>
<h1 id="集群管理：初识-kubectl"><a href="#集群管理：初识-kubectl" class="headerlink" title="集群管理：初识 kubectl"></a>集群管理：初识 kubectl</h1><p>从本节开始，我们来学习 K8S 集群管理相关的知识。通过前面的学习，我们知道 K8S 遵循 C/S 架构，官方也提供了 CLI 工具 <code>kubectl</code> 用于完成大多数集群管理相关的功能。当然凡是你可以通过 <code>kubectl</code> 完成的与集群交互的功能，都可以直接通过 API 完成。</p>
<p>对于我们来说 <code>kubectl</code> 并不陌生，在第 3 章讲 K8S 整体架构时，我们首次提到了它。在第 4 章和第 5 章介绍了两种安装 <code>kubectl</code> 的方式故而本章不再赘述安装的部分。</p>
<h2 id="整体概览"><a href="#整体概览" class="headerlink" title="整体概览"></a>整体概览</h2><p>首先我们在终端下执行下 <code>kubectl</code>:</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl                </span><br><span class="line">kubectl controls the Kubernetes cluster manager.</span><br><span class="line">...</span><br><span class="line">Usage:</span><br><span class="line">  kubectl [flags] [options]</span><br></pre></td></tr></table></figure></div>

<p><code>kubectl</code> 已经将命令做了基本的归类，同时显示了其一般的用法 <code>kubectl [flags] [options]</code> 。</p>
<p>使用 <code>kubectl options</code> 可以看到所有全局可用的配置项。</p>
<h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><p>在我们的用户家目录，可以看到一个名为 <code>.kube/config</code> 的配置文件，我们来看下其中的内容（此处以本地的 minikube 集群为例）。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ ls $HOME&#x2F;.kube&#x2F;config </span><br><span class="line">&#x2F;home&#x2F;tao&#x2F;.kube&#x2F;config</span><br><span class="line">➜  ~ cat $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;ca.crt</span><br><span class="line">    server: https:&#x2F;&#x2F;192.168.99.101:8443</span><br><span class="line">  name: minikube</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: minikube</span><br><span class="line">    user: minikube</span><br><span class="line">  name: minikube</span><br><span class="line">current-context: minikube</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: minikube</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.crt</span><br><span class="line">    client-key: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.key</span><br></pre></td></tr></table></figure></div>

<p><code>$HOME/.kube/config</code> 中主要包含着：</p>
<ul>
<li>K8S 集群的 API 地址</li>
<li>用于认证的证书地址</li>
</ul>
<p>当然，我们在第 5 章时，也已经说过，也可以使用 <code>--kubeconfig</code> 或者环境变量 <code>KUBECONFIG</code> 来传递配置文件。</p>
<p>另外如果你并不想使用配置文件的话，你也可以通过使用直接传递相关参数来使用，例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl --client-key&#x3D;&#39;&#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.key&#39; --client-certificate&#x3D;&#39;&#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.crt&#39; --server&#x3D;&#39;https:&#x2F;&#x2F;192.168.99.101:8443&#39;  get nodes</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br></pre></td></tr></table></figure></div>

<h2 id="从-get-说起"><a href="#从-get-说起" class="headerlink" title="从 get 说起"></a>从 <code>get</code> 说起</h2><p>无论是第 4 章还是第 5 章，当我们创建集群后，我们都做了两个相同的事情，一个是执行 <code>kubectl get nodes</code> 另一个则是 <code>kubectl cluster-info</code>，我们先从查看集群内 <code>Node</code> 开始。</p>
<p>这里我们使用了一个本地已创建好的 <code>minikube</code> 集群。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get nodes</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br><span class="line">➜  ~ kubectl get node</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br><span class="line">➜  ~ kubectl get no</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到以上三种“名称”均可获取当前集群内 <code>Node</code> 信息。这是为了便于使用而增加的别名和缩写。</p>
<p>如果我们想要看到更详细的信息呢？可以通过传递 <code>-o</code> 参数以得到不同格式的输出。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get nodes -o wide </span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE            KERNEL-VERSION   CONTAINER-RUNTIME</span><br><span class="line">minikube   Ready     master    2d        v1.11.3   10.0.2.15     &lt;none&gt;        Buildroot 2018.05   4.15.0           docker:&#x2F;&#x2F;17.12.1-ce</span><br></pre></td></tr></table></figure></div>

<p>当然也可以传递 <code>-o yaml</code> 或者 <code>-o json</code> 得到更加详尽的信息。</p>
<p>使用 <code>-o json</code> 将内容以 JSON 格式输出时，可以配合 <a href="https://stedolan.github.io/jq/" target="_blank" rel="noopener"><code>jq</code></a> 进行内容提取。例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get nodes -o json | jq &quot;.items[] | &#123;name: .metadata.name&#125; + .status.nodeInfo&quot;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;minikube&quot;,</span><br><span class="line">  &quot;architecture&quot;: &quot;amd64&quot;,</span><br><span class="line">  &quot;bootID&quot;: &quot;d675d75b-e58e-40db-8910-6e5dda9e7cf9&quot;,</span><br><span class="line">  &quot;containerRuntimeVersion&quot;: &quot;docker:&#x2F;&#x2F;17.12.1-ce&quot;,</span><br><span class="line">  &quot;kernelVersion&quot;: &quot;4.15.0&quot;,</span><br><span class="line">  &quot;kubeProxyVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;kubeletVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;machineID&quot;: &quot;078e2d22629747178397e29cf1c96cc7&quot;,</span><br><span class="line">  &quot;operatingSystem&quot;: &quot;linux&quot;,</span><br><span class="line">  &quot;osImage&quot;: &quot;Buildroot 2018.05&quot;,</span><br><span class="line">  &quot;systemUUID&quot;: &quot;4073906D-69A1-46EE-A08C-0252D9F79893&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>以此方法可得到 <code>Node</code> 的基础信息。</p>
<p>那么除了 <code>Node</code> 外我们还可以查看那些资源或别名呢？可以通过 <code>kubectl api-resources</code> 查看服务端支持的 API 资源及别名和描述等信息。</p>
<h2 id="答疑解惑-explain"><a href="#答疑解惑-explain" class="headerlink" title="答疑解惑 explain"></a>答疑解惑 <code>explain</code></h2><p>当通过上面的命令拿到所有支持的 API 资源列表后，虽然后面基本都有一个简单的说明，是不是仍然感觉一头雾水？</p>
<p>别担心，在我们使用 Linux 的时候，我们有 <code>man</code> ，在使用 <code>kubectl</code> 的时候，我们除了 <code>--help</code> 外还有 <code>explain</code> 可帮我们进行说明。 例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl explain node                                                                                                              </span><br><span class="line">KIND:     Node</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">DESCRIPTION:              </span><br><span class="line">     Node is a worker node in Kubernetes. Each node will have a unique</span><br><span class="line">     identifier in the cache (i.e. in etcd).</span><br><span class="line"></span><br><span class="line"># ... 省略输出</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h2><p>本节我们大致介绍了 <code>kubectl</code> 的基础使用，尤其是最常见的 <code>get</code> 命令。可通过传递不同参数获取不同格式的输出，配合 <code>jq</code> 工具可方便的进行内容提取。</p>
<p>以及关于 <code>kubectl</code> 的配置文件和无配置文件下通过传递参数直接使用等。</p>
<p>对应于我们前面提到的 K8S 架构，本节相当于 <code>CURD</code> 中的 <code>R</code> 即查询。查询对于我们来说，既是我们了解集群的第一步，同时也是后续验证操作结果或集群状态必不可少的技能。</p>
<p>当然，你在集群管理中可能会遇到各种各样的问题，单纯依靠 <code>get</code> 并不足以定位问题，我们在第 21 节中将介绍 Troubleshoot 的思路及方法。</p>
<p>下节我们来学习关于 <code>C</code> 的部分，即创建。</p>
<h1 id="集群管理：以-Redis-为例-部署及访问"><a href="#集群管理：以-Redis-为例-部署及访问" class="headerlink" title="集群管理：以 Redis 为例-部署及访问"></a>集群管理：以 Redis 为例-部署及访问</h1><p>上节我们已经学习了 <code>kubectl</code> 的基础使用，本节我们使用 <code>kubectl</code> 在 K8S 中进行部署。</p>
<p><strong>前面我们已经说过，Pod 是 K8S 中最小的调度单元，所以我们无法直接在 K8S 中运行一个 container 但是我们可以运行一个 Pod 而这个 Pod 中只包含一个 container 。</strong></p>
<h2 id="从-kubectl-run-开始"><a href="#从-kubectl-run-开始" class="headerlink" title="从 kubectl run 开始"></a>从 <code>kubectl run</code> 开始</h2><p><code>kubectl run</code> 的基础用法如下：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">Usage:</span><br><span class="line">  kubectl run NAME --image&#x3D;image [--env&#x3D;&quot;key&#x3D;value&quot;] [--port&#x3D;port] [--replicas&#x3D;replicas] [--dry-run&#x3D;bool] [--overrides&#x3D;inline-json] [--command] -- [COMMAND] [args...] [options]</span><br></pre></td></tr></table></figure></div>

<p><code>NAME</code> 和 <code>--image</code> 是必需项。分别代表此次部署的名字及所使用的镜像，其余部分之后进行解释。当然，在我们实际使用时，推荐编写配置文件并通过 <code>kubectl create</code> 进行部署。</p>
<h2 id="使用最小的-Redis-镜像"><a href="#使用最小的-Redis-镜像" class="headerlink" title="使用最小的 Redis 镜像"></a>使用最小的 Redis 镜像</h2><p>在 Redis 的<a href="https://hub.docker.com/_/redis/" target="_blank" rel="noopener">官方镜像列表</a>可以看到有很多的 tag 可供选择，其中使用 <a href="https://alpinelinux.org/" target="_blank" rel="noopener">Alpine Linux</a> 作为基础的镜像体积最小，下载较为方便。我们选择 <code>redis:alpine</code> 这个镜像进行部署。</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>现在我们只部署一个 Redis 实例。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl run redis --image&#x3D;&#39;redis:alpine&#39;</span><br><span class="line">deployment.apps&#x2F;redis created</span><br></pre></td></tr></table></figure></div>

<p>可以看到提示 <code>deployment.apps/redis created</code> 这个稍后进行解释，我们使用 <code>kubectl get all</code> 来看看到底发生了什么。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get all</span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-7c7545cbcb-2m6rp   1&#x2F;1       Running   0          30s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   32s</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            1           30s</span><br><span class="line"></span><br><span class="line">NAME                               DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-7c7545cbcb   1         1         1         30s</span><br></pre></td></tr></table></figure></div>

<p>可以看到其中有我们刚才执行 <code>run</code> 操作后创建的 <code>deployment.apps/redis</code>，还有 <code>replicaset.apps/redis-7c7545cbcb</code>, <code>service/kubernetes</code> 以及 <code>pod/redis-7c7545cbcb-f984p</code>。</p>
<p>使用 <code>kubectl get all</code> 输出内容的格式 <code>/</code> 前代表类型，<code>/</code> 后是名称。</p>
<p>这些分别代表什么含义？</p>
<h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p><code>Deployment</code> 是一种高级别的抽象，允许我们进行扩容，滚动更新及降级等操作。我们使用 <code>kubectl run redis --image=&#39;redis:alpine</code> 命令便创建了一个名为 <code>redis</code> 的 <code>Deployment</code>，并指定了其使用的镜像为 <code>redis:alpine</code>。</p>
<p>同时 K8S 会默认为其增加一些标签（<code>Label</code>）。我们可以通过更改 <code>get</code> 的输出格式进行查看。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get deployment.apps&#x2F;redis -o wide </span><br><span class="line">NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">redis     1         1         1            1           40s       redis        redis:alpine   run&#x3D;redis</span><br><span class="line">➜  ~ kubectl get deploy redis -o wide          </span><br><span class="line">NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">redis     1         1         1            1           40s       redis        redis:alpine   run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>那么这些 <code>Label</code> 有什么作用呢？它们可作为选择条件进行使用。如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get deploy -l run&#x3D;redis -o wide </span><br><span class="line">NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">redis     1         1         1            1           11h       redis        redis:alpine   run&#x3D;redis</span><br><span class="line">➜  ~ kubectl get deploy -l run&#x3D;test -o wide  # 由于我们并没有创建过 test 所以查不到任何东西</span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure></div>

<p>我们在应用部署或更新时总是会考虑的一个问题是如何平滑升级，利用 <code>Deployment</code> 也能很方便的进行金丝雀发布（Canary deployments）。这主要也依赖 <code>Label</code> 和 <code>Selector</code>， 后面我们再详细介绍如何实现。</p>
<p><code>Deployment</code> 的创建除了使用我们这里提到的方式外，更推荐的方式便是使用 <code>yaml</code> 格式的配置文件。在配置文件中主要是声明一种预期的状态，而其他组件则负责协同调度并最终达成这种预期的状态。当然这也是它的关键作用之一，将 <code>Pod</code> 托管给下面将要介绍的 <code>ReplicaSet</code>。</p>
<h3 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h3><p><code>ReplicaSet</code> 是一种较低级别的结构，允许进行扩容。</p>
<p>我们上面已经提到 <code>Deployment</code> 主要是声明一种预期的状态，并且会将 <code>Pod</code> 托管给 <code>ReplicaSet</code>，而 <code>ReplicaSet</code> 则会去检查当前的 <code>Pod</code> 数量及状态是否符合预期，并尽量满足这一预期。</p>
<p><code>ReplicaSet</code> 可以由我们自行创建，但一般情况下不推荐这样去做，因为如果这样做了，那其实就相当于跳过了 <code>Deployment</code> 的部分，<code>Deployment</code> 所带来的功能或者特性我们便都使用不到了。</p>
<p>除了 <code>ReplicaSet</code> 外，我们还有一个选择名为 <code>ReplicationController</code>，这两者的主要区别更多的在选择器上，我们后面再做讨论。现在推荐的做法是 <code>ReplicaSet</code> 所以不做太多解释。</p>
<p><code>ReplicaSet</code> 可简写为 <code>rs</code>，通过以下命令查看：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get rs -o wide</span><br><span class="line">NAME               DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES         SELECTOR                           </span><br><span class="line">redis-7c7545cbcb   1         1         1         11h       redis        redis:alpine   pod-template-hash&#x3D;3731017676,run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>在输出结果中，我们注意到这里除了我们前面看到的 <code>run=redis</code> 标签外，还多了一个 <code>pod-template-hash=3731017676</code> 标签，这个标签是由 <code>Deployment controller</code> 自动添加的，目的是为了防止出现重复，所以将 <code>pod-template</code> 进行 hash 用作唯一性标识。</p>
<h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p><code>Service</code> 简单点说就是为了能有个稳定的入口访问我们的应用服务或者是一组 <code>Pod</code>。通过 <code>Service</code> 可以很方便的实现服务发现和负载均衡。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get service -o wide</span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE       SELECTOR</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   16m        &lt;none&gt;</span><br></pre></td></tr></table></figure></div>

<p>通过使用 <code>kubectl</code> 查看，能看到主要会显示 <code>Service</code> 的名称，类型，IP，端口及创建时间和选择器等。我们来具体拆解下。</p>
<h4 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h4><p><code>Service</code> 目前有 4 种类型：</p>
<ul>
<li><code>ClusterIP</code>： 是 K8S 当前默认的 <code>Service</code> 类型。将 service 暴露于一个仅集群内可访问的虚拟 IP 上。</li>
<li><code>NodePort</code>： 是通过在集群内所有 <code>Node</code> 上都绑定固定端口的方式将服务暴露出来，这样便可以通过 <code>:</code> 访问服务了。</li>
<li><code>LoadBalancer</code>： 是通过 <code>Cloud Provider</code> 创建一个外部的负载均衡器，将服务暴露出来，并且会自动创建外部负载均衡器路由请求所需的 <code>Nodeport</code> 或 <code>ClusterIP</code> 。</li>
<li><code>ExternalName</code>： 是通过将服务由 DNS CNAME 的方式转发到指定的域名上将服务暴露出来，这需要 <code>kube-dns</code> 1.7 或更高版本支持。</li>
</ul>
<h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><p>上面已经说完了 <code>Service</code> 的基本类型，而我们也已经部署了一个 Redis ,当还无法访问到该服务，接下来我们将刚才部署的 Redis 服务暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl expose deploy&#x2F;redis --port&#x3D;6379 --protocol&#x3D;TCP --target-port&#x3D;6379 --name&#x3D;redis-server  </span><br><span class="line">service&#x2F;redis-server exposed</span><br><span class="line">➜  ~ kubectl get svc -o wide                                                                       </span><br><span class="line">NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR</span><br><span class="line">kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443&#x2F;TCP    49m       &lt;none&gt;</span><br><span class="line">redis-server   ClusterIP   10.108.105.63   &lt;none&gt;        6379&#x2F;TCP   4s        run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>通过 <code>kubectl expose</code> 命令将 redis server 暴露出来，这里需要进行下说明：</p>
<ul>
<li><code>port</code>： 是 <code>Service</code> 暴露出来的端口，可通过此端口访问 <code>Service</code>。</li>
<li><code>protocol</code>： 是所用协议。当前 K8S 支持 TCP/UDP 协议，在 1.12 版本中实验性的加入了对 <a href="https://zh.wikipedia.org/zh-hans/流控制传输协议" target="_blank" rel="noopener">SCTP 协议</a>的支持。默认是 TCP 协议。</li>
<li><code>target-port</code>： 是实际服务所在的目标端口，请求由 <code>port</code> 进入通过上述指定 <code>protocol</code> 最终流向这里配置的端口。</li>
<li><code>name</code>： <code>Service</code> 的名字，它的用处主要在 dns 方面。</li>
<li><code>type</code>： 是前面提到的类型，如果没指定默认是 <code>ClusterIP</code>。</li>
</ul>
<p>现在我们的 redis 是使用的默认类型 <code>ClusterIP</code>，所以并不能直接通过外部进行访问，我们使用 <code>port-forward</code> 的方式让它可在集群外部访问。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl port-forward svc&#x2F;redis-server 6379:6379</span><br><span class="line">Forwarding from 127.0.0.1:6379 -&gt; 6379</span><br><span class="line">Forwarding from [::1]:6379 -&gt; 6379</span><br><span class="line">Handling connection for 6379</span><br></pre></td></tr></table></figure></div>

<p>在另一个本地终端内可通过 redis-cli 工具进行连接：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ redis-cli -h 127.0.0.1 -p 6379</span><br><span class="line">127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure></div>

<p>当然，我们也可以使用 <code>NodePort</code> 的方式对外暴露服务。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl expose deploy&#x2F;redis --port&#x3D;6379 --protocol&#x3D;TCP --target-port&#x3D;6379 --name&#x3D;redis-server-nodeport --type&#x3D;NodePort</span><br><span class="line">service&#x2F;redis-server-nodeport exposed</span><br><span class="line">➜  ~ kubectl get service&#x2F;redis-server-nodeport -o wide </span><br><span class="line">NAME                    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE       SELECTOR</span><br><span class="line">redis-server-nodeport   NodePort   10.109.248.204   &lt;none&gt;        6379:31913&#x2F;TCP   11s       run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>我们可以通过任意 <code>Node</code> 上的 31913 端口便可连接我们的 redis 服务。当然，这里需要注意的是这个端口范围其实是可以通过 <code>kube-apiserver</code> 的 <code>service-node-port-range</code> 进行配置的，默认是 <code>30000-32767</code>。</p>
<h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>第二节中，我们提到过 <code>Pod</code> 是 K8S 中的最小化部署单元。我们看下当前集群中 <code>Pod</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get pods</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">redis-7c7545cbcb-jwcf2   1&#x2F;1       Running   0          8h</span><br></pre></td></tr></table></figure></div>

<p>我们进行一次简单的扩容操作。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl scale deploy&#x2F;redis --replicas&#x3D;2</span><br><span class="line">deployment.extensions&#x2F;redis scaled</span><br><span class="line">➜  ~ kubectl get pods</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">redis-7c7545cbcb-jwcf2   1&#x2F;1       Running   0          8h</span><br><span class="line">redis-7c7545cbcb-wzh6w   1&#x2F;1       Running   0          4s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 数已经增加，并且也已经是 <code>Running</code> 的状态了。(当然在生产环境中 Redis 服务的扩容并不是使用这种方式进行扩容的，需要看实际的部署方式以及业务的使用姿势。)</p>
<h2 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h2><p>本节我们使用 Redis 作为例子，学习了集群管理相关的基础知识。学习了如何进行应用部署， <code>Service</code> 的基础类型以及如何通过 <code>port-forward</code> 或 <code>NodePort</code> 等方式将服务提供至集群的外部访问。</p>
<p>同时我们学习了应用部署中主要会涉及到的几类资源 <code>Deployment</code>，<code>Replicaset</code>，<code>Service</code> 和 <code>Pod</code> 等。对这些资源及它们之间关系的掌握，对于后续集群维护或定位问题有很大的帮助。</p>
<p>下节，我们开始学习在生产环境中使用 K8S 至关重要的一环，权限控制。</p>
<h1 id="安全重点-认证和授权"><a href="#安全重点-认证和授权" class="headerlink" title="安全重点: 认证和授权"></a>安全重点: 认证和授权</h1><p>本节我们将开始学习将 K8S 应用于生产环境中至关重要的一环，权限控制。当然，不仅是 K8S 对于任何应用于生产环境中的系统，权限管理或者说访问控制都是很重要的。</p>
<h2 id="整体概览-1"><a href="#整体概览-1" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们已经知道 K8S 中几乎所有的操作都需要经过 <code>kube-apiserver</code> 处理，所以为了安全起见，K8S 为它提供了三类安全访问的措施。分别是：用于识别用户身份的认证（Authentication），用于控制用户对资源访问的授权（Authorization）以及用于资源管理方面的准入控制（Admission Control）。</p>
<p>下面的图基本展示了这一过程。来自客户端的请求分别经过认证，授权，准入控制之后，才能真正执行。</p>
<p>当然，这里说<strong>基本展示</strong>是因为我们可以直接通过 <code>kubectl proxy</code> 的方式直接通过 HTTP 请求访问 <code>kube-apiserver</code> 而无需任何认证过程。</p>
<p>另外，也可通过在 <code>kube-apiserver</code> 所启动的机器上，直接访问启动时 <code>--insecure-port</code> 参数配置的端口进行绕过认证和授权，默认是 8080。为了避免安全问题，也可将此参数设置为 0 以规避问题。注意：这个参数和 <code>--insecure-bind-address</code> 都已过期，并将在未来的版本移除。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------------------------------------+</span><br><span class="line">|                                                                                                           |</span><br><span class="line">|               +---------------------------------------------------------------------------+    +--------+ |</span><br><span class="line">|               |                                                                           |    |        | |</span><br><span class="line">| +--------+    |   +------------------+   +----------------+   +--------------+   +------+ |    |        | |</span><br><span class="line">| |        |    |   |                  |   |                |   | Admission    |   |      | |    |        | |</span><br><span class="line">| | Client +------&gt; | Authentication   +-&gt; | Authorization  +-&gt; | Control      +-&gt; |Logic | +--&gt; | Others | |</span><br><span class="line">| |        |    |   |                  |   |                |   |              |   |      | |    |        | |</span><br><span class="line">| +--------+    |   +------------------+   +----------------+   +--------------+   +------+ |    |        | |</span><br><span class="line">|               |                                                                           |    |        | |</span><br><span class="line">|               |                                                                           |    |        | |</span><br><span class="line">|               |                          Kube-apiserver                                   |    |        | |</span><br><span class="line">|               +---------------------------------------------------------------------------+    +--------+ |</span><br><span class="line">|                                                                                                           |</span><br><span class="line">+-----------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<h2 id="认证（Authentication）"><a href="#认证（Authentication）" class="headerlink" title="认证（Authentication）"></a>认证（Authentication）</h2><p>认证，无非是判断当前发起请求的用户身份是否正确。例如，我们通常登录服务器时候需要输入用户名和密码，或者 SSH Keys 之类的。</p>
<p>在讲认证前，我们应该先理一下 K8S 中的用户。</p>
<p>K8S 中有两类用户，一般用户及 <code>Service Account</code>。</p>
<ul>
<li>一般用户：一般用户只能通过外部服务进行管理，由管理员进行私钥分发。这也意味着 K8S 中并没有任何表示一般用户的对象，所以一般用户是无法通过 API 直接添加到集群的。</li>
<li><code>Service Account</code>：由 K8S API 管理的用户，与特定的 <code>NameSpace</code>（命名空间）绑定。由 <code>API Server</code> 自动创建或者通过 API 手动进行创建。 同时，它会自动挂载到 <code>Pod</code> 中容器的 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code> 目录中，其中会包含 <code>NameSpace</code> <code>token</code> 等信息，并允许集群内进程与 <code>API Server</code> 进行交互。</li>
</ul>
<p>对集群操作的 API 都是与用户相关联的，或者被视为匿名请求。匿名请求可通过 <code>kube-apiserver</code> 的 <code>--anonymous-auth</code> 参数进行控制，默认是开启的，匿名用户默认的用户名为 <code>system:anonymous</code>，所属组为 <code>system:unauthenticated</code>。</p>
<p>理完 K8S 中的用户，我们来看下 K8S 中的认证机制。</p>
<p>K8S 支持以下认证机制：</p>
<ul>
<li>X509 客户端证书：这个认证机制我们并不陌生，我们前面搭建集群时，虽然没有指定配置文件，但 <code>kubeadm</code> 已经添加了默认参数 <code>--client-ca-file=/etc/kubernetes/pki/ca.crt</code> 而在进行认证时，将会使用客户端证书 subject 的 <code>CN</code> 域（Common Name）用作用户名，<code>O</code> 域（Organization）用作组名。</li>
<li>引导 Token：这个我们也不会陌生，前面我们搭建集群时，当集群通过 <code>kubeadm init</code> 初始化完成后，将会展示一行提示，其中便携带着引导 Token。如果不使用 <code>kubeadm</code> 时，需要设置 <code>--enable-bootstrap-token-auth=true</code>。</li>
<li>静态 Token 文件：启动 <code>Kube-apiserver</code> 时，设置 <code>--token-auth-file=SOMEFILE</code> 并在请求时，加上 <code>Authorization: Bearer TOKEN</code> 的请求头即可。</li>
<li>静态密码文件：与静态 Token 文件类似，设置 <code>--basic-auth-file=SOMEFILE</code> 并在请求时，加上 <code>Authorization: Basic BASE64ENCODED(USER:PASSWORD)</code> 的头即可。</li>
<li>Service Account Token：这是默认启用的机制，关于 <code>Service Account</code> 前面也已经介绍过了，不再赘述。</li>
<li>OpenID：其实是提供了 <a href="http://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html" target="_blank" rel="noopener">OAuth2</a> 的认证支持，像 Azure 或 Google 这类云厂商都提供了相关支持。</li>
<li>认证代理：主要是配合身份验证代理进行使用，比如提供一个通用的授权网关供用户使用。</li>
<li>Webhook：提供 Webhook 配合一个远端服务器使用。</li>
</ul>
<p>可选择同时开启多个认证机制。比如当我们使用 <code>kubeadm</code> 创建集群时，默认便会开启 X509 客户端证书和引导 Token 等认证机制。</p>
<h2 id="授权（Authorization）"><a href="#授权（Authorization）" class="headerlink" title="授权（Authorization）"></a>授权（Authorization）</h2><p>授权，也就是在验证当前发起请求的用户是否有相关的权限。例如，我们在 Linux 系统中常见的文件夹权限之类的。</p>
<p>授权是以认证的结果为基础的，授权机制检查用户通过认证后的请求中所包含的属性来进行判断。</p>
<p>K8S 支持多种授权机制，用户想要正确操作资源，则必须获得授权，所以 K8S 默认情况下的权限都是拒绝。当某种授权机制通过或者拒绝后，便会立即返回，不再去请求其他的授权机制；当所有授权机制都未通过时便会返回 403 错误了。</p>
<p>K8S 支持以下授权机制：</p>
<ul>
<li>ABAC(Attribute-Based Access Control)：基于属性的访问控制，在使用时需要先配置 <code>--authorization-mode=ABAC</code> 和 <code>--authorization-policy-file=SOME_FILENAME</code> 。ABAC 本身设计是非常好的，但是在 K8S 中使用却有点过于繁琐，这里不再赘述。</li>
<li>RBAC(Role-based access control)：基于角色的访问控制，自 K8S 1.6 开始 beta，1.8 进入稳定版，已被大量使用。而当我们使用 <code>kubeadm</code> 安装集群的时候，默认将会添加 <code>--authorization-mode=Node,RBAC</code> 的参数，表示同时开启 <code>Node</code> 和 <code>RBAC</code> 授权机制。当然，如果你对 <a href="https://www.mongodb.com/cn" target="_blank" rel="noopener">MongoDB</a> 有所了解或者比较熟悉的话，这部分的内容就会很容易理解，因为 MongoDB 的权限控制也使用了 <code>RBAC</code> （Role-based access control）。</li>
<li>Node：这是一种特殊用途的授权机制，专门用于对 <code>kubelet</code> 发出的 API 请求做授权验证。</li>
<li>Webhook：使用外部的 Server 通过 API 进行授权校验，需要在启动时候增加 <code>--authorization-webhook-config-file=SOME_FILENAME</code> 以及 <code>--authorization-mode=Webhook</code></li>
<li>AlwaysAllow：默认配置，允许全部。</li>
<li>AlwaysDeny：通常用于测试，禁止全部。</li>
</ul>
<h2 id="角色（Role）"><a href="#角色（Role）" class="headerlink" title="角色（Role）"></a>角色（Role）</h2><p>上面提到了 <code>RBAC</code>，为了能更好的理解，我们需要先认识下 K8S 中的角色。K8S 中的角色从类别上主要有两类，<code>Role</code> 和 <code>ClusterRole</code>。</p>
<ul>
<li><code>Role</code>：可以当作是一组权限的集合，但被限制在某个 <code>Namespace</code> 内（K8S 的 <code>Namespace</code>）。</li>
<li><code>ClusterRole</code>：对于集群级别的资源是不被 <code>Namespace</code> 所限制的，并且还有一些非资源类的请求，所以便产生了它。</li>
</ul>
<p>当已经了解到角色后，剩下给用户授权也就只是需要做一次绑定即可。在 K8S 中将这一过程称之为 binding，即 <code>rolebinding</code> 和 <code>clusterrolebinding</code>。 我们来看下集群刚初始化后的情况：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get roles --all-namespaces&#x3D;true</span><br><span class="line">NAMESPACE     NAME                                             AGE</span><br><span class="line">kube-public   kubeadm:bootstrap-signer-clusterinfo             1h</span><br><span class="line">kube-public   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   extension-apiserver-authentication-reader        1h</span><br><span class="line">kube-system   kube-proxy                                       1h</span><br><span class="line">kube-system   kubeadm:kubelet-config-1.12                      1h</span><br><span class="line">kube-system   kubeadm:nodes-kubeadm-config                     1h</span><br><span class="line">kube-system   system::leader-locking-kube-controller-manager   1h</span><br><span class="line">kube-system   system::leader-locking-kube-scheduler            1h</span><br><span class="line">kube-system   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   system:controller:cloud-provider                 1h</span><br><span class="line">kube-system   system:controller:token-cleaner                  1h</span><br><span class="line">➜  ~ kubectl get rolebindings --all-namespaces&#x3D;true</span><br><span class="line">NAMESPACE     NAME                                             AGE</span><br><span class="line">kube-public   kubeadm:bootstrap-signer-clusterinfo             1h</span><br><span class="line">kube-public   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   kube-proxy                                       1h</span><br><span class="line">kube-system   kubeadm:kubelet-config-1.12                      1h</span><br><span class="line">kube-system   kubeadm:nodes-kubeadm-config                     1h</span><br><span class="line">kube-system   system::leader-locking-kube-controller-manager   1h</span><br><span class="line">kube-system   system::leader-locking-kube-scheduler            1h</span><br><span class="line">kube-system   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   system:controller:cloud-provider                 1h</span><br><span class="line">kube-system   system:controller:token-cleaner                  1h</span><br></pre></td></tr></table></figure></div>

<p>可以看到默认已经存在了一些 <code>role</code> 和 <code>rolebindings</code>。 对于这部分暂且不做过多说明，我们来看下对于集群全局有效的 <code>ClusterRole</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get clusterroles</span><br><span class="line">NAME                                                                   AGE</span><br><span class="line">admin                                                                  1h</span><br><span class="line">cluster-admin                                                          1h</span><br><span class="line">edit                                                                   1h</span><br><span class="line">flannel                                                                1h</span><br><span class="line">system:aggregate-to-admin                                              1h</span><br><span class="line">system:aggregate-to-edit                                               1h</span><br><span class="line">system:aggregate-to-view                                               1h</span><br><span class="line">system:auth-delegator                                                  1h</span><br><span class="line">system:aws-cloud-provider                                              1h</span><br><span class="line">system:basic-user                                                      1h</span><br><span class="line">system:certificates.k8s.io:certificatesigningrequests:nodeclient       1h</span><br><span class="line">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   1h</span><br><span class="line">system:controller:attachdetach-controller                              1h</span><br><span class="line">system:controller:certificate-controller                               1h</span><br><span class="line">system:controller:clusterrole-aggregation-controller                   1h</span><br><span class="line">system:controller:cronjob-controller                                   1h</span><br><span class="line">system:controller:daemon-set-controller                                1h</span><br><span class="line">system:controller:deployment-controller                                1h</span><br><span class="line">system:controller:disruption-controller                                1h</span><br><span class="line">system:controller:endpoint-controller                                  1h</span><br><span class="line">system:controller:expand-controller                                    1h</span><br><span class="line">system:controller:generic-garbage-collector                            1h</span><br><span class="line">system:controller:horizontal-pod-autoscaler                            1h</span><br><span class="line">system:controller:job-controller                                       1h</span><br><span class="line">system:controller:namespace-controller                                 1h</span><br><span class="line">system:controller:node-controller                                      1h</span><br><span class="line">system:controller:persistent-volume-binder                             1h</span><br><span class="line">system:controller:pod-garbage-collector                                1h</span><br><span class="line">system:controller:pv-protection-controller                             1h</span><br><span class="line">system:controller:pvc-protection-controller                            1h</span><br><span class="line">system:controller:replicaset-controller                                1h</span><br><span class="line">system:controller:replication-controller                               1h</span><br><span class="line">system:controller:resourcequota-controller                             1h</span><br><span class="line">system:controller:route-controller                                     1h</span><br><span class="line">system:controller:service-account-controller                           1h</span><br><span class="line">system:controller:service-controller                                   1h</span><br><span class="line">system:controller:statefulset-controller                               1h</span><br><span class="line">system:controller:ttl-controller                                       1h</span><br><span class="line">system:coredns                                                         1h</span><br><span class="line">system:csi-external-attacher                                           1h</span><br><span class="line">system:csi-external-provisioner                                        1h</span><br><span class="line">system:discovery                                                       1h</span><br><span class="line">system:heapster                                                        1h</span><br><span class="line">system:kube-aggregator                                                 1h</span><br><span class="line">system:kube-controller-manager                                         1h</span><br><span class="line">system:kube-dns                                                        1h</span><br><span class="line">system:kube-scheduler                                                  1h</span><br><span class="line">system:kubelet-api-admin                                               1h</span><br><span class="line">system:node                                                            1h</span><br><span class="line">system:node-bootstrapper                                               1h</span><br><span class="line">system:node-problem-detector                                           1h</span><br><span class="line">system:node-proxier                                                    1h</span><br><span class="line">system:persistent-volume-provisioner                                   1h</span><br><span class="line">system:volume-scheduler                                                1h</span><br><span class="line">view                                                                   1h</span><br><span class="line">➜  ~ kubectl get clusterrolebindings</span><br><span class="line">NAME                                                   AGE</span><br><span class="line">cluster-admin                                          1h</span><br><span class="line">flannel                                                1h</span><br><span class="line">kubeadm:kubelet-bootstrap                              1h</span><br><span class="line">kubeadm:node-autoapprove-bootstrap                     1h</span><br><span class="line">kubeadm:node-autoapprove-certificate-rotation          1h</span><br><span class="line">kubeadm:node-proxier                                   1h</span><br><span class="line">system:aws-cloud-provider                              1h</span><br><span class="line">system:basic-user                                      1h</span><br><span class="line">system:controller:attachdetach-controller              1h</span><br><span class="line">system:controller:certificate-controller               1h</span><br><span class="line">system:controller:clusterrole-aggregation-controller   1h</span><br><span class="line">system:controller:cronjob-controller                   1h</span><br><span class="line">system:controller:daemon-set-controller                1h</span><br><span class="line">system:controller:deployment-controller                1h</span><br><span class="line">system:controller:disruption-controller                1h</span><br><span class="line">system:controller:endpoint-controller                  1h</span><br><span class="line">system:controller:expand-controller                    1h</span><br><span class="line">system:controller:generic-garbage-collector            1h</span><br><span class="line">system:controller:horizontal-pod-autoscaler            1h</span><br><span class="line">system:controller:job-controller                       1h</span><br><span class="line">system:controller:namespace-controller                 1h</span><br><span class="line">system:controller:node-controller                      1h</span><br><span class="line">system:controller:persistent-volume-binder             1h</span><br><span class="line">system:controller:pod-garbage-collector                1h</span><br><span class="line">system:controller:pv-protection-controller             1h</span><br><span class="line">system:controller:pvc-protection-controller            1h</span><br><span class="line">system:controller:replicaset-controller                1h</span><br><span class="line">system:controller:replication-controller               1h</span><br><span class="line">system:controller:resourcequota-controller             1h</span><br><span class="line">system:controller:route-controller                     1h</span><br><span class="line">system:controller:service-account-controller           1h</span><br><span class="line">system:controller:service-controller                   1h</span><br><span class="line">system:controller:statefulset-controller               1h</span><br><span class="line">system:controller:ttl-controller                       1h</span><br><span class="line">system:coredns                                         1h</span><br><span class="line">system:discovery                                       1h</span><br><span class="line">system:kube-controller-manager                         1h</span><br><span class="line">system:kube-dns                                        1h</span><br><span class="line">system:kube-scheduler                                  1h</span><br><span class="line">system:node                                            1h</span><br><span class="line">system:node-proxier                                    1h</span><br><span class="line">system:volume-scheduler                                1h</span><br></pre></td></tr></table></figure></div>

<p>可以看到 K8S 中默认已经有很多的 <code>ClusterRole</code> 和 <code>clusterrolebindings</code> 了，我们选择其中一个做下探究。</p>
<h2 id="查看用户权限"><a href="#查看用户权限" class="headerlink" title="查看用户权限"></a>查看用户权限</h2><p>我们一直都在使用 <code>kubectl</code> 对集群进行操作，那么当前用户是什么权限呢？ 对应于 <code>RBAC</code> 中又是什么情况呢？</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl config current-context   # 获取当前上下文</span><br><span class="line">kubernetes-admin@kubernetes  # 名为 kubernetes-admin 的用户，在名为 kubernetes 的 cluster 上</span><br><span class="line">➜  ~ kubectl config view users -o yaml  # 查看 user 配置，以下省略了部分内容</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:                                   </span><br><span class="line">- cluster:                            </span><br><span class="line">    ...</span><br><span class="line">contexts:                    </span><br><span class="line">- context:  </span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kubernetes-admin</span><br><span class="line">  name: kubernetes-admin@kubernetes</span><br><span class="line">current-context: kubernetes-admin@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kubernetes-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: REDACTED</span><br><span class="line">    client-key-data: REDACTED</span><br></pre></td></tr></table></figure></div>

<p><code>client-certificate-data</code> 的部分默认是不显示的，而它的<strong>内容实际是通过 <code>base64</code> 加密后的证书内容</strong>。我们可以通过通过以下方式进行查看</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl config view users --raw -o jsonpath&#x3D;&#39;&#123; .users[?(@.name &#x3D;&#x3D; &quot;kubernetes-admin&quot;)].user.client-certificate-data&#125;&#39; |base64 -d  </span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIIC8jCCAdqgAwIBAgIIGuC27C9B8LIwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE</span><br><span class="line">...</span><br><span class="line">kae1A&#x2F;d4D5Cm5Qt7M5gr3SxqE5t+O7DP0YhuEPlfY7RzYDksYa8&#x3D;</span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line">➜  ~ kubectl config view users --raw -o jsonpath&#x3D;&#39;&#123; .users[?(@.name &#x3D;&#x3D; &quot;kubernetes-admin&quot;)].user.client-certificate-data&#125;&#39; |base64 -d |openssl x509 -text -noout  # 限于篇幅 省略部分输出</span><br><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 3 (0x2)</span><br><span class="line">        Serial Number: 1936748965290700978 (0x1ae0b6ec2f41f0b2)</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: CN&#x3D;kubernetes</span><br><span class="line">        Subject: O&#x3D;system:masters, CN&#x3D;kubernetes-admin</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></div>

<p>根据前面认证部分的内容，我们知道当前的用户是 <code>kubernetes-admin</code> （CN 域），所属组是 <code>system:masters</code> （O 域） 。</p>
<p>我们看下 <code>clusterrolebindings</code> 中的 <code>cluster-admin</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get clusterrolebindings  cluster-admin  -o yaml                         </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io&#x2F;autoupdate: &quot;true&quot;</span><br><span class="line">  ...</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io&#x2F;bootstrapping: rbac-defaults</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  resourceVersion: &quot;116&quot;</span><br><span class="line">  selfLink: &#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterrolebindings&#x2F;cluster-admin</span><br><span class="line">  uid: 71c550f1-e0e4-11e8-866a-fa163e938a99</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:masters</span><br></pre></td></tr></table></figure></div>

<p>重点内容在 <code>roleRef</code> 和 <code>subjects</code> 中，名为 <code>cluster-admin</code> 的 <code>ClusterRole</code> 与名为 <code>system:masters</code> 的 <code>Group</code> 相绑定。我们继续探究下它们所代表的含义。</p>
<p>先看看这个 <code>ClusterRole</code> 的实际内容：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl get clusterrole cluster-admin -o yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io&#x2F;autoupdate: &quot;true&quot;</span><br><span class="line">  ...</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io&#x2F;bootstrapping: rbac-defaults</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  resourceVersion: &quot;58&quot;</span><br><span class="line">  selfLink: &#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterroles&#x2F;cluster-admin</span><br><span class="line">  uid: 71307108-e0e4-11e8-866a-fa163e938a99</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">  resources:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">  verbs:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">- nonResourceURLs:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">  verbs:</span><br><span class="line">  - &#39;*&#39;</span><br></pre></td></tr></table></figure></div>

<p><code>rules</code> 中定义了它所能操作的资源及对应动作，<code>*</code> 是通配符。</p>
<p>到这里，我们就可以得出结论了，当前用户 <code>kubernetes-admin</code> 属于 <code>system:masters</code> 组，而这个组与 <code>cluster-admin</code> 这个 <code>ClusterRole</code> 所绑定，所以用户也就继承了其权限。具备了对多种资源和 API 的相关操作权限。</p>
<h2 id="实践：创建权限可控的用户"><a href="#实践：创建权限可控的用户" class="headerlink" title="实践：创建权限可控的用户"></a>实践：创建权限可控的用户</h2><p>前面是通过实际用户来反推它所具备的权限，接下来我们开始实践的部分，创建用户并为它进行授权。</p>
<p>我们要创建的用户名为 <code>backend</code> 所属组为 <code>dev</code>。</p>
<h3 id="创建-NameSpace"><a href="#创建-NameSpace" class="headerlink" title="创建 NameSpace"></a>创建 NameSpace</h3><p>为了演示，这里创建一个新的 <code>NameSpace</code> ，名为 <code>work</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl create namespace work</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">➜  ~ kubectl get ns work</span><br><span class="line">NAME   STATUS   AGE</span><br><span class="line">work   Active   14s</span><br></pre></td></tr></table></figure></div>

<h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><h4 id="创建私钥"><a href="#创建私钥" class="headerlink" title="创建私钥"></a>创建私钥</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ mkdir work</span><br><span class="line">➜  ~ cd work</span><br><span class="line">➜  work openssl genrsa -out backend.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">..........................................+++                            </span><br><span class="line">........................+++                   </span><br><span class="line">e is 65537 (0x10001)            </span><br><span class="line">➜  work ls                                       </span><br><span class="line">backend.key </span><br><span class="line"></span><br><span class="line">➜  work cat backend.key                                              </span><br><span class="line">-----BEGIN RSA PRIVATE KEY-----                                          </span><br><span class="line">MIIEpAIBAAKCAQEAzk7blZthwSzachPxrk6pHsuaImTVh6Iw8mNDmtn6sqOqBfZS</span><br><span class="line">...</span><br><span class="line">bNKDWDk8HZREugaOAwjt7xaWOlr9SPCCoXrWoaA1z2215IC4qSA2Nw&#x3D;&#x3D;</span><br><span class="line">-----END RSA PRIVATE KEY-----</span><br></pre></td></tr></table></figure></div>

<h4 id="使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定-subject-信息，传递用户名和组名"><a href="#使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定-subject-信息，传递用户名和组名" class="headerlink" title="使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定 subject 信息，传递用户名和组名"></a>使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定 <code>subject</code> 信息，传递用户名和组名</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work openssl req -new -key backend.key -out backend.csr -subj &quot;&#x2F;CN&#x3D;backend&#x2F;O&#x3D;dev&quot;</span><br><span class="line">➜  work ls</span><br><span class="line">backend.csr  backend.key</span><br><span class="line">➜  work cat backend.csr</span><br><span class="line">-----BEGIN CERTIFICATE REQUEST-----</span><br><span class="line">MIICZTCCAU0CAQAwIDEQMA4GA1UEAwwHYmFja2VuZDEMMAoGA1UECgwDZGV2MIIB</span><br><span class="line">...</span><br><span class="line">lpoSVlNA0trJoiEiZjUqMfXX6ogBhQC4aeRfmbXkW2ZCNxsIm3PDk1Y&#x3D;</span><br><span class="line">-----END CERTIFICATE REQUEST-----</span><br></pre></td></tr></table></figure></div>

<h4 id="使用-CA-进行签名。K8S-默认的证书目录为-etc-kubernetes-pki。"><a href="#使用-CA-进行签名。K8S-默认的证书目录为-etc-kubernetes-pki。" class="headerlink" title="使用 CA 进行签名。K8S 默认的证书目录为 /etc/kubernetes/pki。"></a>使用 CA 进行签名。K8S 默认的证书目录为 <code>/etc/kubernetes/pki</code>。</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work openssl x509 -req -in backend.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -out backend.crt -days 365</span><br><span class="line">Signature ok</span><br><span class="line">subject&#x3D;&#x2F;CN&#x3D;backend&#x2F;O&#x3D;dev</span><br><span class="line">Getting CA Private Key</span><br><span class="line">➜  work ls</span><br><span class="line">backend.crt  backend.csr  backend.key</span><br></pre></td></tr></table></figure></div>

<p>查看生成的证书文件</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work openssl x509 -in backend.crt -text -noout</span><br><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 1 (0x0)</span><br><span class="line">        Serial Number:</span><br><span class="line">            d9:7f:62:f7:38:66:2a:7b</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: CN&#x3D;kubernetes</span><br><span class="line">        Subject: CN&#x3D;backend, O&#x3D;dev</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>CN</code> 域和 <code>O</code> 域已经正确设置</p>
<h4 id="添加-context"><a href="#添加-context" class="headerlink" title="添加 context"></a>添加 context</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work kubectl config set-credentials backend --client-certificate&#x3D;&#x2F;root&#x2F;work&#x2F;backend.crt  --client-key&#x3D;&#x2F;root&#x2F;work&#x2F;backend.key</span><br><span class="line">User &quot;backend&quot; set.</span><br><span class="line">➜  work kubectl config set-context backend-context --cluster&#x3D;kubernetes --namespace&#x3D;work --user&#x3D;backend</span><br><span class="line">Context &quot;backend-context&quot; created.</span><br></pre></td></tr></table></figure></div>

<h4 id="使用新用户测试访问"><a href="#使用新用户测试访问" class="headerlink" title="使用新用户测试访问"></a>使用新用户测试访问</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work kubectl --context&#x3D;backend-context get pods</span><br><span class="line">Error from server (Forbidden): pods is forbidden: User &quot;backend&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;work&quot;</span><br><span class="line"># 可能看得不够清楚，我们添加 &#96;-v&#96; 参数来显示详情</span><br><span class="line">➜  work kubectl --context&#x3D;backend-context get pods -n work -v 5</span><br><span class="line">I1109 05:35:11.870639   18626 helpers.go:201] server response object: [&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;&#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;pods is forbidden: User \&quot;backend\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;work\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;pods&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;]</span><br><span class="line">F1109 05:35:11.870688   18626 helpers.go:119] Error from server (Forbidden): pods is forbidden: User &quot;backend&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;work&quot;</span><br></pre></td></tr></table></figure></div>

<p>可以看到已经使用了新的 <code>backend</code> 用户，并且默认的 <code>Namespace</code> 设置成了 <code>work</code>。</p>
<h4 id="创建-Role"><a href="#创建-Role" class="headerlink" title="创建 Role"></a>创建 Role</h4><p>我们想要让这个用户只具备查看 <code>Pod</code> 的权限。先来创建一个配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work cat &lt;&lt;EOF &gt; backend-role.yaml </span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: work</span><br><span class="line">  name: backend-role</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></div>

<p>创建并查看已生成的 <code>Role</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work kubectl create -f backend-role.yaml </span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;backend-role created</span><br><span class="line">➜  work kubectl get roles  -n work -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">  kind: Role</span><br><span class="line">  metadata:</span><br><span class="line">    ...</span><br><span class="line">    name: backend-role</span><br><span class="line">    namespace: work</span><br><span class="line">    selfLink: &#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;namespaces&#x2F;work&#x2F;roles&#x2F;backend-role</span><br><span class="line">  rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">    - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">    - pods</span><br><span class="line">    verbs:</span><br><span class="line">    - get</span><br><span class="line">    - list</span><br><span class="line">    - watch</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure></div>

<h4 id="创建-Rolebinding"><a href="#创建-Rolebinding" class="headerlink" title="创建 Rolebinding"></a>创建 Rolebinding</h4><p>先创建一个配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work cat &lt;&lt;EOF &gt; backend-rolebind.yaml</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: backend-rolebinding          </span><br><span class="line">  namespace: work</span><br><span class="line">subjects:      </span><br><span class="line">- kind: User</span><br><span class="line">  name: backend</span><br><span class="line">  apiGroup: &quot;&quot;     </span><br><span class="line">roleRef:    </span><br><span class="line">  kind: Role </span><br><span class="line">  name: backend-role</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></div>

<p>创建并查看已生成的 Rolebinding 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work kubectl create -f backend-rolebind.yaml</span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;backend-rolebinding created</span><br><span class="line">➜  work kubectl get rolebinding -o yaml -n work</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">  kind: RoleBinding</span><br><span class="line">  metadata:</span><br><span class="line">    name: backend-rolebinding</span><br><span class="line">    namespace: work</span><br><span class="line">    ...</span><br><span class="line">  roleRef:</span><br><span class="line">    apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: Role</span><br><span class="line">    name: backend-role</span><br><span class="line">  subjects:</span><br><span class="line">  - apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: User</span><br><span class="line">    name: backend</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure></div>

<h4 id="测试用户权限"><a href="#测试用户权限" class="headerlink" title="测试用户权限"></a>测试用户权限</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work kubectl --context&#x3D;backend-context get pods -n work</span><br><span class="line">No resources found.</span><br><span class="line">➜  work kubectl --context&#x3D;backend-context get ns</span><br><span class="line">Error from server (Forbidden): namespaces is forbidden: User &quot;backend&quot; cannot list resource &quot;namespaces&quot; in API group &quot;&quot; at the cluster scope</span><br><span class="line">➜  work kubectl --context&#x3D;backend-context get deploy -n work</span><br><span class="line">Error from server (Forbidden): deployments.extensions is forbidden: User &quot;backend&quot; cannot list resource &quot;deployments&quot; in API group &quot;extensions&quot; in the namespace &quot;work&quot;</span><br></pre></td></tr></table></figure></div>

<p>可以看到用户已经具备查看 <code>Pod</code> 的权限，但并不能查看 <code>Namespace</code> 或者 <code>deployment</code> 等其他资源。当然，K8S 也内置了一种很方便的调试机制。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  work kubectl auth can-i list pods -n work --as&#x3D;&quot;backend&quot;</span><br><span class="line">yes</span><br><span class="line">➜  work kubectl auth can-i list deploy -n work --as&#x3D;&quot;backend&quot;</span><br><span class="line">no - no RBAC policy matched</span><br></pre></td></tr></table></figure></div>

<p><code>--as</code> 是一种建立在 K8S 认证机制之上的机制，可以便于系统管理员验证授权情况，或进行调试。</p>
<p>你也可以仿照 <code>~/.kube/config</code> 文件的内容，将当前生成的证书及私钥文件等写入到配置文件中，通过指定 <code>KUBECONFIG</code> 的环境变量，或者给 <code>kubectl</code> 传递 <code>--kubeconfig</code> 参数来使用。</p>
<h2 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h2><p>本节中，我们学习了 K8S 的认证及授权逻辑，K8S 支持多种认证及授权模式，可按需使用。通过 X509 客户端证书认证的方式使用比较方便也比较推荐，在客户端证书的 <code>CN</code> 域和 <code>O</code> 域可以指定用户名和所属组名。</p>
<p>RBAC 的授权模式现在使用最多，可以通过对 <code>Role</code> 和 <code>subjects</code> (可以是用户或组) 进行绑定，以达到授权的目的。</p>
<p>最后，我们实际新创建了一个用户，并对其授予了预期的权限。在此过程中也涉及到了 <code>openssl</code> 客户端的常规操作，在之后也会常常用到。</p>
<p>下节，我们将开始部署实际的项目到 K8S 中，逐步掌握生成环境中对 K8S 的使用实践。</p>
<p>PS: 也许你会觉得切换 <code>Namespace</code> 之类的操作很繁琐，有一个项目：<a href="https://github.com/ahmetb/kubectx">kubectx</a> 可帮你简化这些步骤，推荐尝试。</p>
<p>留言</p>
<h1 id="应用发布-部署实际项目"><a href="#应用发布-部署实际项目" class="headerlink" title="应用发布: 部署实际项目"></a>应用发布: 部署实际项目</h1><p>本节我们开始学习如何将实际项目部署至 K8S 中，开启生产实践之路。</p>
<h2 id="整体概览-2"><a href="#整体概览-2" class="headerlink" title="整体概览"></a>整体概览</h2><p>本节所用示例项目是一个混合了 Go，NodeJS，Python 等语言的项目，灵感来自于知名程序员 <a href="https://github.com/kennethreitz">Kenneth Reitz</a> 的 <a href="https://saythanks.io/" target="_blank" rel="noopener">Say Thanks</a> 项目。本项目实现的功能主要有两个：1. 用户通过前端发送感谢消息 2. 有个工作进程会持续的计算收到感谢消息的排行榜。项目代码可在 <a href="https://github.com/tao12345666333/saythx">GitHub 上获得</a>。接下来几节中如果需要用到此项目我会统一称之为 saythx 项目。</p>
<p>saythx 项目的基础结构如下图：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/11/18/16722d23f282fa8b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h2><h3 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h3><p>我们使用了前端框架 <a href="https://vuejs.org/" target="_blank" rel="noopener">Vue</a>，所以在做生产部署时，需要先在 <a href="http://nodejs.org/" target="_blank" rel="noopener">Node JS</a> 的环境下进行打包构建。包管理器使用的是 <a href="https://yarnpkg.com/" target="_blank" rel="noopener">Yarn</a>。然后使用 <a href="http://nginx.com/" target="_blank" rel="noopener">Nginx</a> 提供服务，并进行反向代理，将请求正确的代理至后端。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">FROM node:10.13 as builder</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;app</span><br><span class="line">COPY . &#x2F;app</span><br><span class="line"></span><br><span class="line">RUN yarn install \</span><br><span class="line">        &amp;&amp; yarn build</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FROM nginx:1.15</span><br><span class="line"></span><br><span class="line">COPY nginx.conf &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf</span><br><span class="line">COPY --from&#x3D;builder &#x2F;app&#x2F;dist &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;</span><br><span class="line"></span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure></div>

<p>Nginx 的配置文件如下：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">upstream backend-up &#123;</span><br><span class="line">    server saythx-backend:8080;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    charset     utf-8;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">        try_files $uri $uri&#x2F; &#x2F;index.html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ ^&#x2F;(api) &#123;</span><br><span class="line">        proxy_pass   http:&#x2F;&#x2F;backend-up;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>将 API 的请求反向代理到后端服务上。其余请求全部留给前端进行处理。</p>
<h3 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h3><p>后端是使用 <a href="http://golang.org/" target="_blank" rel="noopener">Golang</a> 编写的 API 服务，对请求进行相应处理，并将数据存储至 <a href="http://redis.io/" target="_blank" rel="noopener">Redis</a> 当中。依赖管理使用的是 <a href="https://github.com/golang/dep">dep</a>。由于 Golang 是编译型语言，编译完成后会生成一个二进制文件，为了让镜像尽可能小，所以 Dockerfile 和前端的差不多，都使用了<a href="https://docs.docker.com/develop/develop-images/multistage-build/" target="_blank" rel="noopener">多阶段构建</a>的特性。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">FROM golang:1.11.1 as builder</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;be</span><br><span class="line">COPY . &#x2F;go&#x2F;src&#x2F;be</span><br><span class="line">RUN go get -u github.com&#x2F;golang&#x2F;dep&#x2F;cmd&#x2F;dep \</span><br><span class="line">        &amp;&amp; dep ensure \</span><br><span class="line">        &amp;&amp; go build</span><br><span class="line"></span><br><span class="line">FROM debian:stretch-slim</span><br><span class="line">COPY --from&#x3D;builder &#x2F;go&#x2F;src&#x2F;be&#x2F;be &#x2F;usr&#x2F;bin&#x2F;be</span><br><span class="line">ENTRYPOINT [&quot;&#x2F;usr&#x2F;bin&#x2F;be&quot;]</span><br><span class="line">EXPOSE 8080</span><br></pre></td></tr></table></figure></div>

<p>注意这里会暴露出来后端服务所监听的端口。</p>
<h3 id="Work"><a href="#Work" class="headerlink" title="Work"></a>Work</h3><p>Work 端使用的是 <a href="http://python.org/" target="_blank" rel="noopener">Python</a>，用于计算已经存储至 Redis 当中的数据，并生成排行榜。依赖使用 <a href="https://github.com/pypa/pip">pip</a> 进行安装。对于 Python 的镜像选择，我做了一组<a href="http://moelove.info/docker-python-perf/" target="_blank" rel="noopener">性能对比的测试</a> 有兴趣可以了解下。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">FROM python:3.7-slim</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;app</span><br><span class="line">COPY . &#x2F;app</span><br><span class="line">RUN pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [&quot;python&quot;, &quot;work.py&quot;]</span><br></pre></td></tr></table></figure></div>

<h3 id="构建发布"><a href="#构建发布" class="headerlink" title="构建发布"></a>构建发布</h3><p>接下来，我们只要在对应项目目录中，执行 <code>docker build [OPTIONS] PATH</code> 即可。一般我们会使用 <code>-t name:tag</code> 的方式打 tag。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># 以下分别是在各模块自己的目录内</span><br><span class="line">➜  be docker build -t  taobeier&#x2F;saythx-be .</span><br><span class="line">➜  fe docker build -t  taobeier&#x2F;saythx-fe .</span><br><span class="line">➜  work docker build -t  taobeier&#x2F;saythx-work .</span><br></pre></td></tr></table></figure></div>

<p>需要注意的是，前端项目由于目录内包含开发时的 <code>node_modules</code> 等文件，需要注意添加 <code>.dockerignore</code> 文件，忽略一些非预期的文件。关于 Docker 的 build 原理，有想深入理解的，可参考我之前写的 <a href="http://moelove.info/2018/09/04/Docker-深入篇之-Build-原理/" target="_blank" rel="noopener">Docker 深入篇之 Build 原理</a> 。 当镜像构建完成后，我们需要将它们发布至镜像仓库。这里我们直接使用官方的 <a href="http://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>，执行 <code>docker login</code> 输入用户名密码验证成功后便可进行发布（需要先去 Docker Hub 注册帐号）。</p>
<p>登录成功后，默认情况下在 <code>$HOME/.docker/config.json</code> 中会存储用户相关凭证。</p>
<p>接下来进行发布只需要执行 <code>docker push</code> 即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ docker push taobeier&#x2F;saythx-be</span><br><span class="line">➜  ~ docker push taobeier&#x2F;saythx-fe</span><br><span class="line">➜  ~ docker push taobeier&#x2F;saythx-work</span><br></pre></td></tr></table></figure></div>

<h2 id="容器编排-Docker-Compose"><a href="#容器编排-Docker-Compose" class="headerlink" title="容器编排 Docker Compose"></a>容器编排 Docker Compose</h2><p><a href="https://docs.docker.com/compose/overview/" target="_blank" rel="noopener">Docker Compose</a> 是一种较为简单的可进行容器编排的技术，需要创建一个配置文件，通常情况下为 <code>docker-compose.yml</code> 。在 saythx 项目的根目录下我已经创建好了 <code>docker-compose.yml</code> 的配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">version: &#39;3&#39;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  saythx-frontend:</span><br><span class="line">    build:</span><br><span class="line">      context: fe&#x2F;.</span><br><span class="line">    image: taobeier&#x2F;saythx-fe</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8088:80&quot;</span><br><span class="line">    depends_on:</span><br><span class="line">      - saythx-backend</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line"></span><br><span class="line">  saythx-backend:</span><br><span class="line">    build:</span><br><span class="line">      context: be&#x2F;.</span><br><span class="line">    image: taobeier&#x2F;saythx-be</span><br><span class="line">    depends_on:</span><br><span class="line">      - saythx-redis</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line">    environment:</span><br><span class="line">      - REDIS_HOST&#x3D;saythx-redis</span><br><span class="line">    </span><br><span class="line">  saythx-work:</span><br><span class="line">    build:</span><br><span class="line">      context: work&#x2F;.</span><br><span class="line">    image: taobeier&#x2F;saythx-work</span><br><span class="line">    depends_on:</span><br><span class="line">      - saythx-redis</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line">    environment:</span><br><span class="line">      - REDIS_HOST&#x3D;saythx-redis</span><br><span class="line">      - REDIS_PORT&#x3D;6379</span><br><span class="line"></span><br><span class="line">  saythx-redis:</span><br><span class="line">    image: &quot;redis:5&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  saythx:</span><br></pre></td></tr></table></figure></div>

<p>在项目的根目录下执行 <code>docker-compose up</code> 即可启动该项目。在浏览器中访问 <a href="http://127.0.0.1:8088/" target="_blank" rel="noopener">http://127.0.0.1:8088/</a> 即可看到项目的前端界面。如下图：</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="628"></svg>)</p>
<p>打开另外的终端，进入项目根目录内，执行 <code>docker-compose ps</code> 命令即可看到当前的服务情况。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  saythx git:(master) ✗ docker-compose ps</span><br><span class="line"></span><br><span class="line">          Name                        Command               State          Ports</span><br><span class="line">----------------------------------------------------------------------------------------</span><br><span class="line">saythx_saythx-backend_1    &#x2F;usr&#x2F;bin&#x2F;be                      Up      8080&#x2F;tcp</span><br><span class="line">saythx_saythx-frontend_1   nginx -g daemon off;             Up      0.0.0.0:8088-&gt;80&#x2F;tcp</span><br><span class="line">saythx_saythx-redis_1      docker-entrypoint.sh redis ...   Up      6379&#x2F;tcp</span><br><span class="line">saythx_saythx-work_1       python work.py                   Up</span><br></pre></td></tr></table></figure></div>

<p>可以看到各组件均是 <code>Up</code> 的状态，相关端口也已经暴露出来。</p>
<p>可在浏览器直接访问体验。由于 <code>Docker Compose</code> 并非本册的重点，故不做太多介绍，可参考官方文档进行学习。接下来进入本节的重点内容，将项目部署至 K8S 中。</p>
<h2 id="编写配置文件并部署"><a href="#编写配置文件并部署" class="headerlink" title="编写配置文件并部署"></a>编写配置文件并部署</h2><p>在 K8S 中进行部署或者说与 K8S 交互的方式主要有三种：</p>
<ul>
<li>命令式</li>
<li>命令式对象配置</li>
<li>声明式对象配置</li>
</ul>
<p>第 7 节介绍过的 <code>kubectl run redis --image=&#39;redis:alpine&#39;</code> 这种方式便是命令式，这种方式很简单，但是可重用性低。毕竟你的命令执行完后，其他人也并不清楚到底发生了什么。</p>
<p>命令式对象配置，主要是编写配置文件，但是通过类似 <code>kubectl create</code> 之类命令式的方式进行操作。</p>
<p>再有一种便是声明式对象配置，主要也是通过编写配置文件，但是使用 <code>kubectl apply</code> 之类的放好似进行操作。与第二种命令式对象配置的区别主要在于对对象的操作将会得到保留。但同时这种方式有时候也并不好进行调试。</p>
<p>接下来，为 saythx 项目编写配置文件，让它可以部署至 K8S 中。当然，这里我们已经创建过了 <code>docker-compose.yml</code> 的配置文件，并且也验证了其可用性，可以直接使用 <a href="https://github.com/kubernetes/kompose">Kompose</a> 工具将 <code>docker-compose.yml</code> 的配置文件进行转换。</p>
<p>但这里采用直接编写的方式。同时，我们部署至一个新的名为 <code>work</code> 的 <code>Namespace</code> 中。</p>
<h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: work</span><br></pre></td></tr></table></figure></div>

<p>指定了 <code>Namespace</code> name 为 <code>work</code>。然后进行部署</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f namespace.yaml </span><br><span class="line">namespace&#x2F;work created</span><br></pre></td></tr></table></figure></div>

<h3 id="Redis-资源"><a href="#Redis-资源" class="headerlink" title="Redis 资源"></a>Redis 资源</h3><p>从前面的 <code>docker-compose.yml</code> 中也能发现，saythx 中各个组件，只有 Redis 是无任何依赖的。我们先对它进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: redis</span><br><span class="line">  name: saythx-redis</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: redis</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: redis</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: redis:5</span><br><span class="line">        name: redis</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 6379</span><br></pre></td></tr></table></figure></div>

<p>由于这是本册内第一次出现完整的 <code>Deployment</code> 配置文件，故而进行重点介绍。</p>
<ul>
<li><code>apiVersion</code> ：指定了 API 的版本号，当前我们使用的 K8S 中， <code>Deployment</code> 的版本号为 <code>apps/v1</code>，而在 1.9 之前使用的版本则为 <code>apps/v1beta2</code>，在 1.8 之前的版本使用的版本为 <code>extensions/v1beta1</code>。在编写配置文件时需要格外注意。</li>
<li><code>kind</code> ：指定了资源的类型。这里指定为 <code>Deployment</code> 说明是一次部署。</li>
<li><code>metadata</code> ：指定了资源的元信息。例如其中的 <code>name</code> 和 <code>namespace</code> 分别表示资源名称和所归属的 <code>Namespace</code>。</li>
<li><code>spec</code> ：指定了对资源的配置信息。例如其中的 <code>replicas</code> 指定了副本数当前指定为 1 。<code>template.spec</code> 则指定了 <code>Pod</code> 中容器的配置信息，这里的 <code>Pod</code> 中只部署了一个容器。</li>
</ul>
<p>配置文件已经生产，现在对它进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">No resources found.</span><br><span class="line">➜  conf git:(master) ✗ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-79d8f9864d-x8fp9   1&#x2F;1       Running   0          4s</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            1           4s</span><br><span class="line"></span><br><span class="line">NAME                                      DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-79d8f9864d   1         1         1         4s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 已经在正常运行了。我们进入 <code>Pod</code> 内进行测试。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl -n work exec -it saythx-redis-79d8f9864d-x8fp9 bash</span><br><span class="line">root@saythx-redis-79d8f9864d-x8fp9:&#x2F;data# redis-cli</span><br><span class="line">127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure></div>

<p>响应正常。</p>
<h3 id="Redis-service"><a href="#Redis-service" class="headerlink" title="Redis service"></a>Redis service</h3><p>由于 <code>Redis</code> 是后端服务的依赖，我们将它作为 <code>Service</code> 暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: redis</span><br><span class="line">  name: saythx-redis</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 6379</span><br><span class="line">    targetPort: 6379</span><br><span class="line">  selector:</span><br><span class="line">    app: redis</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>关于 <code>Service</code> 的内容，可参考第 7 节，我们详细做过解释。这里直接使用配置文件进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br><span class="line">➜  conf git:(master) ✗ kubectl get svc -n work</span><br><span class="line">NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">saythx-redis   NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   1m</span><br></pre></td></tr></table></figure></div>

<h3 id="后端服务"><a href="#后端服务" class="headerlink" title="后端服务"></a>后端服务</h3><p>接下来，我们对后端服务进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: backend</span><br><span class="line">  name: saythx-backend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: backend</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: backend</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - env:</span><br><span class="line">        - name: REDIS_HOST</span><br><span class="line">          value: saythx-redis</span><br><span class="line">        image: taobeier&#x2F;saythx-be</span><br><span class="line">        name: backend</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br></pre></td></tr></table></figure></div>

<p>可以看到这里通过环境变量的方式，将 <code>REDIS_HOST</code> 传递给了后端服务。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f backend-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-backend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                 READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-backend-c5f9f6d95-lmtxn   0&#x2F;1       ContainerCreating   0          5s</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-kcmzk     1&#x2F;1       Running             0          17m</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   1m</span><br><span class="line"></span><br><span class="line">NAME                             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-backend   1         1         1            0           5s</span><br><span class="line">deployment.apps&#x2F;saythx-redis     1         1         1            1           17m</span><br><span class="line"></span><br><span class="line">NAME                                       DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-backend-c5f9f6d95   1         1         0         5s</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d     1         1         1         17m</span><br></pre></td></tr></table></figure></div>

<h3 id="后端-Service"><a href="#后端-Service" class="headerlink" title="后端 Service"></a>后端 Service</h3><p>后端服务是前端项目的依赖，故而我们也将其作为 <code>Service</code> 暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: backend</span><br><span class="line">  name: saythx-backend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: backend</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>通过配置文件进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f backend-service.yaml</span><br><span class="line">service&#x2F;saythx-backend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl get svc -n work</span><br><span class="line">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-backend   NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   8s</span><br><span class="line">service&#x2F;saythx-redis     NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   3m</span><br></pre></td></tr></table></figure></div>

<p>我们同样使用 <code>NodePort</code> 将其暴露出来，并在本地进行测试。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ curl http:&#x2F;&#x2F;127.0.0.1:32051&#x2F;api&#x2F;v1&#x2F;list</span><br><span class="line">&#123;&quot;HonorDatas&quot;:null&#125;</span><br></pre></td></tr></table></figure></div>

<p>服务可正常响应。</p>
<h3 id="前端-1"><a href="#前端-1" class="headerlink" title="前端"></a>前端</h3><p>接下来我们编写前端的配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: frontend</span><br><span class="line">  name: saythx-frontend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: frontend</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: frontend</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: taobeier&#x2F;saythx-fe</span><br><span class="line">        name: frontend</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure></div>

<p>需要注意的是，我们必须在后端 <code>Service</code> 暴露出来后才能进行前端的部署，因为前端镜像中 Nginx 的反向代理配置中会去检查后端是否可达。使用配置文件进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f frontend-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-frontend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-backend-c5f9f6d95-lmtxn     1&#x2F;1       Running   0          16m</span><br><span class="line">pod&#x2F;saythx-frontend-678d544b86-wp9gr   1&#x2F;1       Running   0          30s</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-kcmzk       1&#x2F;1       Running   0          34m</span><br><span class="line"></span><br><span class="line">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-backend   NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   15m</span><br><span class="line">service&#x2F;saythx-redis     NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   18m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-backend    1         1         1            1           16m</span><br><span class="line">deployment.apps&#x2F;saythx-frontend   1         1         1            1           30s</span><br><span class="line">deployment.apps&#x2F;saythx-redis      1         1         1            1           34m</span><br><span class="line"></span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-backend-c5f9f6d95     1         1         1         16m</span><br><span class="line">replicaset.apps&#x2F;saythx-frontend-678d544b86   1         1         1         30s</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d       1         1         1         34m</span><br></pre></td></tr></table></figure></div>

<h3 id="前端-Service"><a href="#前端-Service" class="headerlink" title="前端 Service"></a>前端 Service</h3><p>接下来，我们需要让前端可以被直接访问到，同样的需要将它以 <code>Service</code> 的形式暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: frontend</span><br><span class="line">  name: saythx-frontend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: &quot;80&quot;</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: frontend</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>创建 <code>Service</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f frontend-service.yaml</span><br><span class="line">service&#x2F;saythx-frontend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get svc</span><br><span class="line">NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">saythx-backend    NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   17m</span><br><span class="line">saythx-frontend   NodePort   10.96.221.71    &lt;none&gt;        80:32682&#x2F;TCP     11s</span><br><span class="line">saythx-redis      NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   20m</span><br></pre></td></tr></table></figure></div>

<p>我们可以直接通过 Node 的 32682 端口进行访问。</p>
<h3 id="Work-1"><a href="#Work-1" class="headerlink" title="Work"></a>Work</h3><p>最后，是我们的 Work 组件，为它编写配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: work</span><br><span class="line">  name: saythx-work</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: work</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: work</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - env:</span><br><span class="line">        - name: REDIS_HOST</span><br><span class="line">          value: saythx-redis</span><br><span class="line">        - name: REDIS_PORT</span><br><span class="line">          value: &quot;6379&quot;</span><br><span class="line">        image: taobeier&#x2F;saythx-work</span><br><span class="line">        name: work</span><br></pre></td></tr></table></figure></div>

<p>同样的，我们通过环境变量的方式传递了 Redis 相关的配置进去。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f work-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-work created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                   READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-backend-c5f9f6d95-lmtxn     1&#x2F;1       Running             0          22m</span><br><span class="line">pod&#x2F;saythx-frontend-678d544b86-wp9gr   1&#x2F;1       Running             0          5m</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-kcmzk       1&#x2F;1       Running             0          39m</span><br><span class="line">pod&#x2F;saythx-work-6b9958dc47-hh9td       0&#x2F;1       ContainerCreating   0          7s</span><br><span class="line"></span><br><span class="line">NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-backend    NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   20m</span><br><span class="line">service&#x2F;saythx-frontend   NodePort   10.96.221.71    &lt;none&gt;        80:32682&#x2F;TCP     3m</span><br><span class="line">service&#x2F;saythx-redis      NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   23m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-backend    1         1         1            1           22m</span><br><span class="line">deployment.apps&#x2F;saythx-frontend   1         1         1            1           5m</span><br><span class="line">deployment.apps&#x2F;saythx-redis      1         1         1            1           39m</span><br><span class="line">deployment.apps&#x2F;saythx-work       1         1         1            0           7s</span><br><span class="line"></span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-backend-c5f9f6d95     1         1         1         22m</span><br><span class="line">replicaset.apps&#x2F;saythx-frontend-678d544b86   1         1         1         5m</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d       1         1         1         39m</span><br><span class="line">replicaset.apps&#x2F;saythx-work-6b9958dc47       1         1         0         7s</span><br></pre></td></tr></table></figure></div>

<p>现在均已经部署完成。并且可直接通过 Node 端口进行访问。</p>
<h2 id="扩缩容"><a href="#扩缩容" class="headerlink" title="扩缩容"></a>扩缩容</h2><p>如果我们觉得排行榜生成效率较低，则可通过扩容 Work 来得到解决。具体做法是可修改 work 的 <code>Deployment</code> 配置文件，将 <code>spec.replicas</code> 设置为预期的数值，之后执行 <code>kubectl -f work-deployment.yaml</code> 即可。</p>
<p>或者可直接通过命令行进行操作</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl -n work scale --replicas&#x3D;2  deployment&#x2F;saythx-work</span><br></pre></td></tr></table></figure></div>

<p>上面的命令是将 <code>saythx-work</code> 的部署副本数设置为 2 。缩容也差不多是类似的操作。</p>
<h2 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h2><p>通过本节的学习，我们已经学习到了如何将项目实际部署至 K8S 中，可以手动编写配置也可以利用一些工具进行辅助。同时也了解到了如何应对应用的扩缩容。</p>
<p>但如果应用需要进行升级的话，则需要去更改配置文件中相关的配置，这个过程会较为繁琐，并且整体项目线上的版本管理也是个问题：比如组件的个人升级，回滚之间如果有依赖的话，会比较麻烦。我们在接下来的两节来学习如何解决这个问题。</p>
<h1 id="应用管理-初识-Helm"><a href="#应用管理-初识-Helm" class="headerlink" title="应用管理: 初识 Helm"></a>应用管理: 初识 Helm</h1><h2 id="整体概览-3"><a href="#整体概览-3" class="headerlink" title="整体概览"></a>整体概览</h2><p>上节，我们已经学习了如何通过编写配置文件的方式部署项目。而在实际生产环境中，项目所包含组件可能不止 3 个，并且可能项目数会很多，如果每个项目的发布，更新等都通过手动去编写配置文件的方式，实在不利于管理。</p>
<p>并且，当线上出现个别组件升级回滚之类的操作，如果组件之间有相关版本依赖等情况，那事情会变得复杂的多。我们需要有更简单的机制来辅助我们完成这些事情。</p>
<h2 id="Helm-介绍"><a href="#Helm-介绍" class="headerlink" title="Helm 介绍"></a>Helm 介绍</h2><p><a href="https://www.helm.sh/" target="_blank" rel="noopener">Helm</a> 是构建于 K8S 之上的包管理器，可与我们平时接触到的 <code>Yum</code>，<code>APT</code>，<code>Homebrew</code> 或者 <code>Pip</code> 等包管理器相类比。</p>
<p>使用 Helm 可简化包分发，安装，版本管理等操作流程。同时它也是 CNCF 孵化项目。</p>
<h2 id="Helm-安装"><a href="#Helm-安装" class="headerlink" title="Helm 安装"></a>Helm 安装</h2><p>Helm 是 C/S 架构，主要分为客户端 <code>helm</code> 和服务端 <code>Tiller</code>。安装时可直接在 <a href="https://github.com/helm/helm/releases">Helm 仓库的 Release 页面</a> 下载所需二进制文件或者源码包。</p>
<p>由于当前项目的二进制文件存储已切换为 GCS，我已经为国内用户准备了最新版本的二进制包，可通过以下链接进行下载。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1n1zj3rlv2NyfiA6kRGrHfg 提取码: 5huw</span><br></pre></td></tr></table></figure></div>

<p>下载后对文件进行解压，我这里以 Linux amd64 为例。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  &#x2F;tmp tar -zxvf helm-v2.11.0-linux-amd64.tar.gz</span><br><span class="line">linux-amd64&#x2F;</span><br><span class="line">linux-amd64&#x2F;tiller</span><br><span class="line">linux-amd64&#x2F;README.md</span><br><span class="line">linux-amd64&#x2F;helm</span><br><span class="line">linux-amd64&#x2F;LICENSE</span><br><span class="line">➜  &#x2F;tmp tree linux-amd64 </span><br><span class="line">linux-amd64</span><br><span class="line">├── helm</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">└── tiller</span><br><span class="line"></span><br><span class="line">0 directories, 4 files</span><br></pre></td></tr></table></figure></div>

<p>解压完成后，可看到其中包含 <code>helm</code> 和 <code>tiller</code> 二进制文件。</p>
<h3 id="客户端-helm"><a href="#客户端-helm" class="headerlink" title="客户端 helm"></a>客户端 helm</h3><p><code>helm</code> 是个二进制文件，直接将它移动至 <code>/usr/bin</code> 目录下即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  &#x2F;tmp sudo mv linux-amd64&#x2F;helm &#x2F;usr&#x2F;bin&#x2F;helm</span><br></pre></td></tr></table></figure></div>

<p>这时候便可直接通过 <code>helm</code> 命令使用了。比如，我们验证当前使用的版本。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  &#x2F;tmp helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Error: Get http:&#x2F;&#x2F;localhost:8080&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;pods?labelSelector&#x3D;app%3Dhelm%2Cname%3Dtiller: dial tcp 127.0.0.1:8080: connect: connection refused</span><br></pre></td></tr></table></figure></div>

<p>可以看到上面有明显的报错，并且很像 <code>kubectl</code> 未正确配置时的错误。这是因为 <code>helm</code> 默认会去读取 <code>$HOME/.kube/config</code> 的配置文件，用于正确的连接至目标集群。</p>
<p>当我们正确的配置好 <code>$HOME/.kube/config</code> 文件时，再次执行：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  &#x2F;tmp helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Error: could not find tiller</span><br></pre></td></tr></table></figure></div>

<p>这次报错是因为找不到服务端 <code>Tiller</code>，接下来我们部署服务端。</p>
<h3 id="服务端-Tiller"><a href="#服务端-Tiller" class="headerlink" title="服务端 Tiller"></a>服务端 Tiller</h3><p>以下讨论中，前提都是 <code>$HOME/.kube/config</code> 已正确配置，并且 <code>kebectl</code> 有操作集群的权限。</p>
<h4 id="本地安装"><a href="#本地安装" class="headerlink" title="本地安装"></a>本地安装</h4><p>刚才我们解压的文件中，还有一个二进制文件 <code>tiller</code> 。我们可以直接执行它，用于在本地启动服务。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  &#x2F;tmp .&#x2F;linux-amd64&#x2F;tiller</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Starting Tiller v2.11.0 (tls&#x3D;false)</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 GRPC listening on :44134</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Probes listening on :44135</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Storage driver is ConfigMap</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Max history per release is 0</span><br></pre></td></tr></table></figure></div>

<p>直接执行时，默认会监听 <code>44134</code> 和 <code>44135</code> 端口，<code>44134</code> 端口用于和 <code>helm</code> 进行通信，而 <code>44135</code> 主要是用于做探活的，在部署至 K8S 时使用。</p>
<p>当我们使用客户端连接时，只需设置 <code>HELM_HOST</code> 环境变量即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ export HELM_HOST&#x3D;localhost:44134</span><br><span class="line">➜  ~ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>注意</strong> 一定要正确配置 <code>$HOME/.kube/config</code> 文件，否则会影响正常功能使用。</p>
<h4 id="默认安装"><a href="#默认安装" class="headerlink" title="默认安装"></a>默认安装</h4><p>官方提供了一种一键式安装的方式。那便是 <code>helm init</code> 执行这条命令后，会同时在 K8S 中部署服务端 Tiller 和初始化 helm 的默认目录 <code>$HELM_HOME</code> 默认值为 <code>$HOME/.helm</code>。</p>
<p>这种方式下会默认使用官方镜像 <code>gcr.io/kubernetes-helm/tiller</code> 网络原因可能会导致安装失败。所以我已将官方镜像进行同步。可使用以下方式进行使用：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ helm init --tiller-image taobeier&#x2F;tiller:v2.11.0 </span><br><span class="line">Creating &#x2F;root&#x2F;.helm</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;cache</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;local</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;plugins</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;starters</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;cache&#x2F;archive</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;repositories.yaml</span><br><span class="line">Adding stable repo with URL: https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com</span><br><span class="line">Adding local repo with URL: http:&#x2F;&#x2F;127.0.0.1:8879&#x2F;charts</span><br><span class="line">$HELM_HOME has been configured at &#x2F;root&#x2F;.helm.</span><br><span class="line"></span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line"></span><br><span class="line">Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy.</span><br><span class="line">To prevent this, run &#96;helm init&#96; with the --tiller-tls-verify flag.</span><br><span class="line">For more information on securing your installation see: https:&#x2F;&#x2F;docs.helm.sh&#x2F;using_helm&#x2F;#securing-your-helm-installation</span><br><span class="line">Happy Helming!</span><br><span class="line">➜  ~ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>$HELM_HOME</code> 目录已经初始化完成，客户端与服务端已可以正常通信。查看下当前 K8S 集群中的情况：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl -n kube-system get deploy tiller-deploy</span><br><span class="line">NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">tiller-deploy   1         1         1            1           6m</span><br></pre></td></tr></table></figure></div>

<p>可以看到已正常部署。</p>
<h4 id="手动安装"><a href="#手动安装" class="headerlink" title="手动安装"></a>手动安装</h4><p>通过上面的描述，可能你已经发现，安装服务端，其实也就是一次普通的部署，我们可以通过以下方式来自行通过 <code>kubectl</code> 完成部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ helm init --dry-run --debug  # 篇幅原因，以下内容进行了省略</span><br><span class="line">---                               </span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment                                    </span><br><span class="line">metadata:                            </span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:         </span><br><span class="line">    app: helm              </span><br><span class="line">    name: tiller       </span><br><span class="line">  name: tiller-deploy           </span><br><span class="line">  namespace: kube-system   </span><br><span class="line">spec:               </span><br><span class="line">  replicas: 1 </span><br><span class="line">  strategy: &#123;&#125;                </span><br><span class="line">  ...</span><br><span class="line">status: &#123;&#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: helm</span><br><span class="line">    name: tiller</span><br><span class="line">  name: tiller-deploy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: tiller</span><br><span class="line">    port: 44134</span><br><span class="line">    targetPort: tiller</span><br><span class="line">  selector:</span><br><span class="line">    app: helm</span><br><span class="line">    name: tiller</span><br><span class="line">  type: ClusterIP</span><br><span class="line">status:</span><br><span class="line">  loadBalancer: &#123;&#125;</span><br></pre></td></tr></table></figure></div>

<p>将输出内容保存至文件中，自行修改后，通过 <code>kubectl</code> 进行部署即可。建议在修改过程中，尽量不要去更改标签及选择器。</p>
<h4 id="RBAC-使用"><a href="#RBAC-使用" class="headerlink" title="RBAC 使用"></a>RBAC 使用</h4><p>上面的内容中，均未提及到权限控制相关的内容，但是在生产环境中使用，我们一般都是会进行权限控制的。</p>
<p>在第 8 节中，我们已经详细的解释了认证授权相关的内容。所以下面的内容不做太多详细解释。</p>
<p>这里我们创建一个 <code>ServiceAccount</code> 命名为 <code>tiller</code>，为了简单，我们直接将它与 <code>cluster-admin</code> 进行绑定。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: tiller</span><br><span class="line">    namespace: kube-system</span><br></pre></td></tr></table></figure></div>

<p>将此内容保存为 <code>tiller-rbac.yaml</code>，开始进行部署操作。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl apply -f tiller-rbac.yaml</span><br><span class="line">serviceaccount&#x2F;tiller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;tiller created</span><br><span class="line">➜  ~ helm init --service-account tiller</span><br><span class="line">Creating &#x2F;root&#x2F;.helm</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;cache</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;local</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;plugins</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;starters</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;cache&#x2F;archive</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;repositories.yaml</span><br><span class="line">Adding stable repo with URL: https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com</span><br><span class="line">Adding local repo with URL: http:&#x2F;&#x2F;127.0.0.1:8879&#x2F;charts</span><br><span class="line">$HELM_HOME has been configured at &#x2F;root&#x2F;.helm.</span><br><span class="line"></span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line"></span><br><span class="line">Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy.</span><br><span class="line">To prevent this, run &#96;helm init&#96; with the --tiller-tls-verify flag.</span><br><span class="line">For more information on securing your installation see: https:&#x2F;&#x2F;docs.helm.sh&#x2F;using_helm&#x2F;#securing-your-helm-installation</span><br><span class="line">Happy Helming!</span><br><span class="line">➜  ~ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>以此方式完成部署。</p>
<h2 id="Helm-概念"><a href="#Helm-概念" class="headerlink" title="Helm 概念"></a>Helm 概念</h2><h3 id="Chart"><a href="#Chart" class="headerlink" title="Chart"></a>Chart</h3><p><code>chart</code> 就是 Helm 所管理的包，类似于 <code>Yum</code> 所管理的 <code>rpm</code> 包或是 <code>Homebrew</code> 管理的 <code>Formulae</code>。它包含着一个应用要部署至 K8S 上所必须的所有资源。</p>
<h3 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h3><p><code>Release</code> 就是 <code>chart</code> 在 K8S 上部署后的实例。<code>chart</code> 的每次部署都将产生一次 <code>Release</code>。这和上面类比的包管理器就有所不同了，多数的系统级包管理器所安装的包只会在系统中存在一份。我们可以以 <code>Pip</code> 在虚拟环境下的包安装，或者 <code>Npm</code> 的 local install 来进行类比。</p>
<h3 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h3><p><code>Repository</code> 就是字面意思，存储 <code>chart</code> 的仓库。还记得我们上面执行 <code>helm init</code> 时的输出吗？默认情况下，初始化 Helm 的时候，会添加两个仓库，一个是 <code>stable</code> 仓库 <a href="https://kubernetes-charts.storage.googleapis.com/" target="_blank" rel="noopener">kubernetes-charts.storage.googleapis.com</a> 另一个则是 <code>local</code> 仓库，地址是 <a href="http://127.0.0.1:8879/charts" target="_blank" rel="noopener">http://127.0.0.1:8879/charts</a> 。</p>
<h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><p>前面提到了 <code>chart</code> 是应用程序所必须的资源，当然我们实际部署的时候，可能就需要有些自定义的配置了。<code>Config</code> 便是用于完成此项功能的，在部署时候，会将 <code>config</code> 与 <code>chart</code> 进行合并，共同构成我们将部署的应用。</p>
<h2 id="Helm-的工作原理"><a href="#Helm-的工作原理" class="headerlink" title="Helm 的工作原理"></a>Helm 的工作原理</h2><p><code>helm</code> 通过 <code>gRPC</code> 将 <code>chart</code> 发送至 <code>Tiller</code> ，<code>Tiller</code> 则通过内置的 <code>kubernetes</code> 客户端库与 K8S 的 API server 进行交流，将 <code>chart</code> 进行部署，并生成 <code>Release</code> 用于管理。</p>
<p>前面只说到了 <code>helm</code> 与 <code>Tiller</code> 交互的协议，但尚未说其数据链路。</p>
<p>我们来看看 <code>Tiller</code> 的部署情况。主要看 <code>Service</code>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  ~ kubectl -n kube-system get svc</span><br><span class="line">NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kube-dns        ClusterIP   10.96.0.10       &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP   1h</span><br><span class="line">tiller-deploy   ClusterIP   10.107.204.164   &lt;none&gt;        44134&#x2F;TCP       33m</span><br></pre></td></tr></table></figure></div>

<p><code>Tiller</code> 默认采用 <code>ClusterIP</code> 类型的 <code>Service</code> 进行部署。而我们知道的 <code>ClusterIP</code> 类型的 <code>Service</code> 是仅限集群内访问的。</p>
<p>在这里所依赖的技术，便是在第 5 节，我们提到的 <code>socat</code> 。<code>helm</code> 通过 <code>socat</code> 的端口转发（或者说 K8S 的代理），进而实现了本地与 <code>Tiller</code> 的通信。</p>
<p>当然，以上内容均以当前最新版本 <code>2.11.0</code> 为例。当下一个大版本 Helm v3 出现时， <code>Tiller</code> 将不复存在，通信机制和工作原理也将发生变化。</p>
<h2 id="总结-8"><a href="#总结-8" class="headerlink" title="总结"></a>总结</h2><p>通过本节，我们已经学习到了 Helm 的基础知识和工作原理，了解到了 Helm 的用途以及如何在本地和 K8S 中部署它。需要注意的是 <code>$HOME/.kube/config</code> 需要提前配置好，以及 <code>socat</code> 工具需要提前安装，可参考第 5 节的内容。</p>
<p>接下来，我们将上节中的示例项目使用 Helm 部署至 K8S 集群中。</p>
<h1 id="部署实践-以-Helm-部署项目"><a href="#部署实践-以-Helm-部署项目" class="headerlink" title="部署实践: 以 Helm 部署项目"></a>部署实践: 以 Helm 部署项目</h1><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>上节，我们学习到了 Helm 的基础概念和工作原理，本节我们将 Helm 用于我们的实际项目，编写 Helm <code>chart</code> 以及通过 Helm 进行部署。</p>
<h2 id="Helm-chart"><a href="#Helm-chart" class="headerlink" title="Helm chart"></a>Helm chart</h2><p>上节我们解释过 <code>chart</code> 的含义，现在我们要将项目使用 Helm 部署，那么首先，我们需要创建一个 <code>chart</code>。</p>
<h3 id="Chart-结构"><a href="#Chart-结构" class="headerlink" title="Chart 结构"></a>Chart 结构</h3><p>在我们项目的根目录下，通过以下命令创建一个 <code>chart</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  saythx git:(master) helm create saythx</span><br><span class="line">Creating saythx</span><br><span class="line">➜  saythx git:(master) ✗ tree -a saythx</span><br><span class="line">saythx</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── .helmignore</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">2 directories, 8 files</span><br></pre></td></tr></table></figure></div>

<p>创建完成后，我们可以看到默认创建的 <code>chart</code> 中包含了几个文件和目录。我们先对其进行解释。</p>
<h4 id="Chart-yaml"><a href="#Chart-yaml" class="headerlink" title="Chart.yaml"></a>Chart.yaml</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  saythx git:(master) ✗ cat saythx&#x2F;Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for Kubernetes</span><br><span class="line">name: saythx</span><br><span class="line">version: 0.1.0</span><br></pre></td></tr></table></figure></div>

<p>这个文件是每个 <code>chart</code> 必不可少的一个文件，其中包含着几个重要的属性，如：</p>
<ul>
<li><code>apiVersion</code>：目前版本都为 <code>v1</code></li>
<li><code>appVersion</code>：这是应用的版本号，需要与 <code>apiVersion</code>， <code>version</code> 等字段注意区分</li>
<li><code>name</code>: 通常要求 <code>chart</code> 的名字必须和它所在目录保持一致，且此字段必须</li>
<li><code>version</code>：表明当前 <code>chart</code> 的版本号，会直接影响 <code>Release</code> 的记录，且此字段必须</li>
<li><code>description</code>：描述</li>
</ul>
<h4 id="charts"><a href="#charts" class="headerlink" title="charts"></a>charts</h4><p><code>charts</code> 文件夹是用于存放依赖的 <code>chart</code> 的。当有依赖需要管理时，可添加 <code>requirements.yaml</code> 文件，可用于管理项目内或者外部的依赖。</p>
<h4 id="helmignore"><a href="#helmignore" class="headerlink" title=".helmignore"></a>.helmignore</h4><p><code>.helmignore</code> 类似于 <code>.gitignore</code> 和 <code>.dockerignore</code> 之类的，用于忽略掉一些不想包含在 <code>chart</code> 内的文件。</p>
<h4 id="templates"><a href="#templates" class="headerlink" title="templates"></a>templates</h4><p><code>templates</code> 文件夹内存放着 <code>chart</code> 所使用的模板文件，也是 <code>chart</code> 的实际执行内容。在使用 <code>chart</code> 进行安装的时候，会将 下面介绍的 <code>values.yaml</code> 中的配置项与 <code>templates</code> 中的模板进行组装，生成最终要执行的配置文件。</p>
<p><code>templates</code> 中，推荐命名应该清晰，如 <code>xx-deployment.yaml</code>，中间使用 <code>-</code> 进行分割，避免使用驼峰式命名。</p>
<p><code>Notes.txt</code> 文件在 <code>helm install</code> 完成后，会进行回显，可用于解释说明如何访问服务等。</p>
<h4 id="values-yaml"><a href="#values-yaml" class="headerlink" title="values.yaml"></a>values.yaml</h4><p><code>values.yaml</code> 存放着项目的一些可配置项，如镜像的名称或者 tag 之类的。作用就是用于和模板进行组装。</p>
<h3 id="编写-chart"><a href="#编写-chart" class="headerlink" title="编写 chart"></a>编写 chart</h3><p>了解完结构之后，我们来实际编写我们的 chart 。所有完整的代码可在 <a href="https://github.com/tao12345666333/saythx">SayThx 项目</a> 获取。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for SayThx.</span><br><span class="line">name: saythx</span><br><span class="line">version: 0.1.0</span><br><span class="line">maintainers:</span><br><span class="line">  - name: Jintao Zhang</span><br></pre></td></tr></table></figure></div>

<p>可添加 <code>maintainers</code> 字段，表示维护者。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># values.yaml</span><br><span class="line"></span><br><span class="line"># backend is the values for backend</span><br><span class="line">backend:</span><br><span class="line">  image: taobeier&#x2F;saythx-be</span><br><span class="line">  tag: &quot;1.0&quot;</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  replicas: 1</span><br><span class="line"></span><br><span class="line"># namespace is the values for deploy namespace</span><br><span class="line">namespace: work</span><br><span class="line"></span><br><span class="line"># service.type is the values for service type</span><br><span class="line">service:</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p><code>values.yaml</code> 文件中定义了我们预期哪些东西是可配置的，比如 <code>namespace</code> 以及镜像名称 tag 等。这里只是贴出了部分内容，仅做说明使用，完整内容可查看我们的<a href="https://github.com/tao12345666333/saythx">示例项目</a> 。</p>
<p>写 <code>values.yaml</code> 文件的时候，由于是使用 <code>YAML</code> 格式的配置，所以它非常的灵活，即可以使用如上面例子中的 <code>backend</code> 那种字典类型的， 也可以写成简单的 k-v 形式。但通常来讲，应该尽可能的将它写的清晰明确。并且容易被替换。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># templates&#x2F;backend-service.yaml </span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: backend</span><br><span class="line">  name: saythx-backend</span><br><span class="line">  namespace: &#123;&#123; .Values.namespace &#125;&#125;</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: backend</span><br><span class="line">  type: &#123;&#123; .Values.service.type &#125;&#125;</span><br></pre></td></tr></table></figure></div>

<p>将我们之前写的部署文件模板化，与配置项进行组装。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">&#123;&#123;- if contains &quot;NodePort&quot; .Values.service.type &#125;&#125;</span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace &#123;&#123; .Values.namespace &#125;&#125; -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services saythx-frontend)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace &#123;&#123; .Values.namespace &#125;&#125; -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br><span class="line">&#123;&#123;- else if contains &quot;ClusterIP&quot; .Values.service.type &#125;&#125;</span><br><span class="line">  export POD_NAME&#x3D;$(kubectl get pods --namespace &#123;&#123; .Values.namespace &#125;&#125; -l &quot;app&#x3D;frontend&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http:&#x2F;&#x2F;127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl --namespace &#123;&#123; .Values.namespace &#125;&#125; port-forward $POD_NAME 8080:80</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure></div>

<p>上面这是 <code>NOTES.txt</code> 文件内的内容。 这些内容会在 <code>helm install</code> 执行成功后显示在终端，用于说明服务如何访问或者其他注意事项等。</p>
<p>当然，这里的内容主要是为了说明如何编写 <code>chart</code> ，在实践中，尽量避免硬编码配置在里面。</p>
<h2 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h2><h3 id="直接部署"><a href="#直接部署" class="headerlink" title="直接部署"></a>直接部署</h3><p>Helm 的 <code>chart</code> 可以直接在源码目录下通过 <code>helm install</code> 完成部署。例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  saythx helm install saythx</span><br><span class="line">NAME:   handy-seastar</span><br><span class="line">LAST DEPLOYED: Tue Nov 20 23:33:42 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Namespace</span><br><span class="line">NAME  STATUS  AGE</span><br><span class="line">work  Active  1s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME             TYPE      CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE</span><br><span class="line">saythx-backend   NodePort  10.102.206.213  &lt;none&gt;       8080:30663&#x2F;TCP  0s</span><br><span class="line">saythx-frontend  NodePort  10.96.109.45    &lt;none&gt;       80:30300&#x2F;TCP    0s</span><br><span class="line">saythx-redis     NodePort  10.97.174.8     &lt;none&gt;       6379:30589&#x2F;TCP  0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Deployment</span><br><span class="line">NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">saythx-backend   1        1        1           0          0s</span><br><span class="line">saythx-frontend  1        1        1           0          0s</span><br><span class="line">saythx-redis     1        1        1           0          0s</span><br><span class="line">saythx-work      1        1        1           0          0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                              READY  STATUS             RESTARTS  AGE</span><br><span class="line">saythx-backend-7f6d86d9c8-xqttg   0&#x2F;1    ContainerCreating  0         0s</span><br><span class="line">saythx-frontend-777fc64997-9zmq6  0&#x2F;1    Pending            0         0s</span><br><span class="line">saythx-redis-8558c7d7d-lh5df      0&#x2F;1    ContainerCreating  0         0s</span><br><span class="line">saythx-work-9b4446d84-c2pr4       0&#x2F;1    ContainerCreating  0         0s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace work -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services saythx-frontend)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace work -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br></pre></td></tr></table></figure></div>

<h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><p>当然，我们也可以将 <code>chart</code> 打包，以便于分发。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  saythx helm package saythx </span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;root&#x2F;saythx&#x2F;saythx-0.1.0.tgz</span><br></pre></td></tr></table></figure></div>

<p>可以看到打包时是按照 <code>chart</code> 的名字加版本号进行命名的。</p>
<p>至于部署，和前面没什么太大区别， <code>helm install saythx-0.1.0.tgz</code> 即可。</p>
<h3 id="访问服务"><a href="#访问服务" class="headerlink" title="访问服务"></a>访问服务</h3><p>前面在部署完成后，有一些返回信息，我们来按照其内容访问我们的服务：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">➜  saythx export NODE_PORT&#x3D;$(kubectl get --namespace work -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services saythx-frontend)</span><br><span class="line">➜  saythx export NODE_IP&#x3D;$(kubectl get nodes --namespace work -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">➜  saythx echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br><span class="line">http:&#x2F;&#x2F;172.17.0.5:30300</span><br><span class="line">➜  saythx curl http:&#x2F;&#x2F;172.17.0.5:30300</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;en&gt;&lt;head&gt;&lt;meta charset&#x3D;utf-8&gt;&lt;meta http-equiv&#x3D;X-UA-Compatible content&#x3D;&quot;IE&#x3D;edge&quot;&gt;&lt;meta name&#x3D;viewport content&#x3D;&quot;width&#x3D;device-width,initial-scale&#x3D;1&quot;&gt;&lt;link rel&#x3D;icon href&#x3D;&#x2F;favicon.ico&gt;&lt;title&gt;fe&lt;&#x2F;title&gt;&lt;link href&#x3D;&#x2F;css&#x2F;app.0a6f0b04.css rel&#x3D;preload as&#x3D;style&gt;&lt;link href&#x3D;&#x2F;css&#x2F;chunk-vendors.ea3fa8e3.css rel&#x3D;preload as&#x3D;style&gt;&lt;link href&#x3D;&#x2F;js&#x2F;app.ee469174.js rel&#x3D;preload as&#x3D;script&gt;&lt;link href&#x3D;&#x2F;js&#x2F;chunk-vendors.14b9b088.js rel&#x3D;preload as&#x3D;script&gt;&lt;link href&#x3D;&#x2F;css&#x2F;chunk-vendors.ea3fa8e3.css rel&#x3D;stylesheet&gt;&lt;link href&#x3D;&#x2F;css&#x2F;app.0a6f0b04.css rel&#x3D;stylesheet&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;noscript&gt;&lt;strong&gt;We&#39;re sorry but fe doesn&#39;t work properly without JavaScript enabled. Please enable it to continue.&lt;&#x2F;strong&gt;&lt;&#x2F;noscript&gt;&lt;div id&#x3D;app&gt;&lt;&#x2F;div&gt;&lt;script src&#x3D;&#x2F;js&#x2F;chunk-vendors.14b9b088.js&gt;&lt;&#x2F;script&gt;&lt;script src&#x3D;&#x2F;js&#x2F;app.ee469174.js&gt;&lt;&#x2F;script&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure></div>

<p>服务可以正常访问。</p>
<h2 id="总结-9"><a href="#总结-9" class="headerlink" title="总结"></a>总结</h2><p>通过本节我们学习到了 <code>chart</code> 的实际结构，及编写方式。以及编写了我们自己的 <code>chart</code> 并使用该 <code>chart</code> 部署了服务。</p>
<p>示例项目还仅仅是个小项目，试想当我们需要部署一个大型项目，如果不通过类似 Helm 这样的软件进行管理，每次的更新发布，维护 <code>YAML</code> 的配置文件就会很繁琐了。</p>
<p>另外，Helm 的功能还不仅限于此，使用 Helm 我们还可以管理 <code>Release</code> ，并进行更新回滚等操作。以及，我们可以搭建自己的私有 <code>chart</code> 仓库等。</p>
<p>下节开始，我们将进入深入学习阶段，逐个讲解 K8S 的核心组件，以便后续遇到问题时，可快速定位和解决。</p>
<h1 id="庖丁解牛：kube-apiserver"><a href="#庖丁解牛：kube-apiserver" class="headerlink" title="庖丁解牛：kube-apiserver"></a>庖丁解牛：kube-apiserver</h1><h2 id="整体概览-4"><a href="#整体概览-4" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们初次认识到了 <code>kube-apiserver</code> 的存在（以下内容中将统一称之为 <code>kube-apiserver</code>），知道了它作为集群的统一入口，接收来自外部的信号和请求，并将一些信息存储至 <code>etcd</code> 中。</p>
<p>但这只是一种很模糊的说法，本节我们来具体看看 <code>kube-apiserver</code> 的关键功能以及它的工作原理。</p>
<p>注意：本节所有的源码均以 <code>v1.11.3</code> 为准 commit id <code>a4529464e4629c21224b3d52edfe0ea91b072862</code>。</p>
<h2 id="REST-API-Server"><a href="#REST-API-Server" class="headerlink" title="REST API Server"></a>REST API Server</h2><p>先来说下 <code>kube-apiserver</code> 作为整个集群的入口，接受外部的信号和请求所应该具备的基本功能。</p>
<p>首先，它对外提供接口，可处理来自客户端（无论我们在用的 <code>kubeclt</code> 或者 <code>curl</code> 或者其他语言实现的客户端）的请求，并作出响应。</p>
<p>在第 5 节搭建集群时，我们提到要先去检查 <code>6443</code> 端口是否被占用。这样检查的原因在于 <code>kube-apiserver</code> 有个 <code>--secure-port</code> 的参数，通过这个参数来配置它将要监听在哪个端口，默认情况下是 <code>6443</code>。</p>
<p>当然，它还有另一个参数 <code>--insecure-port</code> ，这个参数可将 <code>kube-apiserver</code> 绑定到其指定的端口上，且通过该端口访问时无需认证。</p>
<p>在生产环境中，建议将其设置为 <code>0</code> 以禁用该功能。另外，这个参数也已经被标记为废弃，将在之后版本中移除。如果未禁用该功能，建议通过防火墙策略禁止从外部访问该端口。该端口会绑定在 <code>--insecure-bind-address</code> 参数所设置的地址上，默认为 <code>127.0.0.1</code>。</p>
<p>那么 <code>secure</code> 和 <code>insecure</code> 最主要的区别是什么呢？ 这就引出来了 <code>kube-apiserver</code> 作为 API Server 的一个最主要功能：认证。</p>
<h3 id="认证（Authentication）-1"><a href="#认证（Authentication）-1" class="headerlink" title="认证（Authentication）"></a>认证（Authentication）</h3><p>在第 8 节《认证和授权》中，我们已经讲过认证相关的机制。这里，我们以最简单的获取集群版本号为例。</p>
<p>通常，我们使用 <code>kubeclt version</code> 来获取集群和当前客户端的版本号。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:53:03Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>获取集群版本号的时候，其实也是向 <code>kube-apiserver</code> 发送了一个请求进行查询的，我们可以通过传递 <code>-v</code> 参数来改变 log level 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl version -v 8</span><br><span class="line">I1202 03:15:06.360838   13581 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:15:06.362106   13581 round_trippers.go:383] GET https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;version?timeout&#x3D;32s</span><br><span class="line">I1202 03:15:06.362130   13581 round_trippers.go:390] Request Headers:</span><br><span class="line">I1202 03:15:06.362139   13581 round_trippers.go:393]     Accept: application&#x2F;json, *&#x2F;*</span><br><span class="line">I1202 03:15:06.362146   13581 round_trippers.go:393]     User-Agent: kubectl&#x2F;v1.11.3 (linux&#x2F;amd64) kubernetes&#x2F;a452946</span><br><span class="line">I1202 03:15:06.377653   13581 round_trippers.go:408] Response Status: 200 OK in 15 milliseconds</span><br><span class="line">I1202 03:15:06.377678   13581 round_trippers.go:411] Response Headers:</span><br><span class="line">I1202 03:15:06.377686   13581 round_trippers.go:414]     Content-Type: application&#x2F;json</span><br><span class="line">I1202 03:15:06.377693   13581 round_trippers.go:414]     Content-Length: 263</span><br><span class="line">I1202 03:15:06.377699   13581 round_trippers.go:414]     Date: Sun, 02 Dec 2018 03:15:06 GMT</span><br><span class="line">I1202 03:15:06.379314   13581 request.go:897] Response Body: &#123;</span><br><span class="line">  &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;minor&quot;: &quot;11&quot;,</span><br><span class="line">  &quot;gitVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;gitCommit&quot;: &quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;,</span><br><span class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">  &quot;buildDate&quot;: &quot;2018-09-09T17:53:03Z&quot;,</span><br><span class="line">  &quot;goVersion&quot;: &quot;go1.10.3&quot;,</span><br><span class="line">  &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">  &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">&#125;</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:53:03Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>通过日志就可以很明显看到，首先会加载 <code>$HOME/.kube/config</code> 下的配置，获的集群地址，进而请求 <code>/version</code> 接口，最后格式化输出。</p>
<p>我们使用 <code>curl</code> 去请求同样的接口：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ curl -k https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;version</span><br><span class="line">&#123;</span><br><span class="line">  &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;minor&quot;: &quot;11&quot;,</span><br><span class="line">  &quot;gitVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;gitCommit&quot;: &quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;,</span><br><span class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">  &quot;buildDate&quot;: &quot;2018-09-09T17:53:03Z&quot;,</span><br><span class="line">  &quot;goVersion&quot;: &quot;go1.10.3&quot;,</span><br><span class="line">  &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">  &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>得到了相同的结果。你可能会有些奇怪，使用 <code>curl -k</code> 相当于忽略了认证的过程，为何还能拿到正确的信息。别急，我们来看下一个例子：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl get ns  -v 8</span><br><span class="line">I1202 03:25:40.607886   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.608862   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.611187   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.622737   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.623495   16620 round_trippers.go:383] GET https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;api&#x2F;v1&#x2F;namespaces?limit&#x3D;500</span><br><span class="line">I1202 03:25:40.623650   16620 round_trippers.go:390] Request Headers:</span><br><span class="line">I1202 03:25:40.623730   16620 round_trippers.go:393]     Accept: application&#x2F;json;as&#x3D;Table;v&#x3D;v1beta1;g&#x3D;meta.k8s.io, application&#x2F;json</span><br><span class="line">I1202 03:25:40.623820   16620 round_trippers.go:393]     User-Agent: kubectl&#x2F;v1.11.3 (linux&#x2F;amd64) kubernetes&#x2F;a452946</span><br><span class="line">I1202 03:25:40.644280   16620 round_trippers.go:408] Response Status: 200 OK in 20 milliseconds</span><br><span class="line">I1202 03:25:40.644308   16620 round_trippers.go:411] Response Headers:</span><br><span class="line">I1202 03:25:40.644327   16620 round_trippers.go:414]     Content-Type: application&#x2F;json</span><br><span class="line">I1202 03:25:40.644334   16620 round_trippers.go:414]     Content-Length: 2061</span><br><span class="line">I1202 03:25:40.644338   16620 round_trippers.go:414]     Date: Sun, 02 Dec 2018 03:25:40 GMT</span><br><span class="line">I1202 03:25:40.644398   16620 request.go:897] Response Body: &#123;&quot;kind&quot;:&quot;Table&quot;,&quot;apiVersion&quot;:&quot;meta.k8s.io&#x2F;v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;&#x2F;api&#x2F;v1&#x2F;namespaces&quot;,&quot;resourceVersion&quot;:&quot;3970&quot;&#125;,&quot;columnDefinitions&quot;:[&#123;&quot;name&quot;:&quot;Name&quot;,&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;name&quot;,&quot;description&quot;:&quot;Name must be unique within anamespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;user-guide&#x2F;identifiers#names&quot;,&quot;priority&quot;:0&#125;,&#123;&quot;name&quot;:&quot;Status&quot;,&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;&quot;,&quot;description&quot;:&quot;The status of the namespace&quot;,&quot;priority&quot;:0&#125;,&#123;&quot;name&quot;:&quot;Age&quot;,&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;&quot;,&quot;description&quot;:&quot;CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\n\nPopulated by the system. Read-only. [truncated 1037 chars]</span><br><span class="line">I1202 03:25:40.645111   16620 get.go:443] no kind is registered for the type v1beta1.Table</span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">default       Active    45m</span><br><span class="line">kube-public   Active    45m</span><br><span class="line">kube-system   Active    45m</span><br></pre></td></tr></table></figure></div>

<p>使用 <code>curl</code> 去请求：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ curl -k  https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;api&#x2F;v1&#x2F;namespaces</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;namespaces is forbidden: User \&quot;system:anonymous\&quot; cannot list namespaces at the cluster scope&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;namespaces&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>看到这里，应该就很明显了，当前忽略掉认证过程的 <code>curl</code> 被判定为 <code>system:anonymous</code> 用户，而此用户不具备列出 <code>namespace</code> 的权限。</p>
<p>那我们是否有其他办法使用 <code>curl</code> 获取资源呢？ 当然有，使用 <code>kubectl proxy</code> 可以在本地和集群之间创建一个代理，就像这样：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl proxy &amp;</span><br><span class="line">[1] 22205</span><br><span class="line">master $ Starting to serve on 127.0.0.1:8001</span><br><span class="line"></span><br><span class="line">master $ curl http:&#x2F;&#x2F;127.0.0.1:8001&#x2F;api&#x2F;v1&#x2F;namespaces</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;NamespaceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;5363&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;a5124131-f5db-11e8-9237-0242ac110063&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;4&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2018-12-02T02:40:35Z&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">          &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;kube-public&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-public&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;a5153f73-f5db-11e8-9237-0242ac110063&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;10&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2018-12-02T02:40:35Z&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">          &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;kube-system&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;a514ad25-f5db-11e8-9237-0242ac110063&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;9&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2018-12-02T02:40:35Z&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">          &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>可以看到已经能正确的获取资源了，这是因为 <code>kubectl proxy</code> 使用了 <code>$HOME/.kube/config</code> 中的配置。</p>
<p>在 <code>staging/src/k8s.io/client-go/tools/clientcmd/loader.go</code> 中，有一个名为 <code>LoadFromFile</code> 的函数用来提供加载配置文件的功能。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func LoadFromFile(filename string) (*clientcmdapi.Config, error) &#123;</span><br><span class="line">	kubeconfigBytes, err :&#x3D; ioutil.ReadFile(filename)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	config, err :&#x3D; Load(kubeconfigBytes)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	glog.V(6).Infoln(&quot;Config loaded from file&quot;, filename)</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; set LocationOfOrigin on every Cluster, User, and Context</span><br><span class="line">	for key, obj :&#x3D; range config.AuthInfos &#123;</span><br><span class="line">		obj.LocationOfOrigin &#x3D; filename</span><br><span class="line">		config.AuthInfos[key] &#x3D; obj</span><br><span class="line">	&#125;</span><br><span class="line">	for key, obj :&#x3D; range config.Clusters &#123;</span><br><span class="line">		obj.LocationOfOrigin &#x3D; filename</span><br><span class="line">		config.Clusters[key] &#x3D; obj</span><br><span class="line">	&#125;</span><br><span class="line">	for key, obj :&#x3D; range config.Contexts &#123;</span><br><span class="line">		obj.LocationOfOrigin &#x3D; filename</span><br><span class="line">		config.Contexts[key] &#x3D; obj</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if config.AuthInfos &#x3D;&#x3D; nil &#123;</span><br><span class="line">		config.AuthInfos &#x3D; map[string]*clientcmdapi.AuthInfo&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	if config.Clusters &#x3D;&#x3D; nil &#123;</span><br><span class="line">		config.Clusters &#x3D; map[string]*clientcmdapi.Cluster&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	if config.Contexts &#x3D;&#x3D; nil &#123;</span><br><span class="line">		config.Contexts &#x3D; map[string]*clientcmdapi.Context&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return config, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>逻辑其实很简单，读取指定的文件（一般在调用此函数前，都会先去检查是否有 <code>KUBECONFIG</code> 的环境变量或 <code>--kubeconfig</code>，如果没有才会使用默认的 <code>$HOME/.kube/config</code> 作为文件名）。</p>
<p>从以上的例子中，使用当前配置的用户可以获取资源，而 <code>system:anonymous</code> 不可以。可以得出 <code>kube-apiserver</code> 又一个重要的功能：授权。</p>
<h3 id="授权（Authorization）-1"><a href="#授权（Authorization）-1" class="headerlink" title="授权（Authorization）"></a>授权（Authorization）</h3><p>在第 8 节中，我们也已经讲过，K8S 支持多种授权机制，现在多数都在使用 <code>RBAC</code> ，我们之前使用 <code>kubeadm</code> 创建集群时，默认会开启 <code>RBAC</code>。如何创建权限可控的用户在第 8 节也已经说过。所以本节中不过多赘述了，直接看授权后的处理逻辑。</p>
<h3 id="准入控制（Admission-Control）"><a href="#准入控制（Admission-Control）" class="headerlink" title="准入控制（Admission Control）"></a>准入控制（Admission Control）</h3><p>在请求进来时，会先经过认证、授权接下来会进入准入控制环节。准入控制和前两项内容不同，它不只是关注用户和行为，它还会处理请求的内容。不过它对读操作无效。</p>
<p>准入控制与我们前面说提到的认证、授权插件类似，支持同时开启多个。在 <code>v1.11.3</code> 中，默认开启的准入控制插件有：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeClaimResize,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,Priority</span><br></pre></td></tr></table></figure></div>

<p>相关的代码可查看 <code>pkg/kubeapiserver/options/plugins.go</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func DefaultOffAdmissionPlugins() sets.String &#123;</span><br><span class="line">	defaultOnPlugins :&#x3D; sets.NewString(</span><br><span class="line">		lifecycle.PluginName,                &#x2F;&#x2F;NamespaceLifecycle</span><br><span class="line">		limitranger.PluginName,              &#x2F;&#x2F;LimitRanger</span><br><span class="line">		serviceaccount.PluginName,           &#x2F;&#x2F;ServiceAccount</span><br><span class="line">		setdefault.PluginName,               &#x2F;&#x2F;DefaultStorageClass</span><br><span class="line">		resize.PluginName,                   &#x2F;&#x2F;PersistentVolumeClaimResize</span><br><span class="line">		defaulttolerationseconds.PluginName, &#x2F;&#x2F;DefaultTolerationSeconds</span><br><span class="line">		mutatingwebhook.PluginName,          &#x2F;&#x2F;MutatingAdmissionWebhook</span><br><span class="line">		validatingwebhook.PluginName,        &#x2F;&#x2F;ValidatingAdmissionWebhook</span><br><span class="line">		resourcequota.PluginName,            &#x2F;&#x2F;ResourceQuota</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	if utilfeature.DefaultFeatureGate.Enabled(features.PodPriority) &#123;</span><br><span class="line">		defaultOnPlugins.Insert(podpriority.PluginName) &#x2F;&#x2F;PodPriority</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return sets.NewString(AllOrderedPlugins...).Difference(defaultOnPlugins)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在这里写了一些默认开启的配置。事实上，在早之前，<code>PersistentVolumeClaimResize</code> 默认是不开启的，并且开启了 <code>PersistentVolumeLabel</code>，对于移除 <code>Persistentvolumelabel</code> 感兴趣的朋友可以参考下 <a href="https://github.com/kubernetes/kubernetes/issues/52617">Remove the PersistentVolumeLabel Admission Controller</a> 。</p>
<p>这里对几个比较常见的插件做下说明：</p>
<ul>
<li><p>NamespaceLifecycle：它可以保证正在终止的 <code>Namespace</code> 不允许创建对象，不允许请求不存在的 <code>Namespace</code> 以及保证默认的 <code>default</code>, <code>kube-system</code> 之类的命名空间不被删除。核心的代码是：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">if a.GetOperation() &#x3D;&#x3D; admission.Delete &amp;&amp; a.GetKind().GroupKind() &#x3D;&#x3D; v1.SchemeGroupVersion.WithKind(&quot;Namespace&quot;).GroupKind() &amp;&amp; l.immortalNamespaces.Has(a.GetName()) &#123;</span><br><span class="line">	return errors.NewForbidden(a.GetResource().GroupResource(), a.GetName(), fmt.Errorf(&quot;this namespace may not be deleted&quot;))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>如果删除默认的 <code>Namespace</code> 则会得到下面的异常：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl delete ns kube-system</span><br><span class="line">Error from server (Forbidden): namespaces &quot;kube-system&quot; is forbidden: this namespace may not be deleted</span><br><span class="line">master $ kubectl delete ns kube-public</span><br><span class="line">Error from server (Forbidden): namespaces &quot;kube-public&quot; is forbidden: this namespace may not be deleted</span><br><span class="line">master $ kubectl delete ns default</span><br><span class="line">Error from server (Forbidden): namespaces &quot;default&quot; is forbidden: this namespace may not be deleted</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>LimitRanger：为 <code>Pod</code> 设置默认请求资源的限制。</p>
</li>
<li><p>ServiceAccount：可按照预设规则创建 <code>Serviceaccount</code> 。比如都有统一的前缀：<code>system:serviceaccount:</code>。</p>
</li>
<li><p>DefaultStorageClass：为 <code>PVC</code> 设置默认 <code>StorageClass</code>。</p>
</li>
<li><p>DefaultTolerationSeconds：设置 <code>Pod</code> 的默认 forgiveness toleration 为 5 分钟。这个可能常会看到。</p>
</li>
<li><p>MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook：这两个都是通过 Webhook 验证或者修改请求，唯一的区别是一个是顺序进行，一个是并行进行的。</p>
</li>
<li><p>ResourceQuota：限制 <code>Pod</code> 请求配额。</p>
</li>
<li><p>AlwaysPullImages：总是拉取镜像。</p>
</li>
<li><p>AlwaysAdmit：总是接受所有请求。</p>
</li>
</ul>
<h3 id="处理请求"><a href="#处理请求" class="headerlink" title="处理请求"></a>处理请求</h3><p>前面已经说到，一个请求依次会经过认证，授权，准入控制等环节，当这些环节都已经通过后，该请求便到了 <code>kube-apiserver</code> 的实际处理逻辑中了。</p>
<p>其实和普通的 Web server 类似，<code>kube-apiserver</code> 提供了 <code>restful</code> 的接口，增删改查等基本功能都基本类似。这里先暂时不再深入。</p>
<h2 id="总结-10"><a href="#总结-10" class="headerlink" title="总结"></a>总结</h2><p>通过本节，我们学习到了 <code>kube-apiserver</code> 的基本工作逻辑，各类 API 请求先后通过认证，授权，准入控制等一系列环节后，进入到 <code>kube-apiserver</code> 的 <code>Registry</code> 进行相关逻辑处理。</p>
<p>至于需要进行持久化或者需要与后端存储交互的部分，我们在下节会介绍 <code>etcd</code> 到时再看 K8S 是如何将后端存储抽象化，从 <code>etcd</code> v2 升级至 v3 的。</p>
<p><code>kube-apiserver</code> 包含的东西有很多，当你在终端下执行 <code>./kube-apiserver -h</code> 时，会发现有大量的参数。</p>
<p>这些参数除了认证，授权，准入控制相关功能</p>
<h1 id="庖丁解牛：etcd"><a href="#庖丁解牛：etcd" class="headerlink" title="庖丁解牛：etcd"></a>庖丁解牛：etcd</h1><h2 id="整体概览-5"><a href="#整体概览-5" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>etcd</code> 的存在，知道了 Master 是 K8S 是集群的大脑，而 <code>etcd</code> 则是大脑的核心。为什么这么说？本节我们一同来看看 <code>etcd</code> 为何如此重要。</p>
<h2 id="etcd-是什么"><a href="#etcd-是什么" class="headerlink" title="etcd 是什么"></a><code>etcd</code> 是什么</h2><p>先摘录<a href="https://etcd.readthedocs.io/en/latest/faq.html#what-is-etcd" target="_blank" rel="noopener">官方文档</a>的一句说明:</p>
<blockquote>
<p>etcd is a consistent distributed key-value store. Mainly used as a separate coordination service, in distributed systems. And designed to hold small amounts of data that can fit entirely in memory.</p>
</blockquote>
<p><code>etcd</code> 是由 CoreOS 团队发起的一个分布式，强一致的键值存储。它用 Go 语言编写，使用 <code>Raft</code> 协议作为一致性算法。多数情况下会用于分布式系统中的服务注册发现，或是用于存储系统的关键数据。</p>
<h2 id="etcd-有什么作用"><a href="#etcd-有什么作用" class="headerlink" title="etcd 有什么作用"></a><code>etcd</code> 有什么作用</h2><p><code>etcd</code> 在 K8S 中，最主要的作用便是其高可用，强一致的键值存储以及监听机制。</p>
<p>在 <code>kube-apiserver</code> 收到对应请求经过一系列的处理后，最终如果是集群所需要存储的数据，便会存储至 <code>etcd</code> 中。主部分主要是集群状态信息和元信息。</p>
<p>我们来实际操作 K8S 集群中的 <code>etcd</code> 看下真实情况。</p>
<ul>
<li>查找集群中的 <code>etcd</code> Pod</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># 默认集群中的 etcd 会放到 kube-system Namespace 中</span><br><span class="line">master $ kubectl -n kube-system get pods | grep etcd</span><br><span class="line">etcd-master                      1&#x2F;1       Running   0          1h</span><br></pre></td></tr></table></figure></div>

<ul>
<li>进入该 Pod 并查看 <code>etcd</code> 集群的 <code>member</code></li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system exec -it etcd-master sh</span><br><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt member list</span><br><span class="line">a874c87fd42044f, started, master, https:&#x2F;&#x2F;127.0.0.1:2380, https:&#x2F;&#x2F;127.0.0.1:2379</span><br></pre></td></tr></table></figure></div>

<p>这里由于在 K8S 1.11.3 中默认使用的是 <code>etcd</code> 3.2 版本，所以需要加入 <code>ETCDCTL_API=3</code> 的环境变量，且 <code>etcd</code> 从 2 到 3 很明显的一个变化也是使用上的变化，在 2 中是 HTTP 接口的。</p>
<p>我们通过传递证书等相关参数进去，完成校验，查看 <code>member</code> 。</p>
<ul>
<li>查看存储的元信息</li>
</ul>
<p><code>etcd</code> 中存储的 K8S 集群元信息基本都是 <code>/registry</code> 下，我们可通过下面的方式进行查看</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt get &#x2F;registry --prefix --keys-only</span><br><span class="line">&#x2F;registry&#x2F;apiregistration.k8s.io&#x2F;apiservices&#x2F;v1.</span><br><span class="line">&#x2F;registry&#x2F;clusterroles&#x2F;system:aggregate-to-admin</span><br><span class="line">&#x2F;registry&#x2F;configmaps&#x2F;kube-public&#x2F;cluster-info</span><br><span class="line">&#x2F;registry&#x2F;masterleases&#x2F;172.17.0.53</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;default</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-public</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-system</span><br><span class="line">&#x2F;registry&#x2F;pods&#x2F;kube-system&#x2F;etcd-master</span><br><span class="line">&#x2F;registry&#x2F;pods&#x2F;kube-system&#x2F;kube-apiserver-master</span><br><span class="line">&#x2F;registry&#x2F;ranges&#x2F;serviceips</span><br><span class="line">&#x2F;registry&#x2F;ranges&#x2F;servicenodeports</span><br><span class="line">...</span><br><span class="line"># 篇幅原因，删掉了很多输出</span><br></pre></td></tr></table></figure></div>

<p>可以看到有各种类型的资源。我们直接以 Namespaces 为例。</p>
<ul>
<li>查看 Namespaces 信息</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt get &#x2F;registry&#x2F;namespaces --prefix --keys-only</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;default</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-public</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-system</span><br></pre></td></tr></table></figure></div>

<ul>
<li>使用 <code>kubectl</code> 创建名为 <code>moelove</code> 的 Namespaces</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl create  ns moelove</span><br><span class="line">namespace&#x2F;moelove created</span><br></pre></td></tr></table></figure></div>

<ul>
<li>查看 Namespaces 信息</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt get &#x2F;registry&#x2F;namespaces --prefix --keys-only</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;default</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-public</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-system</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;moelove</span><br></pre></td></tr></table></figure></div>

<p>发现刚创建的 <code>moelove</code> 的 Namespaces 已经在 <code>etcd</code> 中了。</p>
<h2 id="etcd-是如何被使用的"><a href="#etcd-是如何被使用的" class="headerlink" title="etcd 是如何被使用的"></a><code>etcd</code> 是如何被使用的</h2><p>首先，在 <code>staging/src/k8s.io/apiserver/pkg/server/options/etcd.go</code> 中，存在一处声明：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">var storageTypes &#x3D; sets.NewString(</span><br><span class="line">	storagebackend.StorageTypeUnset,</span><br><span class="line">	storagebackend.StorageTypeETCD2,</span><br><span class="line">	storagebackend.StorageTypeETCD3,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<p><strong>在 1.13 发布时，<code>etcd 2</code> 的相关代码已经移除，其中就包含上面声明中的 <code>storagebackend.StorageTypeETCD2</code></strong></p>
<p>这里是在过渡期间为了同时兼容 <code>etcd</code> 2 和 3 而增加的。</p>
<p>我们来看下实际对各类资源的操作，还是以对 <code>Namespace</code> 的操作为例：代码在 <code>pkg/registry/core/namespace/storage/storage.go</code> 中，</p>
<p>比如，<code>Get</code>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func (r *REST) Get(ctx context.Context, name string, options *metav1.GetOptions) (runtime.Object, error) &#123;</span><br><span class="line">	return r.store.Get(ctx, name, options)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>而 <code>REST</code> 则是这样定义的：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">type REST struct &#123;</span><br><span class="line">	store  *genericregistry.Store</span><br><span class="line">	status *genericregistry.Store</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>通过 <code>REST</code> 实现了一个 <code>RESTStorage</code>，实际使用时，也是调用了 <code>staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go</code> 对接口的封装。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func (e *Store) Get(ctx context.Context, name string, options *metav1.GetOptions) (runtime.Object, error) &#123;</span><br><span class="line">	obj :&#x3D; e.NewFunc()</span><br><span class="line">	key, err :&#x3D; e.KeyFunc(ctx, name)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	if err :&#x3D; e.Storage.Get(ctx, key, options.ResourceVersion, obj, false); err !&#x3D; nil &#123;</span><br><span class="line">		return nil, storeerr.InterpretGetError(err, e.qualifiedResourceFromContext(ctx), name)</span><br><span class="line">	&#125;</span><br><span class="line">	if e.Decorator !&#x3D; nil &#123;</span><br><span class="line">		if err :&#x3D; e.Decorator(obj); err !&#x3D; nil &#123;</span><br><span class="line">			return nil, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	return obj, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在该处实现了对各类基本方法的封装，各种资源类型都会统一去调用。而更上层则是对 <code>storagebackend</code> 的统一封装，最终会调用 <code>etcd</code> 客户端的实现完成想对应的操作，这里就不再多展开了。</p>
<h2 id="总结-11"><a href="#总结-11" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>etcd</code> 以及它在 K8S 中的作用以及如何使用。我们还介绍了如何通过 <code>etcdctl</code> 工具去操作 <code>etcd</code>。</p>
<p>在某些极端情况下，也许你需要通过直接操作 <code>etcd</code> 集群去变更数据，这里没有介绍所有的操作命令，感兴趣的可以自行通过下方的链接看官方文档进行学习。</p>
<p>但通常情况下，不建议直接操作 <code>etcd</code> ，除非你已经明确自己在做什么。</p>
<p>另外，由于 <code>etcd</code> 集群使用 <code>Raft</code> 一致性算法，通常情况下 <code>etcd</code> 集群需要部署奇数个节点，如 3，5，7 等。<code>etcd</code> 集群维护也相对容易，很容易可以做成高可用集群。（这也是保障 K8S 集群高可用的重要一环）</p>
<p>学习了 <code>etcd</code> 之后，下节我们来学习同样很重要的一个组件 <code>Controller Manager</code>，学习它是如何保障节群符合预期状态的。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://etcd.readthedocs.io/en/latest/faq.html#what-is-etcd" target="_blank" rel="noopener">What is etcd</a></li>
</ul>
<h1 id="庖丁解牛：controller-manager"><a href="#庖丁解牛：controller-manager" class="headerlink" title="庖丁解牛：controller-manager"></a>庖丁解牛：controller-manager</h1><h2 id="整体概览-6"><a href="#整体概览-6" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>Controller Manager</code> 的存在，知道了 Master 是 K8S 是集群的大脑，而它则是 Master 中最繁忙的部分。为什么这么说？本节我们一同来看看它为何如此繁忙。</p>
<p><strong>注意：<code>Controller Manager</code> 实际由 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code> 两部分组成，<code>cloud-controller-manager</code> 则是为各家云厂商提供了一个抽象的封装，便于让各厂商使用各自的 <code>provide</code>。本文只讨论 <code>kube-controller-manager</code>，为了避免混淆，下文统一使用 <code>kube-controller-manager</code>。</strong></p>
<h2 id="kube-controller-manager-是什么"><a href="#kube-controller-manager-是什么" class="headerlink" title="kube-controller-manager 是什么"></a><code>kube-controller-manager</code> 是什么</h2><p>一句话来讲 <code>kube-controller-manager</code> 是一个嵌入了 K8S 核心控制循环的守护进程。</p>
<p>这里的重点是</p>
<ul>
<li>嵌入：它已经内置了相关逻辑，可独立进行部署。我们在第 5 节下载 K8S 服务端二进制文件解压后，便可以看到 <code>kube-controller-manager</code> 的可执行文件，不过我们使用的是 <code>kubeadm</code> 进行的部署，它会默认使用 <code>k8s.gcr.io/kube-controller-manager</code> 的镜像。我们直接来看下实际情况：</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system describe pods -l component&#x3D;kube-controller-manager</span><br><span class="line">Name:               kube-controller-manager-master</span><br><span class="line">Namespace:          kube-system</span><br><span class="line">Priority:           2000000000</span><br><span class="line">PriorityClassName:  system-cluster-critical</span><br><span class="line">Node:               master&#x2F;172.17.0.35</span><br><span class="line">Start Time:         Mon, 10 Dec 2018 07:14:21 +0000</span><br><span class="line">Labels:             component&#x3D;kube-controller-manager</span><br><span class="line">                    tier&#x3D;control-plane</span><br><span class="line">Annotations:        kubernetes.io&#x2F;config.hash&#x3D;c7ed7a8fa5c430410e84970f8ee7e067</span><br><span class="line">                    kubernetes.io&#x2F;config.mirror&#x3D;c7ed7a8fa5c430410e84970f8ee7e067</span><br><span class="line">                    kubernetes.io&#x2F;config.seen&#x3D;2018-12-10T07:14:21.685626322Z</span><br><span class="line">                    kubernetes.io&#x2F;config.source&#x3D;file</span><br><span class="line">                    scheduler.alpha.kubernetes.io&#x2F;critical-pod&#x3D;</span><br><span class="line">Status:             Running</span><br><span class="line">IP:                 172.17.0.35</span><br><span class="line">Containers:</span><br><span class="line">  kube-controller-manager:</span><br><span class="line">    Container ID:  docker:&#x2F;&#x2F;0653e71ae4287608726490b724c3d064d5f1556dd89b7d3c618e97f0e7f2a533</span><br><span class="line">    Image:         k8s.gcr.io&#x2F;kube-controller-manager-amd64:v1.11.3</span><br><span class="line">    Image ID:      docker-pullable:&#x2F;&#x2F;k8s.gcr.io&#x2F;kube-controller-manager-amd64@sha256:a6d115bb1c0116036ac6e6e4d504665bc48879c421a450566c38b3b726f0a123</span><br><span class="line">    Port:          &lt;none&gt;</span><br><span class="line">    Host Port:     &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      kube-controller-manager</span><br><span class="line">      --address&#x3D;127.0.0.1</span><br><span class="line">      --cluster-signing-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt</span><br><span class="line">      --cluster-signing-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key</span><br><span class="line">      --controllers&#x3D;*,bootstrapsigner,tokencleaner</span><br><span class="line">      --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf</span><br><span class="line">      --leader-elect&#x3D;true</span><br><span class="line">      --root-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt</span><br><span class="line">      --service-account-private-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sa.key</span><br><span class="line">      --use-service-account-credentials&#x3D;true</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Mon, 10 Dec 2018 07:14:24 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        200m</span><br><span class="line">    Liveness:     http-get http:&#x2F;&#x2F;127.0.0.1:10252&#x2F;healthz delay&#x3D;15s timeout&#x3D;15s period&#x3D;10s #success&#x3D;1 #failure&#x3D;8</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      &#x2F;etc&#x2F;ca-certificates from etc-ca-certificates (ro)</span><br><span class="line">      &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf from kubeconfig (ro)</span><br><span class="line">      &#x2F;etc&#x2F;kubernetes&#x2F;pki from k8s-certs (ro)</span><br><span class="line">      &#x2F;etc&#x2F;ssl&#x2F;certs from ca-certs (ro)</span><br><span class="line">      &#x2F;usr&#x2F;libexec&#x2F;kubernetes&#x2F;kubelet-plugins&#x2F;volume&#x2F;exec from flexvolume-dir (rw)</span><br><span class="line">      &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates from usr-local-share-ca-certificates (ro)</span><br><span class="line">      &#x2F;usr&#x2F;share&#x2F;ca-certificates from usr-share-ca-certificates (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             True</span><br><span class="line">  ContainersReady   True</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  usr-share-ca-certificates:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;usr&#x2F;share&#x2F;ca-certificates</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  usr-local-share-ca-certificates:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  etc-ca-certificates:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;ca-certificates</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  k8s-certs:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;kubernetes&#x2F;pki</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  ca-certs:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;ssl&#x2F;certs</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  kubeconfig:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf</span><br><span class="line">    HostPathType:  FileOrCreate</span><br><span class="line">  flexvolume-dir:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;usr&#x2F;libexec&#x2F;kubernetes&#x2F;kubelet-plugins&#x2F;volume&#x2F;exec</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">QoS Class:         Burstable</span><br><span class="line">Node-Selectors:    &lt;none&gt;</span><br><span class="line">Tolerations:       :NoExecute</span><br><span class="line">Events:            &lt;none&gt;</span><br><span class="line">master</span><br></pre></td></tr></table></figure></div>

<p>这是使用 <code>kubeadm</code> 搭建的集群中的 <code>kube-controller-manager</code> 的 <code>Pod</code>，首先可以看到它所使用的镜像，其次可以看到它使用的一系列参数，最后它在 <code>10252</code> 端口提供了健康检查的接口。稍后我们再展开。</p>
<ul>
<li>控制循环：这里拆解为两部分： <strong>控制</strong> 和 <strong>循环</strong> ，它所控制的是集群的状态；至于循环它当然是会有个循环间隔的，这里有个参数可以进行控制。</li>
<li>守护进程：这个就不单独展开了。</li>
</ul>
<h2 id="kube-controller-manager-有什么作用"><a href="#kube-controller-manager-有什么作用" class="headerlink" title="kube-controller-manager 有什么作用"></a><code>kube-controller-manager</code> 有什么作用</h2><p>前面已经说了它一个很关键的点 “控制”：它通过 <code>kube-apiserver</code> 提供的信息持续的监控集群状态，并尝试将集群调整至预期的状态。由于访问 <code>kube-apiserver</code> 也需要通过认证，授权等过程，所以可以看到上面启动 <code>kube-controller-manager</code> 时提供了一系列的参数。</p>
<p>比如，当我们创建了一个 <code>Deployment</code>，默认副本数为 1 ，当我们把 <code>Pod</code> 删除后，<code>kube-controller-manager</code> 会按照原先的预期，重新创建一个 <code>Pod</code> 。下面举个例子：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl run redis --image&#x3D;&#39;redis&#39;</span><br><span class="line">deployment.apps&#x2F;redis created</span><br><span class="line">master $ kubectl get all</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-bb7894d65-w2rsp   1&#x2F;1       Running   0          3m</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   18m</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            1           3m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-bb7894d65   1         1         1         3m</span><br><span class="line">master $ kubectl delete pod&#x2F;redis-bb7894d65-w2rsp</span><br><span class="line">pod &quot;redis-bb7894d65-w2rsp&quot; deleted</span><br><span class="line">master $ kubectl get all  # 可以看到已经重新运行了一个 Pod</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-bb7894d65-62ftk   1&#x2F;1       Running   0          16s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   19m</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            1           4m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-bb7894d65   1         1         1         4m</span><br></pre></td></tr></table></figure></div>

<p>我们来看下 <code>kube-controller-manager</code> 的日志：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system logs -l component&#x3D;kube-controller-manager --tail&#x3D;5</span><br><span class="line">I1210 09:30:17.125377       1 node_lifecycle_controller.go:945] Controller detected that all Nodes are not-Ready. Entering master disruption mode.</span><br><span class="line">I1210 09:31:07.140539       1 node_lifecycle_controller.go:972] Controller detected that some Nodes are Ready. Exiting master disruption mode.</span><br><span class="line">I1210 09:43:30.377649       1 event.go:221] Event(v1.ObjectReference&#123;Kind:&quot;Deployment&quot;, Namespace:&quot;default&quot;, Name:&quot;redis&quot;, UID:&quot;0d1cb2d7-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps&#x2F;v1&quot;, ResourceVersion:&quot;1494&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;ScalingReplicaSet&#39; Scaled up replica setredis-bb7894d65 to 1</span><br><span class="line">I1210 09:43:30.835149       1 event.go:221] Event(v1.ObjectReference&#123;Kind:&quot;ReplicaSet&quot;, Namespace:&quot;default&quot;, Name:&quot;redis-bb7894d65&quot;, UID:&quot;0d344d15-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps&#x2F;v1&quot;, ResourceVersion:&quot;1495&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;SuccessfulCreate&#39; Created pod:redis-bb7894d65-w2rsp</span><br><span class="line">I1210 09:47:41.658781       1 event.go:221] Event(v1.ObjectReference&#123;Kind:&quot;ReplicaSet&quot;, Namespace:&quot;default&quot;, Name:&quot;redis-bb7894d65&quot;, UID:&quot;0d344d15-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps&#x2F;v1&quot;, ResourceVersion:&quot;1558&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;SuccessfulCreate&#39; Created pod:redis-bb7894d65-62ftk</span><br></pre></td></tr></table></figure></div>

<p>可以看到它先观察到有 <code>Deployment</code> 的事件，然后 <code>ScalingReplicaSet</code> 进而创建了对应的 <code>Pod</code>。 而当我们删掉正在运行的 <code>Pod</code> 后，它便会重新创建 <code>Pod</code> 使集群状态符合原先的预期状态。</p>
<p>同时，注意 <code>Pod</code> 的名字已经发生了变化。</p>
<h2 id="kube-controller-manager-是如何工作的"><a href="#kube-controller-manager-是如何工作的" class="headerlink" title="kube-controller-manager 是如何工作的"></a><code>kube-controller-manager</code> 是如何工作的</h2><p>在 <code>cmd/kube-controller-manager/app/controllermanager.go</code> 中列出了大多数的 <code>controllermanager</code>，他们对 <code>controllermanager</code> 函数的实际调用都在 <code>cmd/kube-controller-manager/app/core.go</code> 中，我们以 <code>PodGC</code> 为例：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func startPodGCController(ctx ControllerContext) (bool, error) &#123;</span><br><span class="line">	go podgc.NewPodGC(</span><br><span class="line">		ctx.ClientBuilder.ClientOrDie(&quot;pod-garbage-collector&quot;),</span><br><span class="line">		ctx.InformerFactory.Core().V1().Pods(),</span><br><span class="line">		int(ctx.ComponentConfig.PodGCController.TerminatedPodGCThreshold),</span><br><span class="line">	).Run(ctx.Stop)</span><br><span class="line">	return true, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在前两节中我们已经对 <code>kube-apiserver</code> 和 <code>etcd</code> 有了一些基本的认识，这里它主要会去 watch 相关的资源，但是出于性能上的考虑，也不能过于频繁的去请求 <code>kube-apiserver</code> 或者永久 watch ，所以在实现上借助了 <a href="https://github.com/kubernetes/client-go">client-go</a> 的 <code>informer</code> 包，相当于是实现了一个本地的二级缓存。这里不做过多展开。</p>
<p>它最终会调用 <code>PodGC</code> 的具体实现，位置在 <code>pkg/controller/podgc/gc_controller.go</code> 中：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func NewPodGC(kubeClient clientset.Interface, podInformer coreinformers.PodInformer, terminatedPodThreshold int) *PodGCController &#123;</span><br><span class="line">	if kubeClient !&#x3D; nil &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() !&#x3D; nil &#123;</span><br><span class="line">		metrics.RegisterMetricAndTrackRateLimiterUsage(&quot;gc_controller&quot;, kubeClient.CoreV1().RESTClient().GetRateLimiter())</span><br><span class="line">	&#125;</span><br><span class="line">	gcc :&#x3D; &amp;PodGCController&#123;</span><br><span class="line">		kubeClient:             kubeClient,</span><br><span class="line">		terminatedPodThreshold: terminatedPodThreshold,</span><br><span class="line">		deletePod: func(namespace, name string) error &#123;</span><br><span class="line">			glog.Infof(&quot;PodGC is force deleting Pod: %v:%v&quot;, namespace, name)</span><br><span class="line">			return kubeClient.CoreV1().Pods(namespace).Delete(name, metav1.NewDeleteOptions(0))</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	gcc.podLister &#x3D; podInformer.Lister()</span><br><span class="line">	gcc.podListerSynced &#x3D; podInformer.Informer().HasSynced</span><br><span class="line"></span><br><span class="line">	return gcc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>代码也比较直观，不过这里可以看到有一个注册 <code>metrics</code> 的过程，实际上 <code>kube-controller-manager</code> 在前面的 <code>10252</code> 端口上不仅暴露出来了一个 <code>/healthz</code> 接口，还暴露出了一个 <code>/metrics</code> 的接口，可用于进行监控之类的。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system get pod -l component&#x3D;kube-controller-manager</span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">kube-controller-manager-master   1&#x2F;1       Running   1          2m</span><br><span class="line">master $ kubectl -n kube-system exec -it kube-controller-manager-master sh</span><br><span class="line">&#x2F; # wget -qO- http:&#x2F;&#x2F;127.0.0.1:10252&#x2F;metrics|grep gc_controller</span><br><span class="line"># HELP gc_controller_rate_limiter_use A metric measuring the saturation of the rate limiter for gc_controller</span><br><span class="line"># TYPE gc_controller_rate_limiter_use gauge</span><br><span class="line">gc_controller_rate_limiter_use 0</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-12"><a href="#总结-12" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>kube-controller-manager</code> 以及它在 K8S 中主要是将集群调节至预期的状态，并提供出了 <code>/metrics</code> 的接口可供监控。</p>
<p><code>kube-controller-manager</code> 中有很多的 controller 大多数是默认开启的，当然也有默认关闭的，比如 <code>bootstrapsigner</code> 和 <code>tokencleaner</code>，在我们启动 <code>kube-controller-manager</code> 的时候，可通过 <code>--controllers</code> 的参数进行控制，就比如上面例子中 <code>--controllers=*,bootstrapsigner,tokencleaner</code> 表示开启所有默认开启的以及 <code>bootstrapsigner</code> 和 <code>tokencleaner</code> 。</p>
<p>下节，我们将学习另一个与资源调度有关的组件 <code>kube-scheduler</code>，了解下它对我们使用集群所带来的意义。</p>
<p>留言</p>
<ul>
<li><p><a href="https://juejin.im/user/5937807c2f301e006b268c6c" target="_blank" rel="noopener">楊歌</a></p>
<p>Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。</p>
</li>
</ul>
<h1 id="庖丁解牛：kube-scheduler"><a href="#庖丁解牛：kube-scheduler" class="headerlink" title="庖丁解牛：kube-scheduler"></a>庖丁解牛：kube-scheduler</h1><h2 id="整体概览-7"><a href="#整体概览-7" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>Scheduler</code> 的存在，知道了 Master 是 K8S 是集群的大脑，<code>Controller Manager</code> 负责将集群调整至预期的状态，而 <code>Scheduler</code> 则是集群调度器，将预期的 <code>Pod</code> 资源调度到正确的 <code>Node</code> 节点上，进而令该 <code>Pod</code> 可完成启动。本节我们一同来看看它如何发挥如此大的作用。</p>
<p><strong>下文统一使用 <code>kube-scheduler</code> 进行表述</strong></p>
<h2 id="kube-scheduler-是什么"><a href="#kube-scheduler-是什么" class="headerlink" title="kube-scheduler 是什么"></a><code>kube-scheduler</code> 是什么</h2><p>引用<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" rel="noopener">官方文档</a>一句话：</p>
<blockquote>
<p>The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity.</p>
</blockquote>
<p><code>kube-scheduler</code> 是一个策略丰富，拓扑感知的调度程序，会显著影响可用性，性能和容量。</p>
<p>我们知道资源调度本就是 K8S 这类系统中的一个很复杂的事情，既要能满足系统对资源利用率的需要，同样还需要避免资源竞争，比如说端口冲突之类的。</p>
<p>为了能完成这样的需求，<code>kube-scheduler</code> 便在不断的迭代和发展，通过支持多种策略满足各类需求，通过感知拓扑避免资源竞争和保障系统的可用性及容量等。</p>
<p>我们在第 5 节下载服务端二进制文件解压后，便可看到 <code>kube-scheduler</code> 的可执行文件。当给它传递 <code>--help</code> 查看其支持参数的时候，便可以看到它支持使用 <code>--address</code> 或者 <code>--bind-address</code> 等参数指定所启动的 HTTP server 所绑定的地址之类的。</p>
<p>它和 <code>kube-controller-manager</code> 有点类似，同样是通过定时的向 <code>kube-apiserver</code> 请求获取信息，并进行处理。而他们所起到的作用并不相同。</p>
<h2 id="kube-scheduler-有什么作用"><a href="#kube-scheduler-有什么作用" class="headerlink" title="kube-scheduler 有什么作用"></a><code>kube-scheduler</code> 有什么作用</h2><p>从上层的角度来看，<code>kube-scheduler</code> 的作用就是将待调度的 <code>Pod</code> 调度至最佳的 <code>Node</code> 上，而这个过程中则需要根据不同的策略，考虑到 <code>Node</code> 的资源使用情况，比如端口，内存，存储等。</p>
<h2 id="kube-scheduler-是如何工作的"><a href="#kube-scheduler-是如何工作的" class="headerlink" title="kube-scheduler 是如何工作的"></a><code>kube-scheduler</code> 是如何工作的</h2><p>整体的过程可通过 <code>pkg/scheduler/core/generic_scheduler.go</code> 的代码来看</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) &#123;</span><br><span class="line">	trace :&#x3D; utiltrace.New(fmt.Sprintf(&quot;Scheduling %s&#x2F;%s&quot;, pod.Namespace, pod.Name))</span><br><span class="line">	defer trace.LogIfLong(100 * time.Millisecond)</span><br><span class="line"></span><br><span class="line">	if err :&#x3D; podPassesBasicChecks(pod, g.pvcLister); err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	nodes, err :&#x3D; nodeLister.List()</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line">	if len(nodes) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">		return &quot;&quot;, ErrNoNodesAvailable</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	err &#x3D; g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	trace.Step(&quot;Computing predicates&quot;)</span><br><span class="line">	startPredicateEvalTime :&#x3D; time.Now()</span><br><span class="line">	filteredNodes, failedPredicateMap, err :&#x3D; g.findNodesThatFit(pod, nodes)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if len(filteredNodes) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">		return &quot;&quot;, &amp;FitError&#123;</span><br><span class="line">			Pod:              pod,</span><br><span class="line">			NumAllNodes:      len(nodes),</span><br><span class="line">			FailedPredicates: failedPredicateMap,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime))</span><br><span class="line">	metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime))</span><br><span class="line"></span><br><span class="line">	trace.Step(&quot;Prioritizing&quot;)</span><br><span class="line">	startPriorityEvalTime :&#x3D; time.Now()</span><br><span class="line">	&#x2F;&#x2F; When only one node after predicate, just use it.</span><br><span class="line">	if len(filteredNodes) &#x3D;&#x3D; 1 &#123;</span><br><span class="line">		metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))</span><br><span class="line">		return filteredNodes[0].Name, nil</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	metaPrioritiesInterface :&#x3D; g.priorityMetaProducer(pod, g.cachedNodeInfoMap)</span><br><span class="line">	priorityList, err :&#x3D; PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line">	metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))</span><br><span class="line">	metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime))</span><br><span class="line"></span><br><span class="line">	trace.Step(&quot;Selecting host&quot;)</span><br><span class="line">	return g.selectHost(priorityList)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>它的输入有两个：</p>
<ul>
<li><code>pod</code>：待调度的 <code>Pod</code> 对象；</li>
<li><code>nodeLister</code>：所有可用的 <code>Node</code> 列表</li>
</ul>
<p>备注：<code>nodeLister</code> 的实现稍微用了点技巧，返回的是 <code>[]*v1.Node</code> 而不是 <code>v1.NodeList</code> 可避免拷贝带来的性能损失。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">type NodeLister interface &#123;</span><br><span class="line">	List() ([]*v1.Node, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="处理阶段"><a href="#处理阶段" class="headerlink" title="处理阶段"></a>处理阶段</h3><p><code>kube-scheduler</code> 将处理阶段主要分为三个阶段 <code>Computing predicates</code>，<code>Prioritizing</code>和 <code>Selecting host</code>：</p>
<ul>
<li><p><code>Computing predicates</code>：主要解决的问题是 <code>Pod</code> 能否调度到集群的 <code>Node</code> 上；</p>
<p>主要是通过一个名为 <code>podFitsOnNode</code> 的函数进行实现，在检查的过程中也会先去检查下是否已经有已缓存的判断结果， 当然也会检查 <code>Pod</code> 是否是可调度的，以防有 <code>Pod Affinity</code> (亲合性) 之类的存在。</p>
</li>
<li><p><code>Prioritizing</code>：主要解决的问题是在上个阶段通过 <code>findNodesThatFit</code> 得到了 <code>filteredNodes</code> 的基础之上解决哪些 <code>Node</code> 是最优的，得到一个优先级列表 <code>priorityList</code>;</p>
<p>至于优先级的部分，主要是通过下面的代码：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">for i :&#x3D; range nodes &#123;</span><br><span class="line">	result &#x3D; append(result, schedulerapi.HostPriority&#123;Host: nodes[i].Name, Score: 0&#125;)</span><br><span class="line">	for j :&#x3D; range priorityConfigs &#123;</span><br><span class="line">		result[i].Score +&#x3D; results[j][i].Score * priorityConfigs[j].Weight</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>给每个经过第一步筛选出来的 <code>Node</code> 一个 <code>Score</code>，再按照各种条件进行打分，最终得到一个优先级列表。</p>
</li>
<li><p><code>Selecting host</code>：则是最终选择 <code>Node</code> 调度到哪台机器上。</p>
<p>最后，则是通过 <code>selectHost</code> 选择出最终要调度到哪台机器上。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) &#123;</span><br><span class="line">    if len(priorityList) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">        return &quot;&quot;, fmt.Errorf(&quot;empty priorityList&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sort.Sort(sort.Reverse(priorityList))</span><br><span class="line">    maxScore :&#x3D; priorityList[0].Score</span><br><span class="line">    firstAfterMaxScore :&#x3D; sort.Search(len(priorityList), func(i int) bool &#123; return priorityList[i].Score &lt; maxScore &#125;)</span><br><span class="line"></span><br><span class="line">    g.lastNodeIndexLock.Lock()</span><br><span class="line">    ix :&#x3D; int(g.lastNodeIndex % uint64(firstAfterMaxScore))</span><br><span class="line">    g.lastNodeIndex++</span><br><span class="line">    g.lastNodeIndexLock.Unlock()</span><br><span class="line"></span><br><span class="line">    return priorityList[ix].Host, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<h2 id="总结-13"><a href="#总结-13" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>kube-scheduler</code> 以及它在调度 <code>Pod</code> 的过程中的大致步骤。</p>
<p>不过它实际使用的各种策略及判断条件很多，无法在一节中完全都详细介绍，感兴趣的朋友可以按照本节中提供的思路大致去看看它的实现。</p>
<p>我们通过前面几节的介绍，已经知道了当实际进行部署操作的时候，首先会通过 <code>kubectl</code> 之类的客户端工具与 <code>kube-apiserver</code> 进行交互，在经过一系列的处理后，数据将持久化到 <code>etcd</code> 中；</p>
<p>此时，<code>kube-controller-manager</code> 通过持续的观察，开始按照我们的配置，将集群的状态调整至预期状态；</p>
<p>而 <code>kube-scheduler</code> 也在发挥作用，决定 <code>Pod</code> 应该调度至哪个或者哪些 <code>Node</code> 上；之后则通过其他组件的协作，最总将该 <code>Pod</code> 在相应的 <code>Node</code> 上部署启动。</p>
<p>我们在下节将要介绍的 <code>kubelet</code> 便是后面这部分“实际部署动作”相关的组件中尤为重要的一个，下节我们再详细介绍它是如何完成这些功能的。</p>
<h1 id="庖丁解牛：kubelet"><a href="#庖丁解牛：kubelet" class="headerlink" title="庖丁解牛：kubelet"></a>庖丁解牛：kubelet</h1><h2 id="整体概览-8"><a href="#整体概览-8" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">+--------------------------------------------------------+       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| |      kubelet        |        |     kube-proxy      | |       </span><br><span class="line">| |                     |        |                     | |       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">| | Container Runtime (Docker)                         | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| | |Pod                  |    |Pod                  | | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | | |C1   | |C2   |     |    ||C1   ||C2   ||C3   || | |       </span><br><span class="line">| | | |     | |     |     |    ||     ||     ||     || | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">+--------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们知道了 K8S 中 Node 由一些必要的组件构成，而其中最为核心的当属 <code>kubelet</code> 了，如果没有 <code>kubelet</code> 的存在，那我们预期的各类资源就只能存在于 <code>Master</code> 的相关组件中了，而 K8S 也很能只是一个 CRUD 的普通程序了。本节，我们来介绍下 <code>kubelet</code> 及它是如何完成这一系列任务的。</p>
<h2 id="kubelet-是什么"><a href="#kubelet-是什么" class="headerlink" title="kubelet 是什么"></a><code>kubelet</code> 是什么</h2><p>按照一般架构设计上的习惯，<code>kubelet</code> 所承担的角色一般会被叫做 <code>agent</code>，这里叫做 <code>kubelet</code> 很大程度上受 <code>Borg</code> 的命名影响，<code>Borg</code> 里面也有一个 <code>Borglet</code> 的组件存在。<code>kubelet</code> 便是 K8S 中的 <code>agent</code>，负责 <code>Node</code> 和 <code>Pod</code> 相关的管理任务。</p>
<p>同样的，在我们下载 K8S 二进制文件解压后，便可以得到 <code>kubelet</code> 的可执行文件。在第 5 节中，我们也完成了 <code>kubelet</code> 以 <code>systemd</code> 进行启动和管理的相关配置。</p>
<h2 id="kubelet-有什么作用"><a href="#kubelet-有什么作用" class="headerlink" title="kubelet 有什么作用"></a><code>kubelet</code> 有什么作用</h2><p>通常来讲 <code>agent</code> 这样的角色起到的作用首先便是要能够注册，让 <code>server</code> 端知道它的存在，所以这便是它的第一个作用：节点管理。</p>
<h3 id="节点管理"><a href="#节点管理" class="headerlink" title="节点管理"></a>节点管理</h3><p>当我们执行 <code>kubelet --help</code> 的时候，会看到它所支持的可配置参数，其中有一个 <code>--register-node</code> 参数便是用于控制是否向 <code>kube-apiserver</code> 注册节点的，默认是开启的。</p>
<p>我们在第 5 节中还介绍了如何新增一个 <code>Node</code>，当 <code>kubeadm join</code> 执行成功后，你便可以通过 <code>kubectl get node</code> 查看到新加入集群中的 <code>Node</code>，与此同时，你也可以在该节点上通过以下命令查看 <code>kubelet</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ systemctl status kubelet</span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Agent</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d</span><br><span class="line">           └─kubeadm.conf</span><br><span class="line">   Active: active (running) since Thu 2018-12-13 07:49:51 UTC; 32min ago</span><br><span class="line">     Docs: http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;</span><br><span class="line"> Main PID: 3876259 (kubelet)</span><br><span class="line">   Memory: 66.3M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;kubelet.service</span><br><span class="line">           └─3876259 &#x2F;usr&#x2F;bin&#x2F;kubelet --bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernete...</span><br></pre></td></tr></table></figure></div>

<p>当我们查看 <code>Node</code> 信息时，也能得到如下输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl get nodes node01 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubeadm.alpha.kubernetes.io&#x2F;cri-socket: &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">    node.alpha.kubernetes.io&#x2F;ttl: &quot;0&quot;</span><br><span class="line">    volumes.kubernetes.io&#x2F;controller-managed-attach-detach: &quot;true&quot;</span><br><span class="line">  creationTimestamp: 2018-12-13T07:50:47Z</span><br><span class="line">  labels:</span><br><span class="line">    beta.kubernetes.io&#x2F;arch: amd64</span><br><span class="line">    beta.kubernetes.io&#x2F;os: linux</span><br><span class="line">    kubernetes.io&#x2F;hostname: node01</span><br><span class="line">  name: node01</span><br><span class="line">  resourceVersion: &quot;4242&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;node01</span><br><span class="line">  uid: cd612df6-feab-11e8-9a0b-0242ac110096</span><br><span class="line">spec: &#123;&#125;</span><br><span class="line">status:</span><br><span class="line">  addresses:</span><br><span class="line">  - address: 172.17.0.152</span><br><span class="line">    type: InternalIP</span><br><span class="line">  - address: node01</span><br><span class="line">    type: Hostname</span><br><span class="line">  allocatable:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: &quot;89032026784&quot;</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3894788Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  capacity:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: 96605932Ki</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3997188Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  conditions:</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has sufficient disk space available</span><br><span class="line">    reason: KubeletHasSufficientDisk</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: OutOfDisk</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has sufficient memory available</span><br><span class="line">    reason: KubeletHasSufficientMemory</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: MemoryPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has no disk pressure</span><br><span class="line">    reason: KubeletHasNoDiskPressure</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: DiskPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has sufficient PID available</span><br><span class="line">    reason: KubeletHasSufficientPID</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: PIDPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:51:37Z</span><br><span class="line">    message: kubelet is posting ready status. AppArmor enabled</span><br><span class="line">    reason: KubeletReady</span><br><span class="line">    status: &quot;True&quot;</span><br><span class="line">    type: Ready</span><br><span class="line">  daemonEndpoints:</span><br><span class="line">    kubeletEndpoint:</span><br><span class="line">      Port: 10250</span><br><span class="line">  images:</span><br><span class="line">  - names:</span><br><span class="line">    - k8s.gcr.io&#x2F;kube-apiserver-amd64@sha256:956bea8c139620c9fc823fb81ff9b5647582b53bd33904302987d56ab24fc187</span><br><span class="line">    - k8s.gcr.io&#x2F;kube-apiserver-amd64:v1.11.3</span><br><span class="line">    sizeBytes: 186676561</span><br><span class="line">  nodeInfo:</span><br><span class="line">    architecture: amd64</span><br><span class="line">    bootID: 89ced22c-f7f8-4c4d-ad0d-d10887ab900e</span><br><span class="line">    containerRuntimeVersion: docker:&#x2F;&#x2F;18.6.0</span><br><span class="line">    kernelVersion: 4.4.0-62-generic</span><br><span class="line">    kubeProxyVersion: v1.11.3</span><br><span class="line">    kubeletVersion: v1.11.3</span><br><span class="line">    machineID: 26ba042302eea8095d6576975c120eeb</span><br><span class="line">    operatingSystem: linux</span><br><span class="line">    osImage: Ubuntu 16.04.2 LTS</span><br><span class="line">    systemUUID: 26ba042302eea8095d6576975c120eeb</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>kubelet</code> 不仅将自己注册给了 <code>kube-apiserver</code>，同时它所在机器的信息也都进行了上报，包括 CPU，内存，IP 信息等。</p>
<p>这其中有我们在第 2 节中提到的关于 <code>Node</code> 状态相关的一些信息，可以对照着看看。</p>
<p>当然这里除了这些信息外，还有些值得注意的，比如 <code>daemonEndpoints</code> 之类的，可以看到目前 <code>kubelet</code> 监听在了 <code>10250</code> 端口，这个端口可通过 <code>--port</code> 配置，但是之后会被废弃掉，我们是写入了 <code>/var/lib/kubelet/config.yaml</code> 的配置文件中。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ cat &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml</span><br><span class="line">address: 0.0.0.0</span><br><span class="line">apiVersion: kubelet.config.k8s.io&#x2F;v1beta1</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: false</span><br><span class="line">  webhook:</span><br><span class="line">    cacheTTL: 2m0s</span><br><span class="line">    enabled: true</span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt</span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">  webhook:</span><br><span class="line">    cacheAuthorizedTTL: 5m0s</span><br><span class="line">    cacheUnauthorizedTTL: 30s</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">cgroupsPerQOS: true</span><br><span class="line">clusterDNS:</span><br><span class="line">- 10.96.0.10</span><br><span class="line">clusterDomain: cluster.local</span><br><span class="line">containerLogMaxFiles: 5</span><br><span class="line">containerLogMaxSize: 10Mi</span><br><span class="line">contentType: application&#x2F;vnd.kubernetes.protobuf</span><br><span class="line">cpuCFSQuota: true</span><br><span class="line">cpuManagerPolicy: none</span><br><span class="line">cpuManagerReconcilePeriod: 10s</span><br><span class="line">enableControllerAttachDetach: true</span><br><span class="line">enableDebuggingHandlers: true</span><br><span class="line">enforceNodeAllocatable:</span><br><span class="line">- pods</span><br><span class="line">eventBurst: 10</span><br><span class="line">eventRecordQPS: 5</span><br><span class="line">evictionHard:</span><br><span class="line">  imagefs.available: 15%</span><br><span class="line">  memory.available: 100Mi</span><br><span class="line">  nodefs.available: 10%</span><br><span class="line">  nodefs.inodesFree: 5%</span><br><span class="line">evictionPressureTransitionPeriod: 5m0s</span><br><span class="line">failSwapOn: true</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">healthzBindAddress: 127.0.0.1</span><br><span class="line">healthzPort: 10248</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">imageMinimumGCAge: 2m0s</span><br><span class="line">iptablesDropBit: 15</span><br><span class="line">iptablesMasqueradeBit: 14</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">kubeAPIBurst: 10</span><br><span class="line">kubeAPIQPS: 5</span><br><span class="line">makeIPTablesUtilChains: true</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">maxPods: 110</span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">oomScoreAdj: -999</span><br><span class="line">podPidsLimit: -1</span><br><span class="line">port: 10250</span><br><span class="line">registryBurst: 10</span><br><span class="line">registryPullQPS: 5</span><br><span class="line">resolvConf: &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">rotateCertificates: true</span><br><span class="line">runtimeRequestTimeout: 2m0s</span><br><span class="line">serializeImagePulls: true</span><br><span class="line">staticPodPath: &#x2F;etc&#x2F;kubernetes&#x2F;manifests</span><br><span class="line">streamingConnectionIdleTimeout: 4h0m0s</span><br><span class="line">syncFrequency: 1m0s</span><br><span class="line">volumeStatsAggPeriod: 1m0s</span><br><span class="line">master $</span><br></pre></td></tr></table></figure></div>

<p>这其中有一些需要关注的配置：</p>
<ul>
<li><p><code>maxPods</code>：最大的 <code>Pod</code> 数</p>
</li>
<li><p><code>healthzBindAddress</code> 和 <code>healthzPort</code>：配置了健康检查所监听的地址和端口</p>
<p>我们可用以下方式进行验证：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ curl 127.0.0.1:10248&#x2F;healthz</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></div>
</li>
<li><p><code>authentication</code> 和 <code>authorization</code> ：认证授权相关</p>
</li>
<li><p><code>evictionHard</code>：涉及到 <code>kubelet</code> 的驱逐策略，对 <code>Pod</code> 调度分配之类的影响很大</p>
</li>
</ul>
<p>其余部分，可参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">手册内容</a></p>
<h3 id="Pod-管理"><a href="#Pod-管理" class="headerlink" title="Pod 管理"></a>Pod 管理</h3><p>从上面的配置以及我们之前的介绍中，<code>kube-scheduler</code> 处理了 <code>Pod</code> 应该调度至哪个 <code>Node</code>，而 <code>kubelet</code> 则是保障该 <code>Pod</code> 能按照预期，在对应 <code>Node</code> 上启动并保持工作。</p>
<p>同时，<code>kubelet</code> 在保障 <code>Pod</code> 能按预期工作，主要是做了两方面的事情：</p>
<ul>
<li>健康检查：通过 <code>LivenessProbe</code> 和 <code>ReadinessProbe</code> 探针进行检查，判断是否健康及是否已经准备好接受请求。</li>
<li>资源监控：通过 <code>cAdvisor</code> 进行资源监。</li>
</ul>
<h2 id="kubelet-是如何工作的"><a href="#kubelet-是如何工作的" class="headerlink" title="kubelet 是如何工作的"></a><code>kubelet</code> 是如何工作的</h2><p>大致的功能已经介绍了，我们来看下它大体的实现。</p>
<p>首先是在 <code>cmd/kubelet/app/server.go</code> 文件中的 <code>Run</code> 方法：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) error &#123;</span><br><span class="line">	glog.Infof(&quot;Version: %+v&quot;, version.Get())</span><br><span class="line">	if err :&#x3D; initForOS(s.KubeletFlags.WindowsService); err !&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;failed OS init: %v&quot;, err)</span><br><span class="line">	&#125;</span><br><span class="line">	if err :&#x3D; run(s, kubeDeps, stopCh); err !&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;failed to run Kubelet: %v&quot;, err)</span><br><span class="line">	&#125;</span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>这个方法看起来很简单那，它是在读取完一系列的配置和校验之后开始被调用的，在调用过程中，会在日志中输出当前的版本号，如果你的 <code>kubelet</code> 已经正常运行，当你执行 <code>journalctl -u kubelet</code> 的时候，便会看到一条相关的日志输出。</p>
<p>之后，便是一个 <code>run</code> 方法，其中包含着各种环境检查，容器管理，<code>cAdvisor</code> 初始化之类的操作，直到 <code>kubelet</code> 基本正确运行后，则会调用 <code>pkg/kubelet/kubelet.go</code> 中的一个 <code>BirthCry</code> 方法，该方法从命名就可以看出来，它其实就是宣告 <code>kubelet</code> 已经启动：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">func (kl *Kubelet) BirthCry() &#123;</span><br><span class="line">	kl.recorder.Eventf(kl.nodeRef, v1.EventTypeNormal, events.StartingKubelet, &quot;Starting kubelet.&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>后续则是关于注册，<code>Pod</code> 管理以及资源相关的处理逻辑，内容较多，这里就不再展开了。</p>
<h2 id="总结-14"><a href="#总结-14" class="headerlink" title="总结"></a>总结</h2><p>本节中我们介绍了 <code>kubelet</code> 的主要功能和基本实现，了解到了它不仅可将自身注册到集群，同时还承担着保障 <code>Pod</code> 可在该 <code>Node</code> 上按预期工作。另外 <code>kubelet</code> 其实还承担着清理 <code>Node</code> 上一些由 K8S 调度 <code>Pod</code> 所造成的磁盘占用之类的工作。</p>
<p>从上面的配置中基本能看出来一些，这部分的功能大多数情况下不需要大家人为干预，所以也就不再展开了。</p>
<p>当 <code>Pod</code> 在 <code>Node</code> 上正常运行之后，若是需要对外提供服务，则需要将其暴露出来。下节，我们来介绍下 <code>kube-proxy</code> 是如何来处理这些工作的。</p>
<h1 id="庖丁解牛：kube-proxy"><a href="#庖丁解牛：kube-proxy" class="headerlink" title="庖丁解牛：kube-proxy"></a>庖丁解牛：kube-proxy</h1><h2 id="整体概览-9"><a href="#整体概览-9" class="headerlink" title="整体概览"></a>整体概览</h2><p>在第 3 节中，我们了解到 <code>kube-proxy</code> 的存在，而在第 7 中，我们学习到了如何将运行于 K8S 中的服务以 <code>Service</code> 的方式暴露出来，以供访问。</p>
<p>本节，我们来介绍下 <code>kube-proxy</code> 了解下它是如何支撑起这种类似服务发现和代理相关功能的。</p>
<h2 id="kube-proxy-是什么"><a href="#kube-proxy-是什么" class="headerlink" title="kube-proxy 是什么"></a><code>kube-proxy</code> 是什么</h2><p><code>kube-proxy</code> 是 K8S 运行于每个 <code>Node</code> 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。</p>
<p>我们已经知道，当 <code>Pod</code> 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的服务的异常，所以通常情况下，我们都会使用 <code>Service</code> 将后端 <code>Pod</code> 暴露出来，而 <code>Service</code> 则较为稳定。</p>
<p>还是以我们之前的 <a href="https://github.com/tao12345666333/saythx"><code>SayThx</code></a> 项目为例，但我们只部署其中没有任何依赖的后端资源 <code>Redis</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ git clone https:&#x2F;&#x2F;github.com&#x2F;tao12345666333&#x2F;saythx.git</span><br><span class="line">Cloning into &#39;saythx&#39;...</span><br><span class="line">remote: Enumerating objects: 110, done.</span><br><span class="line">remote: Counting objects: 100% (110&#x2F;110), done.</span><br><span class="line">remote: Compressing objects: 100% (82&#x2F;82), done.</span><br><span class="line">remote: Total 110 (delta 27), reused 102 (delta 20), pack-reused 0</span><br><span class="line">Receiving objects: 100% (110&#x2F;110), 119.42 KiB | 0 bytes&#x2F;s, done.</span><br><span class="line">Resolving deltas: 100% (27&#x2F;27), done.</span><br><span class="line">Checking connectivity... done.</span><br><span class="line">master $ cd saythx&#x2F;deploy</span><br><span class="line">master $ ls</span><br><span class="line">backend-deployment.yaml  frontend-deployment.yaml  namespace.yaml         redis-service.yaml</span><br><span class="line">backend-service.yaml     frontend-service.yaml     redis-deployment.yaml  work-deployment.yaml</span><br></pre></td></tr></table></figure></div>

<p>进入配置文件所在目录后，开始创建相关资源：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl  apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl -n work get all</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-wsn2w   1&#x2F;1       Running   0          21s</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269&#x2F;TCP   6s</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            1           21s</span><br><span class="line"></span><br><span class="line">NAME                                     DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d   1         1         1         21s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 Redis 正在运行，并通过 <code>NodePort</code> 类型的 <code>Service</code> 暴露出来，我们访问来确认下。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ docker run --rm -it --network host redis:alpine redis-cli -p 31269</span><br><span class="line">Unable to find image &#39;redis:alpine&#39; locally</span><br><span class="line">alpine: Pulling from library&#x2F;redis</span><br><span class="line">4fe2ade4980c: Already exists</span><br><span class="line">fb758dc2e038: Pull complete</span><br><span class="line">989f7b0c858b: Pull complete</span><br><span class="line">8dd99d530347: Pull complete</span><br><span class="line">7137334fa8f0: Pull complete</span><br><span class="line">30610ca64487: Pull complete</span><br><span class="line">Digest: sha256:8fd83c5986f444f1a5521e3eda7395f0f21ff16d33cc3b89d19ca7c58293c5dd</span><br><span class="line">Status: Downloaded newer image for redis:alpine</span><br><span class="line">127.0.0.1:31269&gt; set name kubernetes</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:31269&gt; get name </span><br><span class="line">&quot;kubernetes&quot;</span><br></pre></td></tr></table></figure></div>

<p>可以看到已经可以正常访问。接下来，我们来看下 <code>31269</code> 这个端口的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ netstat  -ntlp |grep 31269</span><br><span class="line">tcp6       0      0 :::31269                :::*                    LISTEN      2716&#x2F;kube-proxy</span><br></pre></td></tr></table></figure></div>

<p>可以看到该端口是由 <code>kube-proxy</code> 所占用的。</p>
<p>接下来，查看当前集群的 <code>Service</code> 和 <code>Endpoint</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work get svc</span><br><span class="line">NAME           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269&#x2F;TCP   10m</span><br><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS        AGE</span><br><span class="line">saythx-redis   10.32.0.2:6379   10m</span><br><span class="line">master $ kubectl -n work get pod -o wide</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE</span><br><span class="line">saythx-redis-8558c7d7d-wsn2w   1&#x2F;1       Running   0          12m       10.32.0.2   node01    &lt;none&gt;</span><br></pre></td></tr></table></figure></div>

<p>可以很直观的看到 <code>Endpoint</code> 当中的便是 <code>Pod</code> 的 IP，现在我们将该服务进行扩容（实际情况下并不会这样处理）。</p>
<p>直接通过 <code>kubectl scale</code> 操作</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl  -n work scale --replicas&#x3D;2 deploy&#x2F;saythx-redis</span><br><span class="line">deployment.extensions&#x2F;saythx-redis scaled</span><br><span class="line">master $ kubectl  -n work get all</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-sslpj   1&#x2F;1       Running   0          10s</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-wsn2w   1&#x2F;1       Running   0          16m</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269&#x2F;TCP   16m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   2         2         2            2           16m</span><br></pre></td></tr></table></figure></div>

<p>查看 <code>Endpoint</code> 信息：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS                       AGE</span><br><span class="line">saythx-redis   10.32.0.2:6379,10.32.0.3:6379   17m</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Endpoint</code> 已经自动发生了变化，而这也意味着 <code>Service</code> 代理的后端节点将增加一个。</p>
<h2 id="kube-proxy-如何工作"><a href="#kube-proxy-如何工作" class="headerlink" title="kube-proxy 如何工作"></a><code>kube-proxy</code> 如何工作</h2><p><code>kube-proxy</code> 在 Linux 系统上当前支持三种模式，可通过 <code>--proxy-mode</code> 配置：</p>
<ul>
<li><code>userspace</code>：这是很早期的一种方案，但效率上显著不足，不推荐使用。</li>
<li><code>iptables</code>：当前的默认模式。比 <code>userspace</code> 要快，但问题是会给机器上产生很多 <code>iptables</code> 规则。</li>
<li><code>ipvs</code>：为了解决 <code>iptables</code> 的性能问题而引入，采用增量的方式进行更新。</li>
</ul>
<p>下面我们以 <code>iptables</code> 的模式稍作介绍。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ iptables -t nat -L </span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SERVICES  all  --  anywhere             anywhere             &#x2F;* kubernetes service portals *&#x2F;</span><br><span class="line">DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SERVICES  all  --  anywhere             anywhere             &#x2F;* kubernetes service portals *&#x2F;</span><br><span class="line">DOCKER     all  --  anywhere            !127.0.0.0&#x2F;8          ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain POSTROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-POSTROUTING  all  --  anywhere             anywhere             &#x2F;* kubernetes postrouting rules *&#x2F;</span><br><span class="line">MASQUERADE  all  --  172.18.0.0&#x2F;24        anywhere</span><br><span class="line"></span><br><span class="line">Chain DOCKER (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">RETURN     all  --  anywhere             anywhere</span><br><span class="line"></span><br><span class="line">Chain KUBE-MARK-DROP (0 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">MARK       all  --  anywhere             anywhere             MARK or 0x8000</span><br><span class="line"></span><br><span class="line">Chain KUBE-MARK-MASQ (7 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">MARK       all  --  anywhere             anywhere             MARK or 0x4000</span><br><span class="line"></span><br><span class="line">Chain KUBE-NODEPORTS (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  tcp  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F; tcp dpt:31269</span><br><span class="line">KUBE-SVC-SMQNAAUIAENDDGYQ  tcp  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F; tcp dpt:31269</span><br><span class="line"></span><br><span class="line">Chain KUBE-POSTROUTING (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">MASQUERADE  all  --  anywhere             anywhere             &#x2F;* kubernetes service traffic requiring SNAT *&#x2F; mark match 0x4000&#x2F;0x4000</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-2LZPYBS4HUAJKDFL (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F; tcp to:10.32.0.2:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-3E4LNQKKWZF7G6SH (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.1            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F; tcp to:10.32.0.1:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-3IDG7DUGN3QC2UZF (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  172.17.0.120         anywhere             &#x2F;* default&#x2F;kubernetes:https *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* default&#x2F;kubernetes:https *&#x2F; tcp to:172.17.0.120:6443</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-JZWS2VPNIEMNMNB2 (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F;</span><br><span class="line">DNAT       udp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F; udp to:10.32.0.2:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-OEY6JJQSBCQPRKHS (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.1            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F;</span><br><span class="line">DNAT       udp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F; udp to:10.32.0.1:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-QX7VDAS5KDY6V3EV (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F; tcp to:10.32.0.2:6379</span><br><span class="line"></span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SVC-SMQNAAUIAENDDGYQ  tcp  --  anywhere             10.103.193.175       &#x2F;* work&#x2F;saythx-redis: cluster IP *&#x2F; tcp dpt:6379</span><br><span class="line">KUBE-NODEPORTS  all  --  anywhere             anywhere             &#x2F;* kubernetes service nodeports; NOTE: this must be the last rule in this chain *&#x2F; ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain KUBE-SVC-ERIFXISQEP7F7OF4 (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SEP-3E4LNQKKWZF7G6SH  all  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F; statistic mode random probability 0.50000000000</span><br><span class="line">KUBE-SEP-2LZPYBS4HUAJKDFL  all  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Chain KUBE-SVC-SMQNAAUIAENDDGYQ (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SEP-QX7VDAS5KDY6V3EV  all  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F;</span><br></pre></td></tr></table></figure></div>

<p>以上输出已经尽量删掉了无关的内容。</p>
<p>当开始访问的时候先要经过 <code>PREROUTING</code> 链，转到 <code>KUBE-SERVICES</code> 链，当查询到匹配的规则之后，请求将转向 <code>KUBE-SVC-SMQNAAUIAENDDGYQ</code> 链，进而到达 <code>KUBE-SEP-QX7VDAS5KDY6V3EV</code> 对应于我们的 <code>Pod</code>。(注：为了简洁，上述 iptables 规则是部署一个 <code>Pod</code> 时的场景)</p>
<p>当搞懂了这些之后，如果你想了解这些 <code>iptables</code> 规则实际又是如何创建和维护的，那可以参考下 <code>proxier</code> 的具体实现，这里不再展开。</p>
<h2 id="总结-15"><a href="#总结-15" class="headerlink" title="总结"></a>总结</h2><p>本节中我们介绍了 <code>kube-proxy</code> 的主要功能和基本流程，了解到了它对于服务注册发现和代理访问等起到了很大的作用。而它在 Linux 下的代理模式也有 <code>userspace</code>，<code>iptables</code> 和 <code>ipvs</code> 等。</p>
<p>默认情况下我们使用 <code>iptables</code> 的代理模式，当创建新的 <code>Service</code> ，或者 <code>Pod</code> 进行变化时，<code>kube-proxy</code> 便会去维护 <code>iptables</code> 规则，以确保请求可以正确的到达后端服务。</p>
<p>当然，本节中并没有提到 <code>kube-proxy</code> 的 <code>session affinity</code> 相关的特性，如有需要可进行下尝试。</p>
<p>下节，我们将介绍实际运行着容器的 <code>Docker</code>，大致了解下在 K8S 中它所起的作用，及他们之间的交互方式。</p>
<h1 id="庖丁解牛：Container-Runtime-（Docker）"><a href="#庖丁解牛：Container-Runtime-（Docker）" class="headerlink" title="庖丁解牛：Container Runtime （Docker）"></a>庖丁解牛：Container Runtime （Docker）</h1><h2 id="整体概览-10"><a href="#整体概览-10" class="headerlink" title="整体概览"></a>整体概览</h2><p>我们在第 3 节的时候，提到过 <code>Container Runtime</code> 的概念，也大致介绍过它的主要作用在于下载镜像，运行容器等。</p>
<p>经过我们前面的学习，<code>kube-scheduler</code> 决定了 <code>Pod</code> 将被调度到哪个 <code>Node</code> 上，而 <code>kubelet</code> 则负责 <code>Pod</code> 在此 <code>Node</code> 上可按预期工作。如果没有 <code>Container Runtime</code>，那 <code>Pod</code> 中的 <code>container</code> 在该 <code>Node</code> 上也便无法正常启动运行了。</p>
<p>本节中，我们以当前最为通用的 <code>Container Runtime</code> Docker 为例进行介绍。</p>
<h2 id="Container-Runtime-是什么"><a href="#Container-Runtime-是什么" class="headerlink" title="Container Runtime 是什么"></a>Container Runtime 是什么</h2><p><code>Container Runtime</code> 我们通常叫它容器运行时，而这一概念的产生也是由于容器化技术和 K8S 的大力发展，为了统一工业标准，也为了避免 K8S 绑定于特定的容器运行时，所以便成立了 <a href="https://www.opencontainers.org/" target="_blank" rel="noopener">Open Container Initiative (OCI)</a> 组织，致力于将容器运行时标准化和容器镜像标准化。</p>
<p>凡是遵守此标准的实现，均可由标准格式的镜像启动相应的容器，并完成一些特定的操作。</p>
<h2 id="Docker-是什么"><a href="#Docker-是什么" class="headerlink" title="Docker 是什么"></a>Docker 是什么</h2><p>Docker 是一个容器管理平台，它最初是被设计用于快速创建，发布和运行容器的工具，不过随着它的发展，其中集成了越来越多的功能。</p>
<p>Docker 也可以说是一个包含标准容器运行时的工具集，当前版本中默认的 <code>runtime</code> 称之为 <code>runc</code>。 关于 <code>runc</code> 相关的一些内容可参考<a href="http://moelove.info/2018/11/23/runc-1.0-rc6-发布之际/" target="_blank" rel="noopener">我之前的一篇文章</a>。</p>
<p>当然，这里提到了 <strong>默认的运行时</strong> 那也就意味着它可支持其他的运行时实现。</p>
<h2 id="CRI-是什么"><a href="#CRI-是什么" class="headerlink" title="CRI 是什么"></a>CRI 是什么</h2><p>说到这里，我们就会发现，K8S 作为目前云原生技术体系中最重要的一环，为了让它更有扩展性，当然也不会将自己完全局限于某一种特定的容器运行时。</p>
<p>自 K8S 1.5 （2016 年 11 月）开始，新增了一个容器运行时的插件 API，并称之为 <code>CRI</code> （Container Runtime Interface），通过 <code>CRI</code> 可以支持 <code>kubelet</code> 使用不同的容器运行时，而不需要重新编译。</p>
<p><code>CRI</code> 主要是基于 gRPC 实现了 <code>RuntimeService</code> 和 <code>ImageService</code> 这两个服务，可以参考 <code>pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto</code> 中的 API 定义。由于本节侧重于 <code>Container Runtime/Docker</code> 这里就不对 <code>CRI</code> 的具体实现进行展开了。</p>
<p>只要继续将 <code>kubelet</code> 当作 agent 的角色，而它与基于 <code>CRI</code> 实现的 <code>CRI shim</code> 服务进行通信理解即可。</p>
<h2 id="Docker-如何工作"><a href="#Docker-如何工作" class="headerlink" title="Docker 如何工作"></a>Docker 如何工作</h2><p>这里我们主要介绍在 K8S 中一些 Docker 常见的动作。</p>
<h3 id="部署一个-Redis"><a href="#部署一个-Redis" class="headerlink" title="部署一个 Redis"></a>部署一个 Redis</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl run redis --image&#x3D;redis</span><br><span class="line">deployment.apps&#x2F;redis created</span><br><span class="line">master $ kubectl get all</span><br><span class="line">NAME                        READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-bb7894d65-7vsj8   0&#x2F;1       ContainerCreating   0          6s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   26m</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            0           6s</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-bb7894d65   1         1         0         6s</span><br></pre></td></tr></table></figure></div>

<p>我们直接使用 <code>kubectl run</code> 的方式部署了一个 Redis</p>
<h3 id="查看详情"><a href="#查看详情" class="headerlink" title="查看详情"></a>查看详情</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl describe pod&#x2F;redis-bb7894d65-7vsj8</span><br><span class="line">Name:               redis-bb7894d65-7vsj8</span><br><span class="line">Namespace:          default</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               node01&#x2F;172.17.0.21</span><br><span class="line">Start Time:         Sat, 15 Dec 2018 04:48:49 +0000</span><br><span class="line">Labels:             pod-template-hash&#x3D;663450821</span><br><span class="line">                    run&#x3D;redis</span><br><span class="line">Annotations:        &lt;none&gt;</span><br><span class="line">Status:             Running</span><br><span class="line">IP:                 10.40.0.1</span><br><span class="line">Controlled By:      ReplicaSet&#x2F;redis-bb7894d65</span><br><span class="line">Containers:</span><br><span class="line">  redis:</span><br><span class="line">    Container ID:   docker:&#x2F;&#x2F;ab87085456aca76825dd639bcde27160d9c2c84cac5388585bcc9ed3afda6522</span><br><span class="line">    Image:          redis</span><br><span class="line">    Image ID:       docker-pullable:&#x2F;&#x2F;redis@sha256:010a8bd5c6a9d469441aa35187d18c181e3195368bce309348b3ee639fce96e0</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sat, 15 Dec 2018 04:48:57 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount from default-token-zxt27 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             True</span><br><span class="line">  ContainersReady   True</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-zxt27:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-zxt27</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io&#x2F;not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io&#x2F;unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From               Message</span><br><span class="line">  ----    ------     ----  ----               -------</span><br><span class="line">  Normal  Scheduled  7m    default-scheduler  Successfully assigned default&#x2F;redis-bb7894d65-7vsj8to node01</span><br><span class="line">  Normal  Pulling    7m    kubelet, node01    pulling image &quot;redis&quot;</span><br><span class="line">  Normal  Pulled     7m    kubelet, node01    Successfully pulled image &quot;redis&quot;</span><br><span class="line">  Normal  Created    7m    kubelet, node01    Created container</span><br><span class="line">  Normal  Started    7m    kubelet, node01    Started container</span><br></pre></td></tr></table></figure></div>

<p>可以通过 <code>kubectl describe</code> 查看该 <code>Pod</code> 的事件详情。这里主要有几个阶段。</p>
<h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">Normal  Scheduled  7m    default-scheduler  Successfully assigned default&#x2F;redis-bb7894d65-7vsj8to node01</span><br></pre></td></tr></table></figure></div>

<p>在第 15 小节 <code>kube-scheduler</code> 中我们介绍过，通过 <code>kube-scheduler</code> 可以决定 <code>Pod</code> 会调度到哪个 <code>Node</code>。本例中，<code>redis-bb7894d65-7vsj8to</code> 被调度到了 <code>node01</code>。</p>
<h4 id="pull-镜像"><a href="#pull-镜像" class="headerlink" title="pull 镜像"></a>pull 镜像</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">Normal  Pulling    7m    kubelet, node01    pulling image &quot;redis&quot;</span><br><span class="line">Normal  Pulled     7m    kubelet, node01    Successfully pulled image &quot;redis&quot;</span><br></pre></td></tr></table></figure></div>

<p>这里 <code>kubelet</code> 及该节点上的 <code>Container Runtime</code> （Docker）开始发挥作用，先拉取镜像。如果此刻你登录 <code>node01</code> 的机器，执行 <code>docker pull redis</code> 便可同步看到拉取进度。</p>
<h4 id="创建镜像并启动"><a href="#创建镜像并启动" class="headerlink" title="创建镜像并启动"></a>创建镜像并启动</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">Normal  Created    7m    kubelet, node01    Created container</span><br><span class="line">Normal  Started    7m    kubelet, node01    Started container</span><br></pre></td></tr></table></figure></div>

<p>拉取镜像完成后，便会开始创建并启动该容器，并返回任务结果。此刻登录 <code>node01</code> 机器，便会看到当前在运行的容器了。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">node01 $ docker ps |grep redis</span><br><span class="line">ab87085456ac        redis@sha256:010a8bd5c6a9d469441aa35187d18c181e3195368bce309348b3ee639fce96e0  &quot;docker-entrypoint...&quot;   19 minutes ago      Up 19 minutes                           k8s_redis_redis-bb7894d65-7vsj8_default_b693b56c-0024-11e9-9bab-0242ac11000a_0</span><br><span class="line">8f264abd82fe        k8s.gcr.io&#x2F;pause:3.1  &quot;&#x2F;pause&quot;                 19 minutes ago      Up 19 minutes                           k8s_POD_redis-bb7894d65-7vsj8_default_b693b56c-0024-11e9-9bab-0242ac11000a_0</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-16"><a href="#总结-16" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了 <code>Container Runtime</code> 的基本概念，及 K8S 为了能增加扩展性，提供了统一的 <code>CRI</code> 插件接口，可用于支持多种容器运行时。</p>
<p>当前使用最为广泛的是 <a href="https://github.com/moby/moby/"><code>Docker</code></a>，当前还支持的主要有 <a href="https://github.com/opencontainers/runc"><code>runc</code></a>，<a href="https://github.com/containerd/containerd"><code>Containerd</code></a>，<a href="https://github.com/hyperhq/runv"><code>runV</code></a> 以及 <a href="https://github.com/rkt/rkt"><code>rkt</code></a> 等。</p>
<p>由于 Docker 的知识点很多，关于 Docker 的实践和内部原理可参考我之前的一次分享 <a href="https://github.com/tao12345666333/slides/raw/master/2018.09.13-Tech-Talk-Time/Docker实战和基本原理-张晋涛.pdf">Docker 实战和基本原理</a>。</p>
<p>在使用 K8S 时，也有极个别情况需要通过排查 Docker 的日志来分析问题。</p>
<p>至此，K8S 中主要的核心组件我们已经介绍完毕，下节我们主要集中于在 K8S 环境中，如何定位和解决问题，以及类似刚才提到的需要通过 Docker 进行排查问题的情况。</p>
<h1 id="Troubleshoot"><a href="#Troubleshoot" class="headerlink" title="Troubleshoot"></a>Troubleshoot</h1><h2 id="整体概览-11"><a href="#整体概览-11" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的介绍，我们已经了解到了 K8S 的基础知识，核心组件原理以及如何在 K8S 中部署服务及管理服务等。</p>
<p>但在生产环境中，我们所面临的环境多种多样，可能会遇到各种问题。本节将结合我们已经了解到的知识，介绍一些常见问题定位和解决的思路或方法，以便大家在生产中使用 K8S 能如鱼得水。</p>
<h2 id="应用部署问题"><a href="#应用部署问题" class="headerlink" title="应用部署问题"></a>应用部署问题</h2><p>首先我们从应用部署相关的问题来入手。这里仍然使用我们的<a href="https://github.com/tao12345666333/saythx">示例项目 SayThx</a>。</p>
<p>clone 该项目，进入到 deploy 目录中，先 <code>kubectl apply -f namespace.yaml</code> 或者 <code>kubectl create ns work</code> 来创建一个用于实验的 <code>Namespace</code> 。</p>
<h3 id="使用-describe-排查问题"><a href="#使用-describe-排查问题" class="headerlink" title="使用 describe 排查问题"></a>使用 <code>describe</code> 排查问题</h3><p>对 <code>redis-deployment.yaml</code> 稍作修改，按以下方式操作：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl -n work get all</span><br><span class="line">NAME                                READY     STATUS             RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-7574c98f5d-v66fx   0&#x2F;1       ImagePullBackOff   0          9s</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            0           9s</span><br><span class="line"></span><br><span class="line">NAME                                      DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-7574c98f5d   1         1         0         9s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 此刻的状态是 <code>ImagePullBackOff</code>，这个状态表示镜像拉取失败，<code>kubelet</code> 退出镜像拉取。</p>
<p>我们在前面的内容中介绍过 <code>kubelet</code> 的作用之一就是负责镜像拉取，而实际上，在镜像方面的错误主要预设了 6 种，分别是 <code>ImagePullBackOff</code>，<code>ImageInspectError</code>，<code>ErrImagePull</code>，<code>ErrImageNeverPull</code>，<code>RegistryUnavailable</code>，<code>InvalidImageName</code>。</p>
<p>当遇到以上所述情况时，便可定位为镜像相关异常。</p>
<p>我们回到上面的问题当中，定位问题所在。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work describe pod&#x2F;saythx-redis-7574c98f5d-v66fx</span><br><span class="line">Name:               saythx-redis-7574c98f5d-v66fx</span><br><span class="line">Namespace:          work</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               node01&#x2F;172.17.0.132</span><br><span class="line">Start Time:         Tue, 18 Dec 2018 17:27:56 +0000</span><br><span class="line">Labels:             app&#x3D;redis</span><br><span class="line">                    pod-template-hash&#x3D;3130754918</span><br><span class="line">Annotations:        &lt;none&gt;</span><br><span class="line">Status:             Pending</span><br><span class="line">IP:                 10.40.0.1</span><br><span class="line">Controlled By:      ReplicaSet&#x2F;saythx-redis-7574c98f5d</span><br><span class="line">Containers:</span><br><span class="line">  redis:</span><br><span class="line">    Container ID:</span><br><span class="line">    Image:          redis:5xx</span><br><span class="line">    Image ID:</span><br><span class="line">    Port:           6379&#x2F;TCP</span><br><span class="line">    Host Port:      0&#x2F;TCP</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       ImagePullBackOff</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount from default-token-787w5 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             False</span><br><span class="line">  ContainersReady   False</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-787w5:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-787w5</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io&#x2F;not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io&#x2F;unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason          Age                 From               Message</span><br><span class="line">  ----     ------          ----                ----               -------</span><br><span class="line">  Normal   Scheduled       11m                 default-scheduler  Successfully assigned work&#x2F;saythx-redis-7574c98f5d-v66fx to node01</span><br><span class="line">  Normal   SandboxChanged  10m                 kubelet, node01    Pod sandbox changed, it will bekilled and re-created.</span><br><span class="line">  Normal   BackOff         9m (x6 over 10m)    kubelet, node01    Back-off pulling image &quot;redis:5xx&quot;</span><br><span class="line">  Normal   Pulling         9m (x4 over 10m)    kubelet, node01    pulling image &quot;redis:5xx&quot;</span><br><span class="line">  Warning  Failed          9m (x4 over 10m)    kubelet, node01    Failed to pull image &quot;redis:5xx&quot;: rpc error: code &#x3D; Unknown desc &#x3D; Error response from daemon: manifest for redis:5xx not found</span><br><span class="line">  Warning  Failed          9m (x4 over 10m)    kubelet, node01    Error: ErrImagePull</span><br><span class="line">  Warning  Failed          49s (x44 over 10m)  kubelet, node01    Error: ImagePullBackOff</span><br></pre></td></tr></table></figure></div>

<p>可以看到我们现在 pull 的镜像是 <code>redis:5xx</code> 而实际上并不存在此 tag 的镜像，所以导致拉取失败。</p>
<h3 id="使用-events-排查问题"><a href="#使用-events-排查问题" class="headerlink" title="使用 events 排查问题"></a>使用 <code>events</code> 排查问题</h3><p>当然，我们还有另一种方式同样可进行问题排查：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work get events</span><br><span class="line">LAST SEEN   FIRST SEEN   COUNT     NAME                                             KIND         SUBOBJECT                TYPE      REASON              SOURCE                  MESSAGE</span><br><span class="line">21m         21m          1         saythx-redis.15717d6361a741a8                    Deployment                        Normal    ScalingReplicaSet   deployment-controller   Scaled up replica set saythx-redis-7574c98f5d to 1</span><br><span class="line">21m         21m          1         saythx-redis-7574c98f5d-qwxgm.15717d6363eb60ff   Pod                        Normal    Scheduled           default-scheduler       Successfully assigned work&#x2F;saythx-redis-7574c98f5d-qwxgm to node01</span><br><span class="line">21m         21m          1         saythx-redis-7574c98f5d.15717d636309afa8         ReplicaSet                        Normal    SuccessfulCreate    replicaset-controller   Created pod: saythx-redis-7574c98f5d-qwxgm</span><br><span class="line">20m         21m          2         saythx-redis-7574c98f5d-qwxgm.15717d63fa501b3f   Pod          spec.containers&#123;redis&#125;   Normal    BackOff             kubelet, node01         Back-off pulling image &quot;redis:5xx&quot;</span><br><span class="line">20m         21m          2         saythx-redis-7574c98f5d-qwxgm.15717d63fa5049a9   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ImagePullBackOff</span><br><span class="line">20m         21m          3         saythx-redis-7574c98f5d-qwxgm.15717d6393a1993c   Pod          spec.containers&#123;redis&#125;   Normal    Pulling             kubelet, node01         pulling image &quot;redis:5xx&quot;</span><br><span class="line">20m         21m          3         saythx-redis-7574c98f5d-qwxgm.15717d63e11efc7a   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ErrImagePull</span><br><span class="line">20m         21m          3         saythx-redis-7574c98f5d-qwxgm.15717d63e11e9c25   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Failed to pull image &quot;redis:5xx&quot;: rpc error: code &#x3D; Unknown desc &#x3D; Error response from daemon: manifest for redis:5xxnot found</span><br><span class="line">20m         20m          1         saythx-redis-54984ff94-2bb6g.15717d6dc03799cd    Pod          spec.containers&#123;redis&#125;   Normal    Killing             kubelet, node01         Killing container with id docker:&#x2F;&#x2F;redis:Need to kill Pod</span><br><span class="line">19m         19m          1         saythx-redis-7574c98f5d-v66fx.15717d72356528ec   Pod                        Normal    Scheduled           default-scheduler       Successfully assigned work&#x2F;saythx-redis-7574c98f5d-v66fx to node01</span><br><span class="line">19m         19m          1         saythx-redis-7574c98f5d.15717d722f7f1732         ReplicaSet                        Normal    SuccessfulCreate    replicaset-controller   Created pod: saythx-redis-7574c98f5d-v66fx</span><br><span class="line">19m         19m          1         saythx-redis.15717d722b49e758                    Deployment                        Normal    ScalingReplicaSet   deployment-controller   Scaled up replica set saythx-redis-7574c98f5d to 1</span><br><span class="line">19m         19m          1         saythx-redis-7574c98f5d-v66fx.15717d731a09b0ad   Pod                        Normal    SandboxChanged      kubelet, node01         Pod sandbox changed, it will be killed and re-created.</span><br><span class="line">18m         19m          6         saythx-redis-7574c98f5d-v66fx.15717d733ab20b3d   Pod          spec.containers&#123;redis&#125;   Normal    BackOff             kubelet, node01         Back-off pulling image &quot;redis:5xx&quot;</span><br><span class="line">18m         19m          4         saythx-redis-7574c98f5d-v66fx.15717d729de13541   Pod          spec.containers&#123;redis&#125;   Normal    Pulling             kubelet, node01         pulling image &quot;redis:5xx&quot;</span><br><span class="line">18m         19m          4         saythx-redis-7574c98f5d-v66fx.15717d72e6ded95d   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ErrImagePull</span><br><span class="line">18m         19m          4         saythx-redis-7574c98f5d-v66fx.15717d72e6de7b1c   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Failed to pull image &quot;redis:5xx&quot;: rpc error: code &#x3D; Unknown desc &#x3D; Error response from daemon: manifest for redis:5xxnot found</span><br><span class="line">4m          19m          66        saythx-redis-7574c98f5d-v66fx.15717d733ab23f2c   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ImagePullBackOff</span><br><span class="line">master</span><br></pre></td></tr></table></figure></div>

<p>我们在之前介绍时，也提到过 <code>kubelet</code> 或者 <code>kube-scheduler</code> 等组件会接受某些事件等，<code>event</code> 便是用于记录集群内各处发生的事件之类的。</p>
<h3 id="修正错误"><a href="#修正错误" class="headerlink" title="修正错误"></a>修正错误</h3><ul>
<li><p>修正配置文件</p>
<p>修正配置文件，然后 <code>kubectl apply -f redis-deployment.yaml</code> 便可应用修正后的配置文件。这种方法比较推荐，并且可以将修改过的位置纳入到版本控制系统中，有利于后续维护。</p>
</li>
<li><p>在线修改配置</p>
<p>使用 <code>kubectl -n work edit deploy/saythx-redis</code>，会打开默认的编辑器，我们可以将使用的镜像及 tag 修正为 <code>redis:5</code> 保存退出，便会自动应用新的配置。这种做法比较适合比较紧急或者资源是直接通过命令行创建等情况。 <strong>非特殊情况尽量不要在线修改。</strong> 且这样修改并不利于后期维护。</p>
</li>
</ul>
<h3 id="通过详细内容排查错误"><a href="#通过详细内容排查错误" class="headerlink" title="通过详细内容排查错误"></a>通过详细内容排查错误</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ vi redis-service.yaml # 稍微做了点修改</span><br><span class="line">master $ kubectl apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl -n work get pods,svc</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-z8prg   1&#x2F;1       Running   0          47s</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.108.202.170   &lt;none&gt;        6379:32355&#x2F;TCP   16s</span><br></pre></td></tr></table></figure></div>

<p>通过以上的输出，大多数情况下我们的 <code>Service</code> 应该是可以可以正常访问了，现在我们进行下测试：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ docker run --rm -it --net host redis redis-cli -p 32355</span><br><span class="line">Unable to find image &#39;redis:latest&#39; locally</span><br><span class="line">latest: Pulling from library&#x2F;redis</span><br><span class="line">a5a6f2f73cd8: Pull complete</span><br><span class="line">a6d0f7688756: Pull complete</span><br><span class="line">53e16f6135a5: Pull complete</span><br><span class="line">f52b0cc4e76a: Pull complete</span><br><span class="line">e841feee049e: Pull complete</span><br><span class="line">ccf45e5191d0: Pull complete</span><br><span class="line">Digest: sha256:bf65ecee69c43e52d0e065d094fbdfe4df6e408d47a96e56c7a29caaf31d3c35</span><br><span class="line">Status: Downloaded newer image for redis:latest</span><br><span class="line">Could not connect to Redis at 127.0.0.1:32355: Connection refused</span><br><span class="line">not connected&gt;</span><br></pre></td></tr></table></figure></div>

<p>我们先来介绍这里的测试方法。 使用 Docker 的 Redis 官方镜像， <code>--net host</code> 是使用宿主机网络； <code>--rm</code> 表示停止完后即清除； <code>-it</code> 分别表示获取输入及获取 TTY。</p>
<p>通过以上测试发现不能正常连接，故而说明 <code>Service</code> 还是未配置好。使用前面提到的方法也可以进行排查，不过这里提供另一种排查这类问题的思路。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS        AGE</span><br><span class="line">saythx-redis   10.32.0.4:6380   9m</span><br></pre></td></tr></table></figure></div>

<p>通过之前的章节，我们已经知道 <code>Service</code> 工作的时候是按 <code>Endpoints</code> 来的，这里我们发现此处的 <code>Endpoints</code> 是 <code>6380</code> 与我们预期的 <code>6379</code> 并不相同。所以问题定位于端口配置有误。</p>
<p>前面已经说过修正方法了，不再赘述。当修正完成后，再次验证：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS        AGE</span><br><span class="line">saythx-redis   10.32.0.4:6379   15m</span><br></pre></td></tr></table></figure></div>

<p><code>Endpoints</code> 已经正常，验证下服务是否可用：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ docker run --rm -it --net host redis redis-cli -p 32355</span><br><span class="line">127.0.0.1:32355&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure></div>

<p>验证无误。</p>
<h2 id="集群问题"><a href="#集群问题" class="headerlink" title="集群问题"></a>集群问题</h2><p>由于我们有多个节点，况且在集群搭建和维护过程中，也会比较常见到集群相关的问题。这里我们先举个实际例子进行分析：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl get nodes</span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    Ready      master    58m       v1.11.3</span><br><span class="line">node01    NotReady   &lt;none&gt;    58m       v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>通过 kubectl 查看，发现有一个节点 NotReady ，这在搭建集群的过程中也有可能遇到。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl get  node&#x2F;node01 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubeadm.alpha.kubernetes.io&#x2F;cri-socket: &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">    node.alpha.kubernetes.io&#x2F;ttl: &quot;0&quot;</span><br><span class="line">    volumes.kubernetes.io&#x2F;controller-managed-attach-detach: &quot;true&quot;</span><br><span class="line">  creationTimestamp: 2018-12-19T16:46:59Z</span><br><span class="line">  labels:</span><br><span class="line">    beta.kubernetes.io&#x2F;arch: amd64</span><br><span class="line">    beta.kubernetes.io&#x2F;os: linux</span><br><span class="line">    kubernetes.io&#x2F;hostname: node01</span><br><span class="line">  name: node01</span><br><span class="line">  resourceVersion: &quot;4850&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;node01</span><br><span class="line">  uid: b440d3d5-03ad-11e9-917e-0242ac110035</span><br><span class="line">spec: &#123;&#125;</span><br><span class="line">status:</span><br><span class="line">  addresses:</span><br><span class="line">  - address: 172.17.0.66</span><br><span class="line">    type: InternalIP</span><br><span class="line">  - address: node01</span><br><span class="line">    type: Hostname</span><br><span class="line">  allocatable:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: &quot;89032026784&quot;</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3894652Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  capacity:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: 96605932Ki</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3997052Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  conditions:</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: OutOfDisk</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: MemoryPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: DiskPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T16:46:59Z</span><br><span class="line">    message: kubelet has sufficient PID available</span><br><span class="line">    reason: KubeletHasSufficientPID</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: PIDPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: Ready</span><br><span class="line">  daemonEndpoints:</span><br><span class="line">    kubeletEndpoint:</span><br><span class="line">      Port: 10250</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></div>

<p>我们之前介绍 <code>kubelet</code> 时说过， <code>kubelet</code> 的作用之一便是将自身注册至 <code>kube-apiserver</code>。</p>
<p>这里的 message 信息说明 <code>kubelet</code> 不再向 <code>kube-apiserver</code> 发送心跳包之类的了，所以被判定为 NotReady 的状态。</p>
<p>接下来，我们登录 node01 机器查看 <code>kubelet</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">node01 $ systemctl status kubelet</span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kubelet.service; enabled; vendor preset: enabled)</span><br><span class="line">  Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d</span><br><span class="line">           └─kubeadm.conf</span><br><span class="line">   Active: inactive (dead) since Wed 2018-12-19 17:42:17 UTC; 18min ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;home&#x2F;</span><br><span class="line">  Process: 1693 ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_</span><br><span class="line"> Main PID: 1693 (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br></pre></td></tr></table></figure></div>

<p>可以看到该机器上 <code>kubelet</code> 没有启动。现在将其启动，稍等片刻看看节群中 <code>Node</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    1h        v1.11.3</span><br><span class="line">node01    Ready     &lt;none&gt;    1h        v1.11.3</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-17"><a href="#总结-17" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了 K8S 中常用的问题排查和解决思路，但实际生产环境中情况会有和更多不确定因素，掌握本节中介绍的基础，有利于之后生产环境中进行常规问题的排查。</p>
<p>当然，本节只是介绍通过 kubectl 来定位和解决问题，个别情况下我们需要登录相关的节点，实际去使用 <code>Docker</code> 工具等进行问题的详细排查。</p>
<p>至此，K8S 的基础原理和常规问题排查思路等都已经通过包括本节在内的 19 小节介绍完毕，相信你现在已经迫不及待的想要使用 K8S 了。</p>
<p>不过 kubectl 作为命令行工具也许有些人会不习惯使用，下节，我们将介绍 K8S 的扩展组件 <code>kube-dashboard</code> 了解它的主要功能及带给我们的便利。</p>
<h1 id="扩展增强：Dashboard"><a href="#扩展增强：Dashboard" class="headerlink" title="扩展增强：Dashboard"></a>扩展增强：Dashboard</h1><h2 id="整体概览-12"><a href="#整体概览-12" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的介绍，想必你已经迫不及待的想要将应用部署至 K8S 中，但总是使用 <code>kubectl</code> 或者 <code>Helm</code> 等命令行工具也许不太直观，你可能想要一眼就看到集群当前的状态，或者想要更方便的对集群进行管理。</p>
<p>本节将介绍一个 Web 项目 <a href="https://github.com/kubernetes/dashboard"><code>Dashboard</code></a> 可用于部署容器化的应用程序，管理集群中的资源，甚至是排查和解决问题。</p>
<p>当然它和大多数 Dashboard 类的项目类似，也为集群的状态提供了一个很直观的展示。</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/23/167d6b6e66c60e89?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><p>要想使用 Dashboard，首先我们需要安装它，而 Dashboard 的安装其实也很简单。不过对于国内用户需要注意的是需要解决网络问题，或替换镜像地址等。</p>
<p>这里我们安装当前最新版 <code>v1.10.1</code> 的 Dashboard：</p>
<ul>
<li><p>对于已经解决网络问题的用户：</p>
<p>可使用官方推荐做法进行安装，以下链接是使用了我提交了 path 的版本，由于官方最近的一次更新导致配置文件中的镜像搞错了。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;tao12345666333&#x2F;dashboard&#x2F;67970554aa9275cccec1d1ee5fbf89ae81b3b614&#x2F;aio&#x2F;deploy&#x2F;recommended&#x2F;kubernetes-dashboard.yaml</span><br><span class="line">secret&#x2F;kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount&#x2F;kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard created</span><br><span class="line">service&#x2F;kubernetes-dashboard created</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>也可使用我修改过的这份（使用 Docker Hub 同步了镜像）仓库地址 <a href="https://github.com/tao12345666333/k8s-dashboard">GitHub</a>, 国内 <a href="https://gitee.com/K8S-release/k8s-dashboard" target="_blank" rel="noopener">Gitee</a>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f https:&#x2F;&#x2F;gitee.com&#x2F;K8S-release&#x2F;k8s-dashboard&#x2F;raw&#x2F;master&#x2F;kubernetes-dashboard.yaml</span><br><span class="line">secret&#x2F;kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount&#x2F;kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard created</span><br><span class="line">service&#x2F;kubernetes-dashboard created</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<p>当已经执行完以上步骤后，可检查下是否安装成功：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system get all  -l k8s-app&#x3D;kubernetes-dashboard</span><br><span class="line">NAME                                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;kubernetes-dashboard-67896bc598-dhdpz   1&#x2F;1       Running   0          3m</span><br><span class="line"></span><br><span class="line">NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes-dashboard   ClusterIP   10.109.92.207   &lt;none&gt;        443&#x2F;TCP   3m</span><br><span class="line"></span><br><span class="line">NAME                                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard   1         1         1            1           3m</span><br><span class="line"></span><br><span class="line">NAME                                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;kubernetes-dashboard-67896bc598   1         1         1         3m</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 已经在正常运行，接下来便是访问 Dashboard.</p>
<h2 id="访问-Dashboard"><a href="#访问-Dashboard" class="headerlink" title="访问 Dashboard"></a>访问 Dashboard</h2><p>以当前的部署方式，<code>Service</code> 使用了 <code>ClusterIP</code> 的类型，所以在集群外不能直接访问。我们先使用 <code>kubectl</code> 提供的 <code>port-forward</code> 功能进行访问。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system port-forward pod&#x2F;kubernetes-dashboard-67896bc598-dhdpz 8443</span><br><span class="line">Forwarding from 127.0.0.1:8443 -&gt; 8443</span><br><span class="line">Forwarding from [::1]:8443 -&gt; 8443</span><br></pre></td></tr></table></figure></div>

<p>还记得，我们在第 5 节时候安装过一个名为 <code>socat</code> 的依赖项吗？ <code>socat</code> 的主要功能便是端口转发。</p>
<p>现在在浏览器打开 <a href="https://127.0.0.1:8443/" target="_blank" rel="noopener"><code>https://127.0.0.1:8443</code></a> 便可看到如下的登录界面。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1176" height="565"></svg>)</p>
<p>对于我们的 <strong>新版本</strong> 而言，我们 <strong>使用令牌登录</strong> 的方式。</p>
<h3 id="查找-Token"><a href="#查找-Token" class="headerlink" title="查找 Token"></a>查找 Token</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system get serviceaccount -l k8s-app&#x3D;kubernetes-dashboard -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  kind: ServiceAccount</span><br><span class="line">  metadata:</span><br><span class="line">    annotations:</span><br><span class="line">      kubectl.kubernetes.io&#x2F;last-applied-configuration: |</span><br><span class="line">        &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;ServiceAccount&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;k8s-app&quot;:&quot;kubernetes-dashboard&quot;&#125;,&quot;name&quot;:&quot;kubernetes-dashboard&quot;,&quot;namespace&quot;:&quot;kube-system&quot;&#125;&#125;</span><br><span class="line">    creationTimestamp: 2018-12-20T17:27:14Z</span><br><span class="line">    labels:</span><br><span class="line">      k8s-app: kubernetes-dashboard</span><br><span class="line">    name: kubernetes-dashboard</span><br><span class="line">    namespace: kube-system</span><br><span class="line">    resourceVersion: &quot;1400&quot;</span><br><span class="line">    selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;serviceaccounts&#x2F;kubernetes-dashboard</span><br><span class="line">    uid: 7e01ddda-047c-11e9-b55c-0242ac11002a</span><br><span class="line">  secrets:</span><br><span class="line">  - name: kubernetes-dashboard-token-6ck2l</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure></div>

<p>首先，我们查看刚才创建出的 <code>serviceaccount</code> 可以看到其中有配置 <code>secrets</code> 。</p>
<p>查看该 <code>secret</code> 详情获得 Token</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system describe secrets kubernetes-dashboard-token-6ck2l</span><br><span class="line">Name:         kubernetes-dashboard-token-6ck2l</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io&#x2F;service-account.name&#x3D;kubernetes-dashboard</span><br><span class="line">              kubernetes.io&#x2F;service-account.uid&#x3D;7e01ddda-047c-11e9-b55c-0242ac11002a</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io&#x2F;service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">ca.crt:     1025 bytes</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi02Y2sybCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjdlMDFkZGRhLTA0N2MtMTFlOS1iNTVjLTAyNDJhYzExMDAyYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.WZ5YRUkGlKRSpkBFCk3BrZ6p2t1qVxEs7Kb18DP5X2C2lfMhDrB931PeN05uByLD6biz_4IQvKh4xmvY2RqekfV1BLCfcIiMUbc1lcXGbhH4g4vrsjYx3NZifaBh_5HuBlEL5zs5e_zFkPEhhIqjsY3KueFEuGwxTAsqGBQwawc-v6wqzB3Gzb01o1iN5aTb37PVG5gTTE8cQLih_urKhvdNEKBSRg_zHQlYjFrtUUWYRYMlYz_sWmamYVXHy_7NvKrBfw44WU5tLxMITkoUEGVwROBnHf_BcWVedozLg2uLVontB12YvhmTfJCDEAJ8o937bS-Fq3tLfu_xM40fqw</span><br></pre></td></tr></table></figure></div>

<p>将此处的 token 填入输入框内便可登录，<strong>注意这里使用的是 <code>describe</code>。</strong></p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="635"></svg>)</p>
<h3 id="修正权限"><a href="#修正权限" class="headerlink" title="修正权限"></a>修正权限</h3><p>但是我们注意到这里有很多提示 <code>configmaps is forbidden: User &quot;system:serviceaccount:kube-system:kubernetes-dashboard&quot; cannot list resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;default&quot;</code> 。根据我们前面的介绍，这很明显就是用户权限不足。</p>
<p>我们已经知道，当前我们的集群是开启了 <code>RBAC</code> 的，所以这里我们还是以前面学到的方法创建一个用户并进行授权。</p>
<ul>
<li><p>创建 ServiceAccount：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>创建 RoleBinding: 这里为了方便直接绑定了 <code>cluster-admin</code> 的 ClusterRole ，但是生产环境下，请按照实际情况进行授权，参考前面第 8 节相关的内容。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<p>使用以上配置创建了用户和绑定，然后还是同样的办法获取 Token。</p>
<p>点击 Dashboard 右上角，退出登录后，重新使用新的 Token 进行登录。登录完成后便可看到如下图：</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="607"></svg>)</p>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><p>点击右上角的 <strong>+创建</strong> 可进入创建页面，现在支持三种模式：从文本框输入；从文件创建；直接创建应用。</p>
<p>我们仍然以我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 为例。先 <code>clone</code> 该项目，并进入项目的 <code>deploy</code> 目录中。将 <code>namespace.yaml</code> 的内容复制进输入框，点击上传按钮，便可创建名为 <code>work</code> 的 <code>Namespace</code> 了。</p>
<p>通过以下命令验证：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl get ns</span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">default       Active    2h</span><br><span class="line">kube-public   Active    2h</span><br><span class="line">kube-system   Active    2h</span><br><span class="line">work          Active    10s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 Namespace 已经创建成功。或者刷新下网页，点击左侧的命名空间即可看到当前的所有 <code>Namespace</code> 。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="196" height="246"></svg>)</p>
<p>我们先将左侧的命名空间选择为 <strong>全部命名空间</strong> 或 <strong>work</strong> (当刷新过网页后) ，接下来继续点击右上角的 <strong>+创建</strong> 按钮，将 <code>redis-deployment.yaml</code> 的内容复制进输入框，点击上传按钮，部署 Redis 。</p>
<p>部署成功后，点击 部署 ，点击刚才的 <code>saythx-redis</code> 便可看到其详情。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="603"></svg>)</p>
<p>点击左侧的容器组，便可看到刚才部署的 Pod，</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="612"></svg>)</p>
<p>在此页面的右上角，可以点击命令行按钮，打开新标签页进入其内部执行命令。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="609"></svg>)</p>
<p>或者是点击日志按钮，可打开新标签页，查看日志。</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/23/167d6bafed569ab1?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="总结-18"><a href="#总结-18" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了 <code>Kubernetes Dashboard</code> 的基本功能，以及如何安装和使用它。</p>
<p>Dashboard 相比 <code>kubectl</code> 为用户提供了一种相对直观的 Web 端操作方式，但是并不能完全取代 <code>kubectl</code>，这两者应该是相辅相成的。</p>
<p>如果你所需的功能相对简单或是想要给某些用户提供一种通过 Web 操作的方式，那便推荐使用 Dashboard。Dashboard 的后端使用了 K8S 的 <a href="https://github.com/kubernetes/client-go"><code>client-go</code></a> ，前端主要使用了 <a href="https://angular.io/" target="_blank" rel="noopener">Angular</a>，有兴趣可以大致看看其源代码，对于开发基于 K8S 的云平台会有些启发。</p>
<p>下节，我们将介绍用于 DNS 和服务发现的插件 <a href="https://coredns.io/" target="_blank" rel="noopener">CoreDNS</a>，学习如何利用它完成这些需求。并且它在 K8S 1.13 版本中，已经成为了默认的 DNS server。</p>
<h1 id="扩展增强：CoreDNS"><a href="#扩展增强：CoreDNS" class="headerlink" title="扩展增强：CoreDNS"></a>扩展增强：CoreDNS</h1><h2 id="整体概览-13"><a href="#整体概览-13" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们知道在 K8S 中有一套默认的<a href="https://github.com/kubernetes/dns">集群内 DNS 服务</a>，我们通常把它叫做 <code>kube-dns</code>，它基于 SkyDNS，为我们在服务注册发现方面提供了很大的便利。</p>
<p>比如，在我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 中，各组件便是依赖 DNS 进行彼此间的调用。</p>
<p>本节，我们将介绍的 <a href="https://coredns.io/" target="_blank" rel="noopener">CoreDNS</a> 是 CNCF 旗下又一孵化项目，在 K8S 1.9 版本中加入并进入 Alpha 阶段。我们当前是以 K8S 1.11 的版本进行介绍，它并不是默认的 DNS 服务，但是<a href="https://github.com/kubernetes/enhancements/issues/427">它作为 K8S 的 DNS 插件的功能已经 GA</a> 。</p>
<p>CoreDNS 在 K8S 1.13 版本中才正式成为<a href="https://kubernetes.io/blog/2018/12/03/kubernetes-1-13-release-announcement/" target="_blank" rel="noopener">默认的 DNS 服务</a>。</p>
<h2 id="CoreDNS-是什么"><a href="#CoreDNS-是什么" class="headerlink" title="CoreDNS 是什么"></a>CoreDNS 是什么</h2><p>首先，我们需要明确 CoreDNS 是一个独立项目，它不仅可支持在 K8S 中使用，你也可以在你任何需要 DNS 服务的时候使用它。</p>
<p>CoreDNS 使用 Go 语言实现，部署非常方便。</p>
<p>它的扩展性很强，很多功能特性都是通过插件完成的，它不仅有大量的<a href="https://coredns.io/plugins/" target="_blank" rel="noopener">内置插件</a>，同时也有很丰富的<a href="https://coredns.io/explugins/" target="_blank" rel="noopener">第三方插件</a>。甚至你自己<a href="https://coredns.io/2016/12/19/writing-plugins-for-coredns/" target="_blank" rel="noopener">写一个插件</a>也非常的容易。</p>
<h2 id="如何安装使用-CoreDNS"><a href="#如何安装使用-CoreDNS" class="headerlink" title="如何安装使用 CoreDNS"></a>如何安装使用 CoreDNS</h2><p>我们这里主要是为了说明如何在 K8S 环境中使用它，所以对于独立安装部署它不做说明。</p>
<p>本小册中我们使用的是 K8S 1.11 版本，在第 5 小节 《搭建 Kubernetes 集群》中，我们介绍了使用 <code>kubeadm</code> 搭建集群。</p>
<p>使用 <code>kubeadm</code> 创建集群时候 <code>kubeadm init</code> 可以传递 <code>--feature-gates</code> 参数，用于启用一些额外的特性。</p>
<p>比如在之前版本中，我们可以通过 <code>kubeadm init --feature-gates CoreDNS=true</code> 在创建集群时候启用 CoreDNS。</p>
<p>而在 1.11 版本中，使用 <code>kubeadm</code> 创建集群时 <code>CoreDNS</code> 已经被默认启用，这也从侧面证明了 CoreDNS 在 K8S 中达到了生产可用的状态。</p>
<p>我们来看一下创建集群时的日志输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubeadm init</span><br><span class="line">[init] using Kubernetes version: v1.11.3               </span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">...</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br></pre></td></tr></table></figure></div>

<p>可以看到创建时已经启用了 CoreDNS 的扩展，待集群创建完成后，可用过以下方式进行查看：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system get all  -l k8s-app&#x3D;kube-dns -o wide</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE</span><br><span class="line">pod&#x2F;coredns-78fcdf6894-5zbx4   1&#x2F;1       Running   0          1h        10.32.0.3   node01    &lt;none&gt;</span><br><span class="line">pod&#x2F;coredns-78fcdf6894-cxdw8   1&#x2F;1       Running   0          1h        10.32.0.2   node01    &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE       SELECTOR</span><br><span class="line">service&#x2F;kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP   1h        k8s-app&#x3D;kube-dns</span><br><span class="line"></span><br><span class="line">NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES                     SELECTOR</span><br><span class="line">deployment.apps&#x2F;coredns   2         2         2            2           1h        coredns      k8s.gcr.io&#x2F;coredns:1.1.3   k8s-app&#x3D;kube-dns</span><br><span class="line"></span><br><span class="line">NAME                                 DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES                   SELECTOR</span><br><span class="line">replicaset.apps&#x2F;coredns-78fcdf6894   2         2         2         1h        coredns      k8s.gcr.io&#x2F;coredns:1.1.3   k8s-app&#x3D;kube-dns,pod-template-hash&#x3D;3497892450</span><br></pre></td></tr></table></figure></div>

<p>这里主要是为了兼容 K8S 原有的 <code>kube-dns</code> 所以标签和 <code>Service</code> 的名字都还使用了 <code>kube-dns</code>，但实际在运行的则是 CoreDNS。</p>
<h2 id="验证-CoreDNS-功能"><a href="#验证-CoreDNS-功能" class="headerlink" title="验证 CoreDNS 功能"></a>验证 CoreDNS 功能</h2><p>从上面的输出我们看到 CoreDNS 的 <code>Pod</code> 运行正常，现在测试下它是否能正确解析。仍然以我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 为例，先 clone 项目，进入到项目的 deploy 目录中。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ cd saythx&#x2F;deploy&#x2F;</span><br><span class="line">master $ ls</span><br><span class="line">backend-deployment.yaml  frontend-deployment.yaml  namespace.yaml         redis-service.yaml</span><br><span class="line">backend-service.yaml     frontend-service.yaml     redis-deployment.yaml  work-deployment.yaml</span><br><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br></pre></td></tr></table></figure></div>

<ul>
<li>查看其部署情况：</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n work get all</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-8v4lp   1&#x2F;1       Running   0          2m</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.109.215.147   &lt;none&gt;        6379:31438&#x2F;TCP   2m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            1           2m</span><br><span class="line"></span><br><span class="line">NAME                                     DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d   1         1         1         2m</span><br></pre></td></tr></table></figure></div>

<ul>
<li>验证 DNS 是否正确解析:</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line"># 使用 AlpineLinux 的镜像创建一个 Pod 并进入其中</span><br><span class="line">master $ kubectl run alpine -it --rm --restart&#x3D;&#39;Never&#39; --image&#x3D;&#39;alpine&#39; sh</span><br><span class="line">If you don&#39;t see a command prompt, try pressing enter.</span><br><span class="line">&#x2F; # apk add --no-cache bind-tools</span><br><span class="line">fetch http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.8&#x2F;main&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">fetch http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.8&#x2F;community&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">(1&#x2F;5) Installing libgcc (6.4.0-r9)</span><br><span class="line">(2&#x2F;5) Installing json-c (0.13.1-r0)</span><br><span class="line">(3&#x2F;5) Installing libxml2 (2.9.8-r1)</span><br><span class="line">(4&#x2F;5) Installing bind-libs (9.12.3-r0)</span><br><span class="line">(5&#x2F;5) Installing bind-tools (9.12.3-r0)</span><br><span class="line">Executing busybox-1.28.4-r2.trigger</span><br><span class="line">OK: 9 MiB in 18 packages</span><br><span class="line"></span><br><span class="line"># 安装完 dig 命令所在包之后，使用 dig 命令进行验证</span><br><span class="line">&#x2F; # dig @10.32.0.2 saythx-redis.work.svc.cluster.local +noall +answer</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.12.3 &lt;&lt;&gt;&gt; @10.32.0.2 saythx-redis.work.svc.cluster.local +noall +answer</span><br><span class="line">; (1 server found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">saythx-redis.work.svc.cluster.local. 5 IN A     10.109.215.147</span><br></pre></td></tr></table></figure></div>

<p>通过以上操作，可以看到相应的 <code>Service</code> 记录可被正确解析。这里有几个点需要注意：</p>
<ul>
<li><p>域名解析是可跨 <code>Namespace</code> 的</p>
<p>刚才的示例中，我们没有指定 <code>Namespace</code> 所以刚才我们所在的 <code>Namespace</code> 是 <code>default</code>。而我们的解析实验成功了。说明 CoreDNS 的解析是全局的。<strong>虽然解析是全局的，但不代表网络互通</strong></p>
</li>
<li><p>域名有特定格式</p>
<p>可以看到刚才我们使用的完整域名是 <code>saythx-redis.work.svc.cluster.local</code> , 注意开头的便是 <strong>Service 名.Namespace 名</strong> 当然，我们也可以直接通过 <code>host</code> 命令查询:</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">&#x2F; # host -t srv  saythx-redis.work</span><br><span class="line">saythx-redis.work.svc.cluster.local has SRV record 0 100 6379 saythx-redis.work.svc.cluster.local.</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<h2 id="配置和监控"><a href="#配置和监控" class="headerlink" title="配置和监控"></a>配置和监控</h2><p>CoreDNS 使用 <code>ConfigMap</code> 的方式进行配置，但是如果更改了配置，<code>Pod</code> 重启后才会生效。</p>
<p>我们通过以下命令可查看其配置：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n kube-system get configmap coredns -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           upstream</span><br><span class="line">           fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        proxy . &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">    &#125;</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2018-12-22T16:45:47Z</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: &quot;217&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;configmaps&#x2F;coredns</span><br><span class="line">  uid: 0882e51b-0609-11e9-b25e-0242ac110057</span><br></pre></td></tr></table></figure></div>

<p><code>Corefile</code> 便是它的配置文件，可以看到它启动了类似 <code>kubernetes</code>, <code>prometheus</code> 等插件。</p>
<p>注意 <code>kubernetes</code> 插件的配置，使用的域是 <code>cluster.local</code> ，这也是上面我们提到域名格式时候后半部分未解释的部分。</p>
<p>至于 <code>prometheus</code> 插件，则是监听在 9153 端口上提供了符合 Prometheus 标准的 metrics 接口，可用于监控等。关于监控的部分，可参考第 23 节。</p>
<h2 id="总结-19"><a href="#总结-19" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 CoreDNS 的基本情况，它是以 Go 编写的灵活可扩展的 DNS 服务器。</p>
<p>使用 CoreDNS 代替 kube-dns 主要是为了解决一些 kube-dns 时期的问题，比如说原先 kube-dns 的时候，一个 Pod 中还需要包含 <code>kube-dns</code>, <code>sidecar</code> 和 <code>dnsmasq</code> 的容器，而每当 <code>dnsmasq</code> 出现漏洞时，就不得不让 K8S 发个安全补丁才能进行更新。</p>
<p>CoreDNS 有丰富的插件，可以满足更多样的应用需求，同时 <code>kubernetes</code> 插件还包含了一些独特的功能，比如 Pod 验证之类的，可增加安全性。</p>
<p>同时 CoreDNS 在 1.13 版本中会作为默认的 DNS 服务器使用，所以应该给它更多的关注。</p>
<p>在下节，我们将介绍 <code>Ingress</code>，看看如果使用不同于之前使用的 <code>NodePort</code> 的方式将服务暴露于外部。</p>
<h1 id="服务增强：Ingress"><a href="#服务增强：Ingress" class="headerlink" title="服务增强：Ingress"></a>服务增强：Ingress</h1><h2 id="整体概览-14"><a href="#整体概览-14" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们已经知道 K8S 中有 <code>Service</code> 的概念，同时默认情况下还有 <code>CoreDNS</code> 完成集群内部的域名解析等工作，以此完成基础的服务注册发现能力。</p>
<p>在第 7 节中，我们介绍了 <code>Service</code> 的 4 种基础类型，在前面的介绍中，我们一般都在使用 <code>ClusterIP</code> 或 <code>NodePort</code> 等方式将服务暴露在集群内或者集群外。</p>
<p>本节，我们将介绍另一种处理服务访问的方式 <code>Ingress</code>。</p>
<h2 id="Ingress-是什么"><a href="#Ingress-是什么" class="headerlink" title="Ingress 是什么"></a>Ingress 是什么</h2><p>通过 <code>kubectl explain ingress</code> 命令，我们来看下对 Ingress 的描述。</p>
<blockquote>
<p>Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend. An Ingress can be configured to give services externally-reachable urls, load balance traffic, terminate SSL, offer name based virtual hosting etc.</p>
</blockquote>
<p><code>Ingress</code> 是一组允许外部请求进入集群的路由规则的集合。它可以给 <code>Service</code> 提供集群外部访问的 URL，负载均衡，SSL 终止等。</p>
<p>直白点说，<code>Ingress</code> 就类似起到了智能路由的角色，外部流量到达 <code>Ingress</code> ，再由它按已经制定好的规则分发到不同的后端服务中去。</p>
<p>看起来它很像我们使用的负载均衡器之类的。那你可能会问，<code>Ingress</code> 与 <code>LoadBalancer</code> 类型的 <code>Service</code> 的区别是什么呢？</p>
<ul>
<li><p><code>Ingress</code> 不是一种 <code>Service</code> 类型</p>
<p><code>Ingress</code> 是 K8S 中的一种资源类型，我们可以直接通过 <code>kubectl get ingress</code> 的方式获取我们已有的 <code>Ingress</code> 资源。</p>
</li>
<li><p><code>Ingress</code> 可以有多种控制器（实现）</p>
<p>通过之前的介绍，我们知道 K8S 中有很多的 <code>Controller</code> (控制器)，而这些 <code>Controller</code> 已经打包进了 <code>kube-controller-manager</code> 中，通过 <code>--controllers</code> 参数控制启用哪些。</p>
<p>但是 <code>Ingress</code> 的 <code>Controller</code> 并没有包含在其中，而且有多种选择。</p>
<p>由社区维护（或是说官方支持的）有两个：适用于 Google Cloud 的 <a href="https://github.com/kubernetes/ingress-gce">GLBC</a>，当你使用 GKE 的时候，便会看到它；和 <a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a> 它是使用 <code>ConfigMap</code> 存储 NGINX 配置实现的。</p>
<p>第三方的实现还有：基于 Envoy 的 <a href="https://github.com/heptio/contour">Contour</a>; F5 的 <a href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/v1.7/" target="_blank" rel="noopener">F5 BIG-IP Controller</a>; 基于 HAProxy 的 <a href="https://github.com/jcmoraisjr/haproxy-ingress">haproxy-ingress</a>; 基于 Istio 的 <a href="https://istio.io/docs/tasks/traffic-management/ingress/" target="_blank" rel="noopener">Control Ingress Traffic</a>; 现代化的反向代理服务器 <a href="https://github.com/containous/traefik">Traefik</a>; 以及 Kong 支持的 <a href="https://konghq.com/blog/kubernetes-ingress-controller-for-kong/" target="_blank" rel="noopener">Kong Ingress Controller for Kubernetes</a> 和 NGINX 官方支持的 <a href="https://github.com/nginxinc/kubernetes-ingress">NGINX Ingress Controller</a>。</p>
<p>这里可以看到 K8S 社区和 NGINX 都有 NGINX Ingress Controller，很多人在一开始接触 Ingress 的时候便陷入了选择的苦恼中，除去前面的那些选择外，单 NGINX 的控制器就有两个，到底应该怎么选。</p>
<p>这里提供两点建议：</p>
<ul>
<li>可能多数人使用的都是 NGINX 而非 NGINX Plus，如果你需要会话保持（Session persistence）的话，那你应该选择 K8S 社区维护的版本</li>
<li>即使我们平时使用 NGINX 的时候，也常常会有动态配置的需求，如果你仍然有这样的需求，那你还是继续使用 K8S 社区维护的版本（其中内置了 Lua 支持）。</li>
</ul>
</li>
</ul>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>前面也已经说到了，单纯的创建一个 <code>Ingress</code> 资源没什么意义，我们需要先配置一个 <code>Controller</code> ，才能让它正常工作。国内使用 GKE 的可能不是很多，为了更加通用，这里我们选择 K8S 社区维护的 NGINX Ingress Controller。</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>整个安装过程其实也比较简单，具体步骤如下（以下步骤中都将直接展示该步骤所需的 YAML 配置文件）：</p>
<ul>
<li><p>创建 <code>Namespace</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br></pre></td></tr></table></figure></div>

<p>将以上内容保存为 namespace.yaml 文件，然后执行 <code>kubectl apply -f namespace.yaml</code> 即可。以下步骤均类似，不再赘述。 注意：这里创建 <code>Namespace</code> 只是为了保持集群相对规范，非强制，但推荐此做法。</p>
</li>
<li><p>创建 <code>ConfigMap</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></div>

<p>这里创建了几个 <code>ConfigMap</code>，主要是给 <code>Controller</code> 使用。</p>
</li>
<li><p>由于我们的集群使用 <code>kubeadm</code> 创建时，默认开启了 <code>RBAC</code> ，所以这里需要相应的创建对应的 <code>Role</code> 和 <code>RoleBinding</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-serviceaccount</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - endpoints</span><br><span class="line">      - nodes</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses&#x2F;status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    resourceNames:</span><br><span class="line">      - &quot;ingress-controller-leader-nginx&quot;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role-nisa-binding</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole-nisa-binding</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></div>

<p>关于 <code>RBAC</code> 相关的内容，可查看第 8 节 《安全重点: 认证和授权》，了解此处的配置及其含义。</p>
</li>
<li><p>部署 NGINX Ingress Controller</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">      app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">        app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;port: &quot;10254&quot;</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: taobeier&#x2F;nginx-ingress-controller:0.21.0</span><br><span class="line">          args:</span><br><span class="line">            - &#x2F;nginx-ingress-controller</span><br><span class="line">            - --configmap&#x3D;$(POD_NAMESPACE)&#x2F;nginx-configuration</span><br><span class="line">            - --tcp-services-configmap&#x3D;$(POD_NAMESPACE)&#x2F;tcp-services</span><br><span class="line">            - --udp-services-configmap&#x3D;$(POD_NAMESPACE)&#x2F;udp-services</span><br><span class="line">            - --publish-service&#x3D;$(POD_NAMESPACE)&#x2F;ingress-nginx</span><br><span class="line">            - --annotations-prefix&#x3D;nginx.ingress.kubernetes.io</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: &#x2F;healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: &#x2F;healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br></pre></td></tr></table></figure></div>

<p>注意，这里的镜像是我从官方镜像直接同步的，为了解决国内无法下载镜像的情况。</p>
<p>另外，在启动参数中，指定了我们第二步中创建的 <code>ConfigMap</code> 。以及，在此部署中，用到了之前尚未详细说明的 <code>readinessProbe</code> 和 <code>livenessProbe</code>：我们之前在详解 <code>kubelet</code> 时，大致提到过关于它所具备的职责，这两个配置主要是用于做探针，用户检查 Pod 是否已经准备好接受请求流量和是否存活。</p>
<p>这里还进行了 <code>annotations</code> 里面标注了关于 <code>Prometheus</code> 的相关内容，我们会在下节中描述。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n ingress-nginx get all</span><br><span class="line">NAME                                            READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;nginx-ingress-controller-6f647f7866-659ph   1&#x2F;1       Running   0          75s</span><br><span class="line"></span><br><span class="line">NAME                                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;nginx-ingress-controller   1         1         1            1           75s</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;nginx-ingress-controller-6f647f7866   1         1         1         75s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 NGINX Ingress Controller 已经部署成功。</p>
</li>
<li><p><strong>将 NGINX Ingress Controller 暴露至集群外</strong></p>
<p>经过前面的介绍，我们已经知道 Ingress 的作用在于将集群外的请求流量转向集群内的服务，而我们知道，默认情况下集群外和集群内是不互通的，所以必须将 NGINX Ingress Controller 暴露至集群外，以便让其能接受来自集群外的请求。</p>
<p>将其暴露的方式有很多种，这里我们选择我们之前已经介绍过的 <code>NodePort</code> 的方式。选择它主要有以下原因：</p>
<ul>
<li>我们可以使用纯的 LB 实现完成服务暴露，比如 <a href="https://metallb.universe.tf/" target="_blank" rel="noopener">MetalLB</a>，但它还处于 Beta 阶段，尚未有大规模生产环境使用的验证。</li>
<li>我们可以直接使用宿主机的网络，只需设置 <code>hostNetwork: true</code> 即可，但这个方式可能会带来安全问题。</li>
<li>我们可以选择 External IPs 的方式，但这种方式无法保留请求的源 IP，所以并不是很好。</li>
<li>其实我们一般会选择自己提供边缘节点的方式，不过这种方式是建立在 <code>NodePort</code> 的方式之上，并且需要提供额外的组件，此处就暂不做展开了。</li>
</ul>
<p>我们使用以下的配置，将 NGINX Ingress Controller 暴露至集群外</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - name: http</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">    - name: https</span><br><span class="line">      port: 443</span><br><span class="line">      targetPort: 443</span><br><span class="line">      protocol: TCP</span><br><span class="line">  selector:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br></pre></td></tr></table></figure></div>

<p>创建该 <code>Service</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl -n ingress-nginx get svc                                                  </span><br><span class="line">NAME            TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">ingress-nginx   NodePort   10.0.38.53   &lt;none&gt;        80:30871&#x2F;TCP,443:30356&#x2F;TCP   11s</span><br></pre></td></tr></table></figure></div>

<p>现在，我们直接访问 <code>Node:Port</code> 便可访问到该 Controller。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ curl 172.17.0.3:30871                    </span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;404 Not Found&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;404 Not Found&lt;&#x2F;h1&gt;&lt;&#x2F;center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx&#x2F;1.15.6&lt;&#x2F;center&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure></div>

<p>由于我们并没有设置任何的默认响应后端，所以当直接请求时，便返回 404 。</p>
</li>
</ul>
<h3 id="实践-1"><a href="#实践-1" class="headerlink" title="实践"></a>实践</h3><p>将我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 通过 <code>Ingress</code> 的方式进行访问。</p>
<p>该示例项目的部署，不再进行赘述。可在 <a href="https://github.com/tao12345666333/saythx/blob/ingress/deploy/ingress.yaml">ingress 分支</a> 查看此处所需配置。</p>
<p>在我们将 NGINX Ingress Controller 及 SayThx 项目部署好之后，我们使用以下的配置创建 <code>Ingress</code> 资源。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: saythx-ing</span><br><span class="line">  namespace: work</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io&#x2F;ssl-redirect: &quot;false&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: saythx.moelove.info</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: saythx-frontend</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>创建</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f ingress.yaml         </span><br><span class="line">ingress.extensions&#x2F;saythx-ing created</span><br><span class="line">master $ kubectl -n work get ing</span><br><span class="line">NAME         HOSTS                 ADDRESS   PORTS     AGE</span><br><span class="line">saythx-ing   saythx.moelove.info             80        23s</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>验证</p>
<p>这里来解释下刚才的配置文件。首先，指定了 <code>host: saythx.moelove.info</code> 表示我们想要以 <code>saythx.moelove.info</code> 这个域名来访问它。<code>path</code> 直接写 <code>/</code> 表示所有的请求都转发至名为 <code>saythx-frontend</code> 的服务。</p>
<p>与我们平时使用 NGINX 基本一致。 现在编辑本地的 HOSTS 文件绑定 Node 的IP 与 <code>saythx.moelove.info</code> 这个域名。使用浏览器进行访问 <code>saythx.moelove.info:刚才 Controller 使用 NodePort 暴露服务时的端口</code>：</p>
</li>
</ul>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="586"></svg>)</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">可以看到已经成功访问。</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-20"><a href="#总结-20" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>Ingress</code> 的基本情况，了解了它是 K8S 中的一种资源对象，主要负责将集群外部流量与集群内服务的通信。但它的正常工作离不开 <code>Ingress Controller</code> ，当前官方团队维护的主要有两个 GLBC 和 NGINX Ingress Controller。</p>
<p>我们大致介绍了现有的 Controller 实现，也实践了如何部署 NGINX Ingress Controller 以及如何使用 Ingress 将我们的示例项目暴露至集群外。</p>
<p>NGINX Ingress Controller 的使用，比较符合我们平时使用 NGINX 的习惯，相对来说也比较灵活，后续可看实际情况再进行更多的实践。</p>
<p>至此，K8S 集群的管理，相关原理以及服务的部署等内容就基本介绍完毕。下节，我们将介绍生产实践中至关重要的一环 <strong>监控</strong> 相关的实践。</p>
<h1 id="监控实践：对-K8S-集群进行监控"><a href="#监控实践：对-K8S-集群进行监控" class="headerlink" title="监控实践：对 K8S 集群进行监控"></a>监控实践：对 K8S 集群进行监控</h1><h2 id="整体概览-15"><a href="#整体概览-15" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们对 K8S 有了一定的了解，也具备了一定的集群管理和排错能力。但如果要应用于生产环境中，不可能随时随地的都盯着集群，我们需要扩展我们对集群的感知能力。</p>
<p>本节，我们将介绍下 K8S 集群监控相关的内容。</p>
<h2 id="监控什么"><a href="#监控什么" class="headerlink" title="监控什么"></a>监控什么</h2><p>除去 K8S 外，我们平时自己开发的系统或者负责的项目，一般都是有监控的。监控可以提升我们的感知能力，便于我们及时了解集群的变化，以及知道哪里出现了问题。</p>
<p>K8S 是一个典型的分布式系统，组件很多，那么监控的目标，就变的很重要了。</p>
<p>总体来讲，对 K8S 集群的监控的话，主要有以下方面：</p>
<ul>
<li>节点情况</li>
<li>K8S 集群自身状态</li>
<li>部署在 K8S 内的应用的状态</li>
</ul>
<h2 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h2><p>对于 K8S 的监控，我们选择 CNCF 旗下次于 K8S 毕业的项目<a href="https://prometheus.io/" target="_blank" rel="noopener"> Prometheus </a>。</p>
<p>Prometheus 是一个非常灵活易于扩展的监控系统，它通过各种 <code>exporter</code> 暴露数据，并由 <code>prometheus server</code> 定时去拉数据，然后存储。</p>
<p>它自己提供了一个简单的前端界面，可在其中使用 <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank" rel="noopener">PromQL </a>的语法进行查询，并进行图形化展示。</p>
<h2 id="安装-Prometheus"><a href="#安装-Prometheus" class="headerlink" title="安装 Prometheus"></a>安装 Prometheus</h2><blockquote>
<p>这里推荐一个项目 <a href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a>, 尽管该项目还处于 Beta 阶段，但是它给在 K8S 中搭建基于 Prometheus 的监控提供了很大的便利。</p>
</blockquote>
<p>我们此处选择以一般的方式进行部署，带你了解其整体的过程。</p>
<ul>
<li><p>创建一个独立的 <code>Namespace</code>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: monitoring</span><br><span class="line"></span><br><span class="line"># 将文件保存为 namespace.yaml 的文件，并执行 kubectl apply -f namespace.yaml 即可，后面不再赘述。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;monitoring created</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>RBAC</p>
<p>我们的集群使用 <code>kubeadm</code> 创建，默认开启了 <code>RBAC</code>，所以现在需要创建相关的 Role 和 binding。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: prometheus</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: prometheus-k8s</span><br><span class="line">  namespace: monitoring</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources:</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes&#x2F;proxy</span><br><span class="line">  - services</span><br><span class="line">  - endpoints</span><br><span class="line">  - pods</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br><span class="line">- nonResourceURLs: [&quot;&#x2F;metrics&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-k8s</span><br><span class="line">  namespace: monitoring</span><br></pre></td></tr></table></figure></div>

<p>执行创建</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl  apply -f rbac.yaml</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;prometheus created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;prometheus created</span><br><span class="line">serviceaccount&#x2F;prometheus-k8s created</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>创建 Promethes 的配置文件</p>
<p>其中的内容主要参考 <a href="https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml">Prometheus 官方提供的示例</a> 和 <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config" target="_blank" rel="noopener">Prometheus 官方文档</a>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-core</span><br><span class="line">  namespace: monitoring</span><br><span class="line">data:</span><br><span class="line">  prometheus.yaml: |</span><br><span class="line">    global:</span><br><span class="line">      scrape_interval: 30s</span><br><span class="line">      scrape_timeout: 30s</span><br><span class="line">    scrape_configs:</span><br><span class="line">    - job_name: &#39;kubernetes-apiservers&#39;</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: endpoints</span><br><span class="line">      scheme: https</span><br><span class="line">    </span><br><span class="line">      tls_config:</span><br><span class="line">        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]</span><br><span class="line">        action: keep</span><br><span class="line">        regex: default;kubernetes;https</span><br><span class="line">    </span><br><span class="line">    # Scrape config for nodes (kubelet).</span><br><span class="line">    - job_name: &#39;kubernetes-nodes&#39;</span><br><span class="line">      scheme: https</span><br><span class="line">    </span><br><span class="line">      tls_config:</span><br><span class="line">        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: node</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_node_label_(.+)</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: kubernetes.default.svc:443</span><br><span class="line">      - source_labels: [__meta_kubernetes_node_name]</span><br><span class="line">        regex: (.+)</span><br><span class="line">        target_label: __metrics_path__</span><br><span class="line">        replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics</span><br><span class="line">    </span><br><span class="line">    # Scrape config for Kubelet cAdvisor.</span><br><span class="line">    - job_name: &#39;kubernetes-cadvisor&#39;</span><br><span class="line">    </span><br><span class="line">      scheme: https</span><br><span class="line">    </span><br><span class="line">      tls_config:</span><br><span class="line">        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: node</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_node_label_(.+)</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: kubernetes.default.svc:443</span><br><span class="line">      - source_labels: [__meta_kubernetes_node_name]</span><br><span class="line">        regex: (.+)</span><br><span class="line">        target_label: __metrics_path__</span><br><span class="line">        replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics&#x2F;cadvisor</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-service-endpoints&#39;</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: endpoints</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_service_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_service_name]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_name</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-services&#39;</span><br><span class="line">    </span><br><span class="line">      metrics_path: &#x2F;probe</span><br><span class="line">      params:</span><br><span class="line">        module: [http_2xx]</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: service</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - source_labels: [__address__]</span><br><span class="line">        target_label: __param_target</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: blackbox-exporter.example.com:9115</span><br><span class="line">      - source_labels: [__param_target]</span><br><span class="line">        target_label: instance</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_service_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_service_name]</span><br><span class="line">        target_label: kubernetes_name</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-ingresses&#39;</span><br><span class="line">    </span><br><span class="line">      metrics_path: &#x2F;probe</span><br><span class="line">      params:</span><br><span class="line">        module: [http_2xx]</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: ingress</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]</span><br><span class="line">        regex: (.+);(.+);(.+)</span><br><span class="line">        replacement: $&#123;1&#125;:&#x2F;&#x2F;$&#123;2&#125;$&#123;3&#125;</span><br><span class="line">        target_label: __param_target</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: blackbox-exporter.example.com:9115</span><br><span class="line">      - source_labels: [__param_target]</span><br><span class="line">        target_label: instance</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_ingress_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_ingress_name]</span><br><span class="line">        target_label: kubernetes_name</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-pods&#39;</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: pod</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_pod_name</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>部署 Prometheus</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-core</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: core</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: prometheus-main</span><br><span class="line">      labels:</span><br><span class="line">        app: prometheus</span><br><span class="line">        component: core</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: prometheus-k8s</span><br><span class="line">      containers:</span><br><span class="line">      - name: prometheus</span><br><span class="line">        image: taobeier&#x2F;prometheus:v2.6.0</span><br><span class="line">        args:</span><br><span class="line">          - &#39;--storage.tsdb.retention&#x3D;24h&#39;</span><br><span class="line">          - &#39;--storage.tsdb.path&#x3D;&#x2F;prometheus&#39;</span><br><span class="line">          - &#39;--config.file&#x3D;&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yaml&#39;</span><br><span class="line">        ports:</span><br><span class="line">        - name: webui</span><br><span class="line">          containerPort: 9090</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 500M</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 500M</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &#x2F;prometheus</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;prometheus</span><br><span class="line">      volumes:</span><br><span class="line">      - name: data</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      - name: config-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: prometheus-core</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看部署情况</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">master $ kubectl  -n monitoring get all</span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;prometheus-core-86b8455f76-mvrn4   1&#x2F;1       Running   0          12s</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;prometheus-core   1         1         1            1           12s</span><br><span class="line"></span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;prometheus-core-86b8455f76   1         1         1         12s</span><br></pre></td></tr></table></figure></div>

<p>Prometheus 的主体就已经部署完成。</p>
</li>
<li><p>使用 <code>Service</code> 将 <code>Promethes</code> 的服务暴露出来</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: core</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 9090</span><br><span class="line">    targetPort: 9090</span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: core</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>这里为了方便演示，直接使用了 <code>NodePort</code> 的方式暴露服务。当然你也可以参考上一节，使用 <code>Ingress</code> 的方式将服务暴露出来。</p>
</li>
<li><p>查询当前状态</p>
<p>我们使用 Promethes 自带的 PromQL 语法，查询在当前 <code>monitoring</code> Namespace 中 up 的任务。这里对查询的结果暂不进行展开。</p>
</li>
</ul>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1275" height="766"></svg>)</p>
<h2 id="安装-Node-exporter"><a href="#安装-Node-exporter" class="headerlink" title="安装 Node exporter"></a>安装 Node exporter</h2><p>我们刚才在介绍时，提到过 <code>Promethes</code> 支持多种 <code>exporter</code> 暴露指标。我们现在使用 <a href="https://github.com/prometheus/node_exporter">Node exporter</a> 完成对集群中机器的基础监控。</p>
<p>这里有一个需要考虑的点：</p>
<ul>
<li><p>使用什么方式部署 Node exporter ？</p>
<p>Node exporter 有已经编译好的二进制文件，可以很方便的进行部署。当我们要监控集群中所有的机器时，我们是该将它直接部署在机器上，还是部署在集群内？</p>
<p>我建议是直接部署在集群内，使用 <code>DaemonSet</code> 的方式进行部署。这里的考虑是当我们直接部署在宿主机上时，我们最起码需要保证两点：1. Promethes 服务可与它正常通信（Promethes 采用 Pull 的方式采集数据） ；2. 需要服务保活，如果 exporter 挂掉了，那自然就取不到数据。</p>
<p><code>DaemonSet</code> 是一种很合适的的部署方式，可直接将 Node exporter 部署至集群的每个节点上。</p>
</li>
<li><p>创建 <code>DaemonSet</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-node-exporter</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: node-exporter</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: prometheus-node-exporter</span><br><span class="line">      labels:</span><br><span class="line">        app: prometheus</span><br><span class="line">        component: node-exporter</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">      - image: taobeier&#x2F;node-exporter:v0.17.0</span><br><span class="line">        name: prometheus-node-exporter</span><br><span class="line">        ports:</span><br><span class="line">        - name: prom-node-exp</span><br><span class="line">          containerPort: 9100</span><br><span class="line">          hostPort: 9100</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      hostPID: true</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>让 Promethes 抓取数据</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    prometheus.io&#x2F;scrape: &#39;true&#39;</span><br><span class="line">  name: prometheus-node-exporter</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: node-exporter</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">    - name: prometheus-node-exporter</span><br><span class="line">      port: 9100</span><br><span class="line">      protocol: TCP</span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: node-exporter</span><br><span class="line">  type: ClusterIP</span><br></pre></td></tr></table></figure></div>

<p>这里我们直接使用了添加 <code>annotations</code> 的方式，让 Promethes 自动的通过 Kubernetes SD 发现我们新添加的 exporter （或者说资源）</p>
<p>我们访问 Promethes 的 web 端，进行验证。</p>
</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/24/167df6106b58b2fb?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="总结-21"><a href="#总结-21" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>Prometheus</code> 的基本情况，也部署了 <code>Prometheus</code> 的主体服务。</p>
<p>但这是结束么？这并不是，这才刚刚开始。</p>
<p>我们提到 <code>Prometheus</code> 支持多种 <code>exporter</code> 暴露各种指标，而且我们还可以使用 <a href="https://grafana.com/" target="_blank" rel="noopener">Grafana</a> 作为我们监控的展示手段。</p>
<p>如果要做 Dashboard 推荐使用 <a href="https://grafana.com/dashboards/162" target="_blank" rel="noopener">Kubernetes cluster monitoring (via Prometheus)</a> 。</p>
<p>另外，监控其实涉及的内容很多，包括数据持久化方式。以及是否考虑与集群外的 Prometheus 集群做邦联模式等。这里需要考虑的实际情况较多，暂不一一展开了。</p>
<p>Prometheus 已经从 CNCF 毕业，其在云原生时代下作为标准的监控技术栈也基本确立。至于应用监控，也可使用它的 SDK 来完成。</p>
<p>下节，我们将对本小册进行一次总结。</p>
<h1 id="总结-22"><a href="#总结-22" class="headerlink" title="总结"></a>总结</h1><h2 id="快速回顾"><a href="#快速回顾" class="headerlink" title="快速回顾"></a>快速回顾</h2><p>经过了前面 23 节的内容，我们从 K8S 的基础概念入手，通过其基础架构了解到了 K8S 中所涉及到的各类组件。</p>
<p>通过动手实践，使用 <code>minikube</code> 搭建了本地的集群，使用 <code>kubeadm</code> 完成了服务器上的集群搭建，对 K8S 的部署有了更加清晰的认识。</p>
<p>这里再推荐另一种正在快速迭代的方式 <a href="https://github.com/kubernetes-sigs/kind">Kubernetes In Docker</a> 可以很方便的创建廉价的 K8S 集群，目前至支持单节点集群，多节点支持正在开发中。</p>
<p>后面，我们通过学习 <code>kubectl</code> 的使用，部署了 Redis 服务，了解到了一个服务在 K8S 中部署的操作，以及如何将服务暴露至集群外，以便访问。</p>
<p>当集群真正要被使用之前，权限管控也愈发重要，我们通过学习 <code>RBAC</code> 的相关知识，学习到了如何在 K8S 集群中创建权限可控的用户，而这部分的内容在后续小节中也被频繁用到。</p>
<p>接下来，我们以我们实际的一个项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 为例，一步步的完成了项目的部署，在此过程中也学习到了配置文件的编写规范和要求。</p>
<p>当项目变大时，维护项目的更新也变成了一件很麻烦的事情。由此，我们引入了 <code>Helm</code> 作为我们的包管理软件，并使用它进行了项目的部署。</p>
<p>在此过程中也学习到了 Helm 的架构，以及如何编写一个 <code>Chart</code> 等知识。</p>
<p>前面我们主要集中于如何使用 K8S 上，接下了庖丁解牛系列便带我们一同深入至 K8S 内部，了解到了各基础组件的实际工作原理，也深入到了源码内部，了解其实现逻辑。</p>
<p>有这些理论知识作为基础，我们便可以大胆的将应用部署至 K8S 之上了。但实际环境可能多种多样，你可以会遇到各种各样的问题。</p>
<p>这里我们介绍了一些常见的 Troubleshoot 的方法，以便你在后续使用 K8S 的过程中遇到问题也可以快速的定位并解决问题。</p>
<p>此外，我们学习了 K8S 的一些扩展，比如 Dashboard 和 CoreDNS ， Dashboard 是一个比较直观的管理资源的方式，它也还在快速的发展和迭代中。</p>
<p>CoreDNS 在 K8S 1.13 中已经成为默认的 DNS 服务器，相信在不久之后， CoreDNS 也将会从 CNCF 毕业。</p>
<p>我们介绍了 <code>Ingress</code> 和在 K8S 中使用 <code>Promethes</code> 进行监控，不过监控涉及的方面很多，除了集群自身的监控外，应用层的监控也同样很重要。另外，监控和告警也是相辅相成的，在已有监控数据的前提下，如何更智能更优雅的告警也是我们需要考虑的点。否则，很容易造成告警风暴，有用的告警被忽略之类的。</p>
<h2 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h2><p>基于 K8S 的生态已经在逐步形成，只靠一本小册还远远不够，我们需要更多的对操作系统的了解，对 K8S 及其生态的了解。</p>
<p>以下推荐一些扩展阅读：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/container" target="_blank" rel="noopener">K8S 生态</a></li>
<li><a href="https://kubernetes.io/" target="_blank" rel="noopener">K8S 网站</a></li>
<li><a href="https://www.cncf.io/newsroom/blog/" target="_blank" rel="noopener">CNCF 博客</a></li>
<li><a href="https://github.com/kubernetes/">K8S 组织</a></li>
<li><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker 文档</a></li>
<li><a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="noopener">Promethes 文档</a></li>
<li><a href="https://grafana.com/" target="_blank" rel="noopener">Grafana 主页</a></li>
<li><a href="https://www.fluentd.org/" target="_blank" rel="noopener">Fluentd 主页</a></li>
</ul>
<h2 id="总结-23"><a href="#总结-23" class="headerlink" title="总结"></a>总结</h2><p>围绕 K8S 的云原生生态已经逐步形成，希望本小册能在你未来发展道路上起到一定的帮助。</p>
<p>K8S 涉及的知识面很广，小册中篇幅有限未能一一详解，欢迎大家共同讨探。</p>
]]></content>
      <categories>
        <category>青云</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title>categories</title>
    <url>/frank-wong.github.io/categories/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>tags</title>
    <url>/frank-wong.github.io/tags/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
