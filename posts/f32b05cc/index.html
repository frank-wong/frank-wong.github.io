<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/frank-wong.github.io/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/frank-wong.github.io/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/frank-wong.github.io/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/frank-wong.github.io/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/frank-wong.github.io/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/frank-wong.github.io/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/frank-wong.github.io/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="K8S," />










<meta name="description" content="开篇： Kubernetes 是什么以及为什么需要它Kubernetes 是一个可扩展的，用于容器化应用程序编排，管理的平台。由 Google 于 2014 年基于其大规模生产实践经验而开源出来的。Kubernetes 目前在容器编排领域已经成为事实上的标准，社区也非常活跃。 Kubernetes 在国内外都已经得到广泛的应用，无论是Google, Amazon, GitHub 等还是国内的阿里，">
<meta property="og:type" content="article">
<meta property="og:title" content="青云 Kubernetes">
<meta property="og:url" content="https://github.com/frank-wong/frank-wong.github.io/posts/f32b05cc/index.html">
<meta property="og:site_name" content="Frank.Wong">
<meta property="og:description" content="开篇： Kubernetes 是什么以及为什么需要它Kubernetes 是一个可扩展的，用于容器化应用程序编排，管理的平台。由 Google 于 2014 年基于其大规模生产实践经验而开源出来的。Kubernetes 目前在容器编排领域已经成为事实上的标准，社区也非常活跃。 Kubernetes 在国内外都已经得到广泛的应用，无论是Google, Amazon, GitHub 等还是国内的阿里，">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/1/29/16898b9e3d57fcab?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2018/9/16/165de25c7c517d78?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2018/11/18/16722d23f282fa8b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2018/12/23/167d6b6e66c60e89?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2018/12/23/167d6bafed569ab1?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2018/12/24/167df6106b58b2fb?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="article:published_time" content="2020-07-13T04:56:11.638Z">
<meta property="article:modified_time" content="2020-07-13T04:56:32.047Z">
<meta property="article:author" content="Frank.Wong">
<meta property="article:tag" content="K8S">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://user-gold-cdn.xitu.io/2019/1/29/16898b9e3d57fcab?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/frank-wong.github.io/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/frank-wong/frank-wong.github.io/posts/f32b05cc/"/>





  <title>青云 Kubernetes | Frank.Wong</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/frank-wong.github.io/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Frank.Wong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一万年太久，只争朝夕</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/frank-wong.github.io/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/frank-wong.github.io/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/frank-wong.github.io/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/frank-wong.github.io/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/frank-wong/frank-wong.github.io/frank-wong.github.io/posts/f32b05cc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frank.Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/frank-wong.github.io/images/jd.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Frank.Wong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">青云 Kubernetes</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-13T12:56:11+08:00">
                2020-07-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/frank-wong.github.io/categories/%E9%9D%92%E4%BA%91/" itemprop="url" rel="index">
                    <span itemprop="name">青云</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  56.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  254
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="开篇：-Kubernetes-是什么以及为什么需要它"><a href="#开篇：-Kubernetes-是什么以及为什么需要它" class="headerlink" title="开篇： Kubernetes 是什么以及为什么需要它"></a>开篇： Kubernetes 是什么以及为什么需要它</h1><p>Kubernetes 是一个可扩展的，用于容器化应用程序编排，管理的平台。由 Google 于 2014 年基于其大规模生产实践经验而开源出来的。Kubernetes 目前在容器编排领域已经成为事实上的标准，社区也非常活跃。</p>
<p>Kubernetes 在国内外都已经得到广泛的应用，无论是Google, Amazon, GitHub 等还是国内的阿里，腾讯，百度，华为，京东或其他中小公司等也都已全力推进 Kubernetes 在生产中的使用。</p>
<p>现在无论是运维，后端，DBA，亦或者是前端，机器学习工程师等都需要在工作中或多或少的用到 Docker， 而在生产中大量使用的话 Kubernetes 也将会成为趋势，所以了解或掌握 Kubernetes 也成为了工程师必不可少的技能之一。</p>
<h2 id="Kubernetes-是什么"><a href="#Kubernetes-是什么" class="headerlink" title="Kubernetes 是什么?"></a>Kubernetes 是什么?</h2><p>当提到 Kubernetes 的时候，大多数人的可能会想到它可以容器编排，想到它是 PaaS (Platform as a Service) 系统，但其实不然，Kubernetes 并不是 PasS 系统，因为它工作在容器层而不是硬件层，它只不过提供了一些与 PasS 类似或者共同的功能，类似部署，扩容，监控，负载均衡，日志记录等。然而它并不是个完全一体化的平台，这些功能基本都是可选可配置的。</p>
<p>Kubernetes 可支持公有云，私有云及混合云等，具备良好的可移植性。我们可直接使用它或在其之上构建自己的容器/云平台，以达到快速部署，快速扩展，及优化资源使用等。</p>
<p>它致力于提供通用接口类似 CNI( Container Network Interface ), CSI（Container Storage Interface）, CRI（Container Runtime Interface）等规范，以便有更多可能, 让更多的厂商共同加入其生态体系内。它的目标是希望在以后，任何团队都可以在不修改 Kubernetes 核心代码的前提下，可以方便的扩展和构建符合自己需求的平台。</p>
<h2 id="为什么需要-Kubernetes"><a href="#为什么需要-Kubernetes" class="headerlink" title="为什么需要 Kubernetes"></a>为什么需要 Kubernetes</h2><p>我们回到实际的工作环境中。</p>
<ul>
<li>如果你是个前端，你是否遇到过 npm 依赖安装极慢，或是 node sass 安装不了或者版本不对的情况？</li>
<li>如果你是个后端，是否遇到过服务器与本地环境不一致的情况，导致部分功能出现非预期的情况？</li>
<li>如果你是个运维，是否遇到过频繁部署环境，但中间可能出现各种安装不了或者版本不对的问题？</li>
</ul>
<p>目前来看，对于这些问题，最好的解决方案便是标准化，容器化，现在用到最多的也就是 Docker。 Docker 通过 Dockerfile 来对环境进行描述，通过镜像进行交付，使用时不再需要关注环境不一致相关的问题。</p>
<p>现在面试的时候，无论前后端，我们总会多问下是否了解或者使用过 Docker 。如果使用过，那自然会问如果规模变大或者在生产中如何进行容器编排，部署扩容机制如何。</p>
<p>多数人在这个时候都已经回答不上来了，一方面是因为非运维相关岗位的同学，可能在实际工作中并不了解整体的架构体系，没有相关的知识积累。另一方面，对于运维同学可能尚未接触到这部分。</p>
<p>作为一个技术人员，我们应该对整体的体系架构有所了解, 掌握更多的技能，了解软件的完整生命周期，包括开发，交付，部署，以及当流量变大时的扩容等。</p>
<p>在容器编排领域，比较著名的主要有三个：Kubernetes, Mesos, 及 Docker 自家的 Swarm . 对这三者而言，较为简单的是 Swarm, 因为它本身只专注于容器编排，并且是官方团队所作，从各方面来看，对于新手都相对友好一些。但如果是用于生产中大规模使用，反而就略有不及。</p>
<p>而 Mesos 也并不仅限于容器编排，它的创建本身是为了将数据中心的所有资源进行抽象，比如 CPU，内存，网络，存储等，将整个 Mesos 集群当作是一个大的资源池，允许各种 Framework 来进行调度。比如，可以使用 Marathon 来实现 PaaS，可以运行 Spark，Hadoop 等进行计算等。同时因为它支持比如 Docker 或者 LXC 等作资源隔离，所以前几年也被大量用于容器编排。</p>
<p>随着 Kubernetes 在目前的认可度已经超过 Mesos， Docker Swarm 等，无疑它是生产环境中容器应用管理的不二之选。</p>
<p>本小册的目标是帮助更多开发者（不局限于运维，后端，前端等）认识并掌握 Kubernetes 的基础技能，了解其基础架构等。但是 Kubernetes 涉及知识点很多，且更新迭代很快，本小册集中于使用轻快的文字帮助大家掌握 K8S 的通用基础技能，对于其中需掌握的关于 Docker，及 Linux 内核相关的知识不会过于深入解释。主要以最常见 case 入手，帮助大家更快的掌握相关知识并将其用于生产实践中。</p>
<h1 id="初步认识：Kubernetes-基础概念"><a href="#初步认识：Kubernetes-基础概念" class="headerlink" title="初步认识：Kubernetes 基础概念"></a>初步认识：Kubernetes 基础概念</h1><p>好了，总算开始进入正题，抛弃掉死板的说教模式，我们以一个虚构的新成立的项目组为例开始我们的 Kubernetes 探索。(以下统一将 Kubernetes 简写为 K8S) 项目组目前就只有一个成员，我们称他为小张。项目组刚成立的时候，小张也没想好，具体要做什么，但肯定要对外提供服务的，所以先向公司申请了一台服务器。</p>
<h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><p>这台服务器可以用来做什么呢？跑服务，跑数据库，跑测试之类的都可以，我们将它所做的事情统称为工作(work) 那么，它便是工作节点 (worker Node) 对应于 K8S 中，这就是我们首先要认识的 Node 。</p>
<p>Node 可以是一台物理机，也可以是虚拟机，对于我们此处的项目来讲，这台服务器便是 K8S 中的 Node 。</p>
<h3 id="Node-状态"><a href="#Node-状态" class="headerlink" title="Node 状态"></a>Node 状态</h3><p>当我们拿到这台服务器后，首先我们登录服务器查看下服务器的基本配置和信息。其实对于一个新加入 K8S 集群的 Node 也是一样，需要先检查它的状态，并将状态上报至集群的 master 。我们来看看服务器有哪些信息是我们所关心的。</p>
<h4 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h4><p>首先，我们所关心的是我们服务器的 IP 地址，包括内网 IP 和外网 IP。对应于 K8S 集群的话这个概念是类似的，内部 IP 可在 K8S 集群内访问，外部 IP 可在集群外访问。</p>
<p>其次，我们也会关心一下我们的主机名，比如在服务器上执行 <code>hostname</code> 命令，便可得到主机名。K8S 集群中，每个 Node 的主机名也会被记录下来。当然，我们可以通过给 Kubelet 传递一个 <code>--hostname-override</code> 的参数来覆盖默认的主机名。 (Kubelet 是什么，我们后面会解释)</p>
<h4 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h4><p>再之后，我们需要看下服务器的基本信息，比如看看系统版本信息， <code>cat /etc/issue</code> 或者 <code>cat /etc/os-release</code> 等方法均可查看。对于 K8S 集群会将每个 Node 的这些基础信息都记录下来。</p>
<h4 id="容量"><a href="#容量" class="headerlink" title="容量"></a>容量</h4><p>我们通常也都会关注下，我们有几个核心的 CPU ，可通过 <code>cat /proc/cpuinfo</code> 查看，有多大的内存 通过 <code>cat /proc/meminfo</code> 或 <code>free</code> 等查看。对于 K8S 集群，会默认统计这些信息，并计算在此 Node 上可调度的 Pod 数量。（Pod 后面做解释）</p>
<h4 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h4><p>对于我们拿到的服务器，我们关心上述的一些基本信息，并根据这些信息进行判断，这台机器是否能满足我们的需要。对 K8S 集群也同样，当判断上述信息均满足要求时候，便将集群内记录的该 Node 信息标记为 <code>Ready</code> （<code>Ready = True</code>），这样我们的服务器便正式的完成交付。我们来看下其他的部分。</p>
<h2 id="Deployment-和-Pod"><a href="#Deployment-和-Pod" class="headerlink" title="Deployment 和 Pod"></a>Deployment 和 Pod</h2><p>现在小张拿到的服务器已经是可用状态，虽然此时尚不知要具体做什么，但姑且先部署一个主页来宣布下项目组的成立。</p>
<p>我们来看下一般情况下的做法，先写一个静态页面，比如叫 index.html 然后在服务器上启动一个 Nginx 或者其他任何 Web 服务器，来提供对 index.html 的访问。</p>
<p>Nginx 的安装及配置可参考 <a href="https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open-source/" target="_blank" rel="noopener">Nginx 的官方文档</a>。最简单的配置大概类似下面这样（仅保留关键部分）：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location &#x2F; &#123;</span><br><span class="line">    root   &#x2F;www;</span><br><span class="line">    index  index.html;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>对于 K8S 而言，我们想要的，能提供对 index.html 访问的服务便可理解为 Deployment 的概念，表明一种我们预期的目标状态。</p>
<p>而对于 Nginx 和 index.html 这个组合可以理解为其中的 Pod 概念，作为最小的调度单元。</p>
<h2 id="Container-Runtime"><a href="#Container-Runtime" class="headerlink" title="Container Runtime"></a>Container Runtime</h2><p>虽然此刻部署的内容只有官网，但是为了避免单点故障，于是小张又申请了两台服务器（虽然看起来可能是浪费了点），现在要对原有的服务进行扩容，其实在新的服务器上我们所做的事情也还保持原样，部署 Nginx，提供对 index.html 的访问，甚至配置文件都完全是一样的。可以看到在这种情况下，增加一台服务器，我们需要做一件完全重复的事情。</p>
<p>本着不浪费时间做重复的工作的想法，小张想，要不然用 <a href="https://www.ansible.com/" target="_blank" rel="noopener">Ansible</a> 来统一管理服务器操作和配置吧，但考虑到后续服务器上还需要部署其他的服务，常规的这样部署，容易干扰彼此的环境。</p>
<p>所以我们想到了用虚拟化的技术，但是根据一般的经验，类似 <a href="https://www.linux-kvm.org/page/Main_Page" target="_blank" rel="noopener">KVM</a> 这样的虚拟化技术，可能在资源消耗上较多, 不够轻量级。而容器化相对来看，比较轻量级，也比较符合我们的预期，一次构建，随处执行。我们选择当前最热门的 Docker .</p>
<p>既然技术选型确定了，那很简单，在我们现在三台服务器上安装 Docker ，安装过程不再赘述，可以参考 <a href="https://docs.docker.com/install/linux/docker-ce/centos/" target="_blank" rel="noopener">Docker 的官方安装文档</a> 。</p>
<p>此时，我们需要做的事情，也便只是将我们的服务构建成一个镜像，需要编写一个 Dockerfile，构建一个镜像并部署到每台服务器上便可。</p>
<p>聊了这么多，我们现在已经将我们的服务运行到了容器中，而此处的 Docker 便是我们选择的容器运行时。选择它的最主要原因，便是为了环境隔离和避免重复工作。</p>
<p>而 Docker 如果对应于 K8S 集群中的概念，便是 Container Runtime，这里还有其他的选择，比如 rkt，runc 和其他实现了 OCI 规范的运行时。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这节里面，我们了解到了 Node 其实就是用于工作的服务器，它有一些状态和信息，当这些条件都满足一些条件判断时，Node 便处于 Ready 状态，可用于执行后续的工作。</p>
<p>Deployment 可理解为一种对期望状态的描述， Pod 作为集群中可调度的最小单元，我们会在后面详细讲解其细节。</p>
<p>Docker 是我们选择的容器运行时，可运行我们构建的服务镜像，减少在环境方面所做的重复工作，并且也非常便于部署。除了 Docker 外还存在其他的容器运行时。</p>
<p>了解到这些基本概念后，下节我们从宏观的角度上来认识 K8S 的整体架构，以便我们后续的学习和实践。</p>
<h1 id="宏观认识：整体架构"><a href="#宏观认识：整体架构" class="headerlink" title="宏观认识：整体架构"></a>宏观认识：整体架构</h1><p>工欲善其事，必先利其器。本节我们来从宏观上认识下 K8S 的整体架构，以便于后续在此基础上进行探索和实践。</p>
<h2 id="C-S-架构"><a href="#C-S-架构" class="headerlink" title="C/S 架构"></a>C/S 架构</h2><p>从更高层来看，K8S 整体上遵循 C/S 架构，从这个角度来看，可用下面的图来表示其结构：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">                               +-------------+                              </span><br><span class="line">                               |             |                              </span><br><span class="line">                               |             |               +---------------+</span><br><span class="line">                               |             |       +-----&gt; |     Node 1    |</span><br><span class="line">                               | Kubernetes  |       |       +---------------+</span><br><span class="line">+-----------------+            |   Server    |       |                      </span><br><span class="line">|       CLI       |            |             |       |       +---------------+</span><br><span class="line">|    (Kubectl)    |-----------&gt;| ( Master )  |&lt;------+-----&gt; |     Node 2    |</span><br><span class="line">|                 |            |             |       |       +---------------+</span><br><span class="line">+-----------------+            |             |       |       </span><br><span class="line">                               |             |       |       +---------------+</span><br><span class="line">                               |             |       +-----&gt; |     Node 3    |</span><br><span class="line">                               |             |               +---------------+</span><br><span class="line">                               +-------------+</span><br></pre></td></tr></table></figure></div>

<p>左侧是一个官方提供的名为 <code>kubectl</code> 的 CLI （Command Line Interface）工具，用于使用 K8S 开放的 API 来管理集群和操作对象等。</p>
<p>右侧则是 K8S 集群的后端服务及开放出的 API 等。根据上一节的内容，我们知道 Node 是用于工作的机器，而 Master 是一种角色（Role），表示在这个 Node 上包含着管理集群的一些必要组件。具体组件的详细介绍参考第 11 小节对各组件的详细剖析。</p>
<p>当然在这里，只画出了一个 Master，在生产环境中，为了保障集群的高可用，我们通常会部署多个 Master 。</p>
<h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2><p>下面我们来逐层分解， 首先是 Master ，这里我们只介绍其管理集群的相关组件。Master 是整个 K8S 集群的“大脑”，与大脑类似，它有几个重要的功能：</p>
<ul>
<li>接收：外部的请求和集群内部的通知反馈</li>
<li>发布：对集群整体的调度和管理</li>
<li>存储：存储</li>
</ul>
<p>这些功能，也通过一些组件来共同完成，通常情况下，我们将其称为 control plane 。如下图所示：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>它主要包含以下几个重要的组成部分。</p>
<h3 id="Cluster-state-store"><a href="#Cluster-state-store" class="headerlink" title="Cluster state store"></a>Cluster state store</h3><p>存储集群所有需持久化的状态，并且提供 watch 的功能支持，可以快速的通知各组件的变更等操作。</p>
<p>因为目前 Kubernetes 的存储层选择是 etcd ，所以一般情况下，大家都直接以 etcd 来代表集群状态存储服务。即：将所有状态存储到 etcd 实例中。</p>
<p>刚才我们说 Master 相当于是 K8S 集群的大脑，更细化来看，etcd 则是大脑中的核心，为什么这么说？可以参考后面详细剖析的章节，本章我们先从更高的层次来看集群的整体架构。</p>
<p>你可能会问， etcd 是必须的吗？就目前而言，etcd 是必须的，这主要是 Kubernetes 的内部实现。</p>
<p>而早在 2014 年左右，社区就一直在提议将存储层抽象出来，后端的实际存储作为一种插件化的存在。<a href="https://github.com/kubernetes/kubernetes/issues/1957">呼声</a>比较大的是另一种提供 k/v 存储功能的 <a href="https://www.consul.io/" target="_blank" rel="noopener">Consul</a> 。</p>
<p>不过得益于 etcd 的开发团队较为活跃，而且根据 K8S 社区的反馈做了相当大的改进，并且当时 K8S 团队主要的关注点也不在此，所以直到现在 etcd 仍不是一个可选项。</p>
<p>如果现在去看下 Kubernetes 的源代码，你会发现存储层的代码还比较简洁清晰，后续如果有精力也许将此处插件化也不是不可能。</p>
<h3 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h3><p>这是整个集群的入口，类似于人体的感官，接收外部的信号和请求，并将一些信息写入到 etcd 中。</p>
<p>实际处理逻辑比三次握手简单的多：</p>
<ul>
<li>请求 API Server ：“嗨，我有些东西要放到 etcd 里面”</li>
<li>API Server 收到请求：“你是谁？我为啥要听你的”</li>
<li>从请求中，拿出自己的身份凭证（一般是证书）：“是我啊，你的master，给我把这些东西放进去”</li>
<li>这时候就要看是些什么内容了，如果这些内容 API Server 能理解，那就放入 etcd 中 “好的 master 我放进去了”；如果不能理解，“抱歉 master 我理解不了”</li>
</ul>
<p>可以看到，它提供了认证相关的功能，用于判断是否有权限进行操作。当然 API Server 支持多种认证方法，不过一般情况下，我们都使用 x509 证书进行认证。</p>
<p>API Server 的目标是成为一个极简的 server，只提供 REST 操作，更新 etcd ，并充当着集群的网关。至于其他的业务逻辑之类的，通过插件或者在其他组件中完成。关于这部分的详细实现，可以参考后面的 API Server 剖析相关章节。</p>
<h3 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h3><p>Controller Manager 大概是 K8S 集群中最繁忙的部分，它在后台运行着许多不同的控制器进程，用来调节集群的状态。</p>
<p>当集群的配置发生变更，控制器就会朝着预期的状态开始工作。</p>
<h3 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h3><p>顾名思义，Scheduler 是集群的调度器，它会持续的关注集群中未被调度的 Pod ，并根据各种条件，比如资源的可用性，节点的亲和性或者其他的一些限制条件，通过绑定的 API 将 Pod 调度/绑定到 Node 上。</p>
<p>在这个过程中，调度程序一般只考虑调度开始时， Node 的状态，而不考虑在调度过程中 Node 的状态变化 (比如节点亲和性等，截至到目前 v1.11.2 也暂未加入相关功能的稳定特性)</p>
<h2 id="Node-1"><a href="#Node-1" class="headerlink" title="Node"></a>Node</h2><p>Node 的概念我们在上节已经提过了，这里不再过多赘述，简单点理解为加入集群中的机器即可。</p>
<p>那 Node 是如何加入集群接受调度，并运行服务的呢？这都要归功于运行在 Node 上的几个核心组件。我们先来看下整体结构：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">+--------------------------------------------------------+       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| |      kubelet        |        |     kube-proxy      | |       </span><br><span class="line">| |                     |        |                     | |       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">| | Container Runtime (Docker)                         | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| | |Pod                  |    |Pod                  | | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | | |C1   | |C2   |     |    ||C1   ||C2   ||C3   || | |       </span><br><span class="line">| | | |     | |     |     |    ||     ||     ||     || | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">+--------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<h3 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h3><p>Kubelet 实现了集群中最重要的关于 Node 和 Pod 的控制功能，如果没有 Kubelet 的存在，那 Kubernetes 很可能就只是一个纯粹的通过 API Server CRUD 的应用程序。</p>
<p>K8S 原生的执行模式是操作应用程序的容器，而不像传统模式那样，直接操作某个包或者是操作某个进程。基于这种模式，可以让应用程序之间相互隔离，互不影响。此外，由于是操作容器，所以应用程序可以说和主机也是相互隔离的，毕竟它不依赖于主机，在任何的容器运行时（比如 Docker）上都可以部署和运行。</p>
<p>我们在上节介绍过 Pod，Pod 可以是一组容器（也可以包含存储卷），K8S 将 Pod 作为可调度的基本单位， 分离开了构建时和部署时的关注点：</p>
<ul>
<li>构建时，重点关注某个容器是否能正确构建，如何快速构建</li>
<li>部署时，关心某个应用程序的服务是否可用，是否符合预期，依赖的相关资源是否都能访问到</li>
</ul>
<p>这种隔离的模式，可以很方便的将应用程序与底层的基础设施解耦，极大的提高集群扩/缩容，迁移的灵活性。</p>
<p>在前面，我们提到了 Master 节点的 <code>Scheduler</code> 组件，它会调度未绑定的 Pod 到符合条件的 Node 上，而至于最终该 Pod 是否能运行于 Node 上，则是由 <code>Kubelet</code> 来裁定的。关于 Kubelet 的具体原理，后面有详细剖析的章节。</p>
<h3 id="Container-runtime"><a href="#Container-runtime" class="headerlink" title="Container runtime"></a>Container runtime</h3><p>容器运行时最主要的功能是下载镜像和运行容器，我们最常见的实现可能是 <a href="https://www.docker.com/" target="_blank" rel="noopener">Docker</a> , 目前还有其他的一些实现，比如 <a href="https://github.com/rkt/rkt">rkt</a>, <a href="https://github.com/kubernetes-sigs/cri-o">cri-o</a>。</p>
<p>K8S 提供了一套通用的容器运行时接口 CRI (Container Runtime Interface), 凡是符合这套标准的容器运行时实现，均可在 K8S 上使用。</p>
<h3 id="Kube-Proxy"><a href="#Kube-Proxy" class="headerlink" title="Kube Proxy"></a>Kube Proxy</h3><p>我们都知道，想要访问某个服务，那要么通过域名，要么通过 IP。而每个 Pod 在创建后都会有一个虚拟 IP，K8S 中有一个抽象的概念，叫做 <code>Service</code> ，<code>kube-proxy</code> 便是提供一种代理的服务，让你可以通过 <code>Service</code> 访问到 Pod。</p>
<p>实际的工作原理是在每个 Node 上启动一个 <code>kube-proxy</code> 的进程，通过编排 <code>iptables</code> 规则来达到此效果。深入的解析，在后面有对应的章节。</p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>本节中，我们了解到了 K8S 的整体遵循 C/S 架构，集群的 Master 包含着几个重要的组成部分，比如 <code>API Server</code>, <code>Controller Manager</code> 等。</p>
<p>而 Node 上，则运行着三个必要的组件 <code>kubelet</code>, <code>container runtime</code> (一般是 Docker), <code>kube-proxy</code> 。</p>
<p>通过所有组件的分工协作，最终实现了 K8S 对容器的编排和调度。</p>
<p>完成了这节的学习，那我们就开始着手搭建一个属于我们自己的集群吧。</p>
<h1 id="搭建-Kubernetes-集群-本地快速搭建"><a href="#搭建-Kubernetes-集群-本地快速搭建" class="headerlink" title="搭建 Kubernetes 集群 - 本地快速搭建"></a>搭建 Kubernetes 集群 - 本地快速搭建</h1><p>通过之前的学习，我们已经知道了 K8S 中有一些组件是必须的，集群中有不同的角色。本节，我们在本地快速搭建一个集群，以加深我们学习到的东西。</p>
<h2 id="方案选择"><a href="#方案选择" class="headerlink" title="方案选择"></a>方案选择</h2><p>在上一节中，我们知道 K8S 中有多种功能组件，而这些组件要在本地全部搭建好，需要一些基础知识，以及在搭建过程中会浪费不少的时间，从而可能会影响我们正常的搭建集群的目标。</p>
<p>所以，我们这里提供两个最简单，最容易实现我们目标的工具</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kind/">KIND</a> 。</li>
<li><a href="https://github.com/kubernetes/minikube">Minikube</a> 。</li>
</ul>
<h2 id="KIND"><a href="#KIND" class="headerlink" title="KIND"></a>KIND</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>KIND（Kubernetes in Docker）是为了能提供更加简单，高效的方式来启动 K8S 集群，目前主要用于比如 <code>Kubernetes</code> 自身的 CI 环境中。</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ul>
<li>可以直接在项目的 <a href="https://github.com/kubernetes-sigs/kind/releases">Release 页面</a> 下载已经编译好的二进制文件。(下文中使用的是 v0.1.0 版本的二进制包)</li>
</ul>
<p>注意：如果不直接使用二进制包，而是使用 <code>go get sigs.k8s.io/kind</code> 的方式下载，则与下文中的配置文件不兼容。<strong>请参考<a href="https://zhuanlan.zhihu.com/p/60464867" target="_blank" rel="noopener">使用 Kind 搭建你的本地 Kubernetes 集群</a></strong> 这篇文章。</p>
<p>更新（2020年2月5日）：KIND 已经发布了 v0.7.0 版本，如果你想使用新版本，建议参考 <a href="https://zhuanlan.zhihu.com/p/105173589" target="_blank" rel="noopener">使用 Kind 在离线环境创建 K8S 集群</a> ，这篇文章使用了最新版本的 KIND。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/1/29/16898b9e3d57fcab?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><p><strong>在使用 KIND 之前，你需要本地先安装好 Docker 的环境</strong> ，此处暂不做展开。</p>
<p>由于网络问题，我们此处也需要写一个配置文件，以便让 <code>kind</code> 可以使用国内的镜像源。（KIND 最新版本中已经内置了所有需要的镜像，无需此操作）</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kind.sigs.k8s.io&#x2F;v1alpha1</span><br><span class="line">kind: Config</span><br><span class="line"></span><br><span class="line">kubeadmConfigPatches:</span><br><span class="line">- |</span><br><span class="line">  apiVersion: kubeadm.k8s.io&#x2F;v1alpha3</span><br><span class="line">  kind: InitConfiguration</span><br><span class="line">  nodeRegistration:</span><br><span class="line">  kubeletExtraArgs:</span><br><span class="line">    pod-infra-container-image: registry.aliyuncs.com&#x2F;google_containers&#x2F;pause-amd64:3.1</span><br><span class="line">- |</span><br><span class="line">  apiVersion: kubeadm.k8s.io&#x2F;v1alpha3</span><br><span class="line">  kind: ClusterConfiguration</span><br><span class="line">  imageRepository: registry.aliyuncs.com&#x2F;google_containers</span><br><span class="line">  kubernetesVersion: v1.12.2</span><br><span class="line">  networking:</span><br><span class="line">    serviceSubnet: 10.0.0.0&#x2F;16</span><br></pre></td></tr></table></figure></div>

<p>将上面的内容保存成 <code>kind-config.yaml</code> 文件，执行以下命令即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kind create cluster --image kindest&#x2F;node:v1.12.2 --config kind-config.yaml --name moelove</span><br></pre></td></tr></table></figure></div>

<p>下面为在我机器上执行的程序输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(MoeLove) ➜  kind ✗ kind create cluster --image kindest&#x2F;node:v1.12.2 --config kind-config.yaml --name moelove                  </span><br><span class="line">Creating cluster &#39;kind-moelove&#39; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.12.2) 🖼</span><br><span class="line"> ✓ [kind-moelove-control-plane] Creating node container 📦                                                         </span><br><span class="line"> ✓ [kind-moelove-control-plane] Fixing mounts 🗻</span><br><span class="line"> ✓ [kind-moelove-control-plane] Starting systemd 🖥</span><br><span class="line"> ✓ [kind-moelove-control-plane] Waiting for docker to be ready 🐋                                                  </span><br><span class="line"> ✓ [kind-moelove-control-plane] Starting Kubernetes (this may take a minute) ☸                                     </span><br><span class="line">Cluster creation complete. You can now use the cluster with:</span><br><span class="line"></span><br><span class="line">export KUBECONFIG&#x3D;&quot;$(kind get kubeconfig-path --name&#x3D;&quot;moelove&quot;)&quot;</span><br><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure></div>

<p>这里，通过传递上面的 <code>kind-config.yaml</code> 文件给 <code>kind create cluster</code>, 并传递了一个名字通过 <code>--name</code> 参数。</p>
<p>我们按照程序输出的提示进行操作：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export KUBECONFIG&#x3D;&quot;$(kind get kubeconfig-path --name&#x3D;&quot;moelove&quot;)&quot;</span><br><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure></div>

<p>下面为在我机器上执行的程序输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(MoeLove) ➜  kind ✗ export KUBECONFIG&#x3D;&quot;$(kind get kubeconfig-path --name&#x3D;&quot;moelove&quot;)&quot;</span><br><span class="line">(MoeLove) ➜  kind ✗ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;localhost:35911</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;localhost:35911&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br><span class="line">(MoeLove) ➜  kind ✗ kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;12&quot;, GitVersion:&quot;v1.12.2&quot;, GitCommit:&quot;17c77c7898218073f14c8d573582e8d2313dc740&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-10-24T06:43:59Z&quot;, GoVersion:&quot;go1.10.4&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>注意，这里需要安装 <code>kubectl</code>。 <code>kubectl</code> 的安装可参考下面的内容。</p>
<p>当你执行 <code>kubectl cluster-info</code> 如果可以看到类似我上面的输出，那你本地的 K8S 集群就已经部署好了。你可以直接阅读第 5 节或者第 6 节的内容。</p>
<p>如果你已经对 K8S 有所了解，并且对 Dashboard 有比较强烈需求的话, 可直接参考第 20 节的内容。</p>
<h2 id="Minikube"><a href="#Minikube" class="headerlink" title="Minikube"></a>Minikube</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>Minikube 是 K8S 官方为了开发者能在个人电脑上运行 K8S 而提供的一套工具。实现上是通过 Go 语言编写，通过调用虚拟化管理程序，创建出一个运行在虚拟机内的单节点集群。</p>
<p>注：从这里也可以看出，对于 K8S 集群的基本功能而言，节点数并没有什么限制。只有一个节点同样可以创建集群。</p>
<h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><ul>
<li>首先需要确认 BIOS 已经开启了 <code>VT-x</code> 或者 <code>AMD-v</code> 虚拟化的支持。具体方式可参考 <a href="https://www.shaileshjha.com/how-to-find-out-if-intel-vt-x-or-amd-v-virtualization-technology-is-supported-in-windows-10-windows-8-windows-vista-or-windows-7-machine/" target="_blank" rel="noopener">确认是否已开启 BIOS 虚拟化</a>, <a href="https://www.howtogeek.com/213795/how-to-enable-intel-vt-x-in-your-computers-bios-or-uefi-firmware/" target="_blank" rel="noopener">开启 BIOS 虚拟化支持</a> 这两篇文章。</li>
<li>其次我们需要安装一个虚拟化管理程序，这里的选择可根据你实际在用的操作系统来决定。官方推荐如下:<ul>
<li>macOS: <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://www.vmware.com/products/fusion" target="_blank" rel="noopener">VMware Fusion</a> 或 <a href="https://github.com/moby/hyperkit">HyperKit</a>。如果使用 Hyperkit 需要注意系统必须是 <code>OS X 10.10.3 Yosemite</code> 及之后版本的。</li>
<li>Linux: <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="http://www.linux-kvm.org/" target="_blank" rel="noopener">KVM</a>。</li>
<li>Windows: <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://msdn.microsoft.com/en-us/virtualization/hyperv_on_windows/quick_start/walkthrough_install" target="_blank" rel="noopener">Hyper-V</a>。</li>
</ul>
</li>
</ul>
<p>我个人推荐无论你在以上哪种操作系统中使用 Minikube 都选择用 <code>Virtualbox</code> 作为虚拟化管理程序，1. Virtualbox 无论操作体验还是安装都比较简单 2. Minikube 对其支持更完备，并且也已经经过大量用户测试，相关问题均已基本修复。</p>
<p><em>如果你是在 Linux 系统上面，其实还有一个选择，便是将 Minikube 的 <code>--vm-driver</code> 参数设置为 <code>none</code> ，并且在本机已经正确安装 Docker。 这种方式是无需虚拟化支持的。</em></p>
<h3 id="安装-kubectl"><a href="#安装-kubectl" class="headerlink" title="安装 kubectl"></a>安装 kubectl</h3><p>上一节我们已经学到 K8S 集群是典型的 C/S 架构，有一个官方提供的名为 <code>kubectl</code> 的 CLI 工具。在这里，我们要安装 <code>kubectl</code> 以便后续我们可以对搭建好的集群进行管理。</p>
<p><strong>注：由于 API 版本兼容的问题，尽量保持 <code>kubectl</code> 版本与 K8S 集群版本保持一致，或版本相差在在一个小版本内。</strong></p>
<p>官方文档提供了 <code>macOS</code>, <code>Linux</code>, <code>Windows</code> 等操作系统上的安装方式，且描述很详细，这里不过多赘述，<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl" target="_blank" rel="noopener">文档地址</a>。</p>
<p><strong>此处提供一个不同于官方文档中的安装方式。</strong></p>
<ul>
<li>访问 K8S 主仓库的 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md">CHANGELOG 文件</a> 找到你所需要的版本。 由于我们将要使用的 Minikube 是官方最新的稳定版 v0.28.2，而它内置的 Kubernetes 的版本是 v1.10 所以，我们选择使用对应的 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md">1.10 版本</a>的 <code>kubectl</code>。当然，我们也可以通过传递参数的方式来创建不同版本的集群。如 <code>minikube start --kubernetes-version v1.11.3</code> 用此命令创建 <code>v1.11.3</code> 版本的集群，当然 <code>kubectl</code> 的版本也需要相应升级。</li>
</ul>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="687" height="538"></svg>)</p>
<p>点击<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#client-binaries">Client Binaries</a> 找到你符合所需系统架构的对应包下载即可。这里我以 <a href="https://dl.k8s.io/v1.10.7/kubernetes-client-linux-amd64.tar.gz" target="_blank" rel="noopener">Linux 下 64 位的包</a>为例。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  wget https:&#x2F;&#x2F;dl.k8s.io&#x2F;v1.10.7&#x2F;kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">➜  echo &#39;169b57c6707ed8d8be9643b0088631e5c0c6a37a5e99205f03c1199cd32bc61e  kubernetes-client-linux-amd64.tar.gz&#39; | sha256sum -c -</span><br><span class="line">kubernetes-client-linux-amd64.tar.gz: 成功</span><br><span class="line">➜  tar zxf kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">➜  sudo mv kubernetes&#x2F;client&#x2F;bin&#x2F;kubectl &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubectl</span><br><span class="line">➜  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubectl version --client</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;10&quot;, GitVersion:&quot;v1.10.7&quot;, GitCommit:&quot;0c38c362511b20a098d7cd855f1314dad92c2780&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-08-20T10:09:03Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>执行以上命令即可完成 <code>kubectl</code> 的安装，最后一步会看到当前安装的版本信息等。</p>
<h3 id="安装-Minikube"><a href="#安装-Minikube" class="headerlink" title="安装 Minikube"></a>安装 Minikube</h3><p>先查看 Minikube 的 <a href="https://github.com/kubernetes/minikube/releases">Release 页面</a>，当前最新的稳定版本是 v0.28.2，找到你所需系统的版本，点击下载，并将下载好的可执行文件加入你的 PATH 中。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="948" height="637"></svg>)</p>
<p><strong>注：当前 Windows 系统下的安装包还处于实验性质，如果你是在 Windows 环境下，同样是可以下载安装使用的</strong></p>
<p>以 Linux 下的安装为例：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  wget -O minikube https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;minikube&#x2F;releases&#x2F;download&#x2F;v0.28.2&#x2F;minikube-linux-amd64</span><br><span class="line">➜  chmod +x minikube</span><br><span class="line">➜  sudo mv minikube &#x2F;usr&#x2F;local&#x2F;bin&#x2F;minikube</span><br><span class="line"></span><br><span class="line">➜  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;minikube version</span><br><span class="line">minikube version: v0.28.2</span><br></pre></td></tr></table></figure></div>

<p>最后一步可查看当前已安装好的 Minikube 的版本信息。如果安装成功将会看到和我上面内容相同的结果。</p>
<h3 id="创建第一个-K8S-集群"><a href="#创建第一个-K8S-集群" class="headerlink" title="创建第一个 K8S 集群"></a>创建第一个 K8S 集群</h3><p>使用 Minikube 创建集群，只要简单的执行 <code>minikube start</code> 即可。正常情况下，你会看到和我类似的输出。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ minikube start</span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br><span class="line"></span><br><span class="line">➜  ~ minikube status</span><br><span class="line">minikube: Running</span><br><span class="line">cluster: Running</span><br><span class="line">kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100</span><br></pre></td></tr></table></figure></div>

<p>为了验证我们的集群目前是否均已配置正确，可以执行以下命令查看。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl cluster-info </span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;192.168.99.100:8443</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;192.168.99.100:8443&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br><span class="line"></span><br><span class="line">➜  ~ kubectl get nodes</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    1d       v1.10.0</span><br></pre></td></tr></table></figure></div>

<p>如果出现类似拒绝连接之类的提示，那可能是因为你的 <code>kubectl</code> 配置不正确，可查看 <code>$HOME/.kube/config</code> 文件检查配置。示例输出如下：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat .kube&#x2F;config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;ca.crt</span><br><span class="line">    server: https:&#x2F;&#x2F;192.168.99.100:8443</span><br><span class="line">  name: minikube</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: minikube</span><br><span class="line">    user: minikube</span><br><span class="line">  name: minikube</span><br><span class="line">current-context: minikube</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: minikube</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.crt</span><br><span class="line">    client-key: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.key</span><br></pre></td></tr></table></figure></div>

<p>如果没有该文件，可按上面示例内容进行创建，替换掉其中的路径及 <code>server</code> 地址配置。 <code>server</code> 地址可通过 <code>minikube status</code> 或者 <code>minikube ip</code> 查看或检查。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(Tao) ➜  ~ minikube ip</span><br><span class="line">192.168.99.100</span><br><span class="line"></span><br><span class="line">(Tao) ➜  ~ minikube status</span><br><span class="line">minikube: Running</span><br><span class="line">cluster: Running</span><br><span class="line">kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100</span><br></pre></td></tr></table></figure></div>

<h3 id="通过-Dashboard-查看集群当前状态"><a href="#通过-Dashboard-查看集群当前状态" class="headerlink" title="通过 Dashboard 查看集群当前状态"></a>通过 Dashboard 查看集群当前状态</h3><p>使用 <code>Minikube</code> 的另一个好处在于，你可以不用关注太多安装方面的过程，直接在终端下输入 <code>minikube dashboard</code> 打开系统 Dashboard 并通过此来查看集群相关状态。</p>
<p>执行 <code>minikube dashboard</code> 后会自动打开浏览器，默认情况下监听在通过 <code>minikube ip</code> 所获得 IP 的 3000 端口。如下图所示：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/9/16/165de25c7c517d78?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接:"></a>相关链接:</h3><ul>
<li><a href="https://websiteforstudents.com/installing-virtualbox-ubuntu-17-04/" target="_blank" rel="noopener">安装 VirtualBox</a></li>
<li><a href="https://juejin.im/post/5c99ed6c6fb9a0710e47ebeb" target="_blank" rel="noopener">使用 Kind 搭建你的本地 Kubernetes 集群</a></li>
</ul>
<h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>本节中，我们为了能更快的体验到 K8S 集群，避免很多繁琐的安装步骤，我们选择了使用官方提供的 <code>Minikube</code> 工具来搭建一个本地集群。</p>
<p>Minikube 的本质其实是将一套 “定制化” 的 K8S 集群打包成 ISO 镜像，当执行 <code>minikube start</code> 的时候，便通过此镜像启动一个虚拟机，在此虚拟机上通过 <code>kubeadm</code> 工具来搭建一套只有一个节点的 K8S 集群。关于 <code>kubeadm</code> 工具，我们在下节进行讲解。</p>
<p>同时，会通过虚拟机的相关配置接口拿到刚才所启动虚拟机的地址信息等，并完成本地的 <code>kubectl</code> 工具的配置，以便于让用户通过 <code>kubectl</code> 工具对集群进行操作。</p>
<p>事实上，当前 Docker for Mac 17.12 CE Edge 和 Docker for Windows 18.02 CE Edge ，以及这两种平台更高的 Edge 版本, 均已内置了对 K8S 的支持，但均为 Edge 版本，此处暂不做过多介绍。</p>
<h1 id="动手实践：搭建一个-Kubernetes-集群-生产可用"><a href="#动手实践：搭建一个-Kubernetes-集群-生产可用" class="headerlink" title="动手实践：搭建一个 Kubernetes 集群 - 生产可用"></a>动手实践：搭建一个 Kubernetes 集群 - 生产可用</h1><p>通过上一节的学习，我们快速的使用 <code>Minikube</code> 搭建了一个本地可用的 K8S 集群。默认情况下，节点是一个虚拟机实例，我们可以在上面体验一些基本的功能。</p>
<p>大多数人的需求并不只是包含一个虚拟机节点的本地测试集群，而是一个可在服务器运行，可自行扩/缩容，具备全部功能的，达到生产可用的集群。</p>
<p>K8S 集群的搭建，一直让很多人头疼，本节我们来搭建一个生产可用的集群，便于后续的学习或使用。</p>
<h2 id="方案选择-1"><a href="#方案选择-1" class="headerlink" title="方案选择"></a>方案选择</h2><p>K8S 生产环境可用的集群方案有很多，本节我们选择一个 Kubernetes 官方推荐的方案 <code>kubeadm</code> 进行搭建。</p>
<p><code>kubeadm</code> 是 Kubernetes 官方提供的一个 CLI 工具，可以很方便的搭建一套符合官方最佳实践的最小化可用集群。当我们使用 <code>kubeadm</code> 搭建集群时，集群可以通过 K8S 的一致性测试，并且 <code>kubeadm</code> 还支持其他的集群生命周期功能，比如升级/降级等。</p>
<p>我们在此处选择 <code>kubeadm</code> ，因为我们可以不用过于关注集群的内部细节，便可以快速的搭建出生产可用的集群。我们可以通过后续章节的学习，快速上手 K8S ，并学习到 K8S 的内部原理。在此基础上，想要在物理机上完全一步步搭建集群，便轻而易举。</p>
<h2 id="安装基础组件"><a href="#安装基础组件" class="headerlink" title="安装基础组件"></a>安装基础组件</h2><h3 id="前期准备-1"><a href="#前期准备-1" class="headerlink" title="前期准备"></a>前期准备</h3><p>使用 <code>kubeadm</code> 前，我们需要提前做一些准备。</p>
<ul>
<li><p><strong>我们需要禁用 <code>swap</code></strong>。通过之前的学习，我们知道每个节点上都有个必须的组件，名为 <code>kubelet</code>，自 K8S 1.8 开始，启动 <code>kubelet</code> 时，需要禁用 <code>swap</code> 。或者需要更改 <code>kubelet</code> 的启动参数 <code>--fail-swap-on=false</code>。</p>
<p>虽说可以更改参数让其可用，但是我建议还是禁用 <code>swap</code> 除非你的集群有特殊的需求，比如：有大内存使用的需求，但又想节约成本；或者你知道你将要做什么，否则可能会出现一些非预期的情况，尤其是做了内存限制的时候，当某个 Pod 达到内存限制的时候，它可能会溢出到 swap 中，这会导致 K8S 无法正常进行调度。</p>
<p>如何禁用：</p>
<ol>
<li>使用 <code>sudo cat /proc/swaps</code> 验证 swap 配置的设备和文件。</li>
<li>通过 <code>swapoff -a</code> 关闭 swap 。</li>
<li>使用 <code>sudo blkid</code> 或者 <code>sudo lsblk</code> 可查看到我们的设备属性，请注意输出结果中带有 <code>swap</code> 字样的信息。</li>
<li>将 <code>/etc/fstab</code> 中和上一条命令中输出的，和 swap 相关的挂载点都删掉，以免在机器重启或重挂载时，再挂载 <code>swap</code> 分区。</li>
</ol>
<p>执行完上述操作，<code>swap</code> 便会被禁用，当然你也可以再次通过上述命令，或者 <code>free</code> 命令来确认是否还有 <code>swap</code> 存在。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# free </span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:        1882748       85608     1614836       16808      182304     1630476</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>通过 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 可查看机器的 <code>product_uuid</code> 请确保要搭建集群的所有节点的 <code>product_uuid</code> 均不相同。同时所有节点的 Mac 地址也不能相同，通过 <code>ip a</code> 或者 <code>ifconfig -a</code> 可进行查看。</p>
<p>我们在第二章提到过，每个 Node 都有一些信息会被记录进集群内，而此处我们需要保证的这些唯一的信息，便会记录在集群的 <code>nodeInfo</code> 中，比如 <code>product_uuid</code> 在集群内以 <code>systemUUID</code> 来表示。具体信息我们可以通过集群的 <code>API Server</code> 获取到，在后面的章节会详细讲述。</p>
</li>
<li><p>第三章中，我们已经谈过 K8S 是 C/S 架构，在启动后，会固定监听一些端口用于提供服务。可以通过 <code>sudo netstat -ntlp |grep -E &#39;6443|23[79,80]|1025[0,1,2]&#39;</code> 查看这些端口是否被占用，如果被占用，请手动释放。</p>
<p>如果你执行上述命令时，提示 <code>command not found</code>，则表明你需要先安装 <code>netstat</code>，在 CentOS 系统中需要通过 <code>sudo yum install net-tools</code> 安装，而在 Debian/Ubuntu 系统中，则需要通过 <code>sudo apt install net-tools</code> 进行安装。</p>
</li>
<li><p>前面我们也提到了，我们需要一个容器运行时，通常情况下是 <code>Docker</code>，我们可以通过<a href="https://docs.docker.com/install/overview/" target="_blank" rel="noopener">官方的 Docker 文档</a> 进行安装，安装完成后记得启动服务。</p>
<p>官方推荐使用 <code>17.03</code> ，但我建议你可以直接安装 <code>18.03</code> 或者更新的版本，因为 <code>17.03</code> 版本的 Docker 已经在 2018 年 3 月 <code>EOL</code>（End Of Life）了。对于更新版本的 Docker，虽然 K8S 尚未在新版本中经过大量测试，但毕竟新版本有很多 Bugfix 和新特性的增加，也能规避一些可能遇到的问题（比如个别情况下 container 不会自动删除的情况 (17.09) ）。</p>
<p>另外，由于 Docker 的 API 都是带有版本的，且有良好的兼容性，当使用低版本 API 请求时会自动降级，所以一般情况下也不会有什么问题。</p>
</li>
</ul>
<h3 id="安装-kubectl-1"><a href="#安装-kubectl-1" class="headerlink" title="安装 kubectl"></a>安装 kubectl</h3><p>第三章中，我们已经说过 <code>kubectl</code> 是集群的客户端，我们现在搭建集群时，也必须要安装它，用于验证集群功能。</p>
<p>安装步骤在第 4 章已经详细说明了，此处不做赘述，可查阅第 4 章或参考下面的内容。</p>
<h3 id="安装-kubeadm-和-kubelet"><a href="#安装-kubeadm-和-kubelet" class="headerlink" title="安装 kubeadm 和 kubelet"></a>安装 kubeadm 和 kubelet</h3><p>首先是版本的选择，我们可以通过下面的命令获取到当前的 stable 版本号。要访问这个地址，需要自行处理网络问题或使用我后面提供的解决办法。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# curl -sSL https:&#x2F;&#x2F;dl.k8s.io&#x2F;release&#x2F;stable.txt</span><br><span class="line">v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>下载二进制包，并通过 <code>kubeadm version</code> 验证版本是否正确。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# curl -sSL https:&#x2F;&#x2F;dl.k8s.io&#x2F;release&#x2F;v1.11.3&#x2F;bin&#x2F;linux&#x2F;amd64&#x2F;kubeadm &gt; &#x2F;usr&#x2F;bin&#x2F;kubeadm</span><br><span class="line">[root@master ~]# chmod a+rx &#x2F;usr&#x2F;bin&#x2F;kubeadm</span><br><span class="line">[root@master ~]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:59:42Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>当然，我们其实可以使用如同上一章的方式，直接进入到 <code>kubernetes</code> 的<a href="https://github.com/kubernetes/kubernetes">官方仓库</a>，找到我们所需版本 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#v1113">v1.11.3</a> 下载 <code>Server Binaries</code>，如下图：</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1019" height="670"></svg>)</p>
<p>终端下可使用如下方式下载：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master tmp]# wget -q https:&#x2F;&#x2F;dl.k8s.io&#x2F;v1.11.3&#x2F;kubernetes-server-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure></div>

<p><strong>对于国内用户，我已经准备了下面的方式，方便使用。</strong></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1FSEcEUplQQGsjyBIZ6j2fA 提取码: cu4s</span><br></pre></td></tr></table></figure></div>

<p>下载完成后，验证文件是否正确无误，验证通过后进行解压。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master tmp]# echo &#39;e49d0db1791555d73add107d2110d54487df538b35b9dde0c5590ac4c5e9e039 kubernetes-server-linux-amd64.tar.gz&#39; | sha256sum -c -</span><br><span class="line">kubernetes-server-linux-amd64.tar.gz: 确定</span><br><span class="line">[root@master tmp]# tar -zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">[root@master tmp]# ls kubernetes</span><br><span class="line">addons  kubernetes-src.tar.gz  LICENSES  server</span><br><span class="line">[root@master tmp]# ls kubernetes&#x2F;server&#x2F;bin&#x2F; | grep -E &#39;kubeadm|kubelet|kubectl&#39;</span><br><span class="line">kubeadm</span><br><span class="line">kubectl</span><br><span class="line">kubelet</span><br></pre></td></tr></table></figure></div>

<p>可以看到在 <code>server/bin/</code> 目录下有我们所需要的全部内容，将我们所需要的 <code>kubeadm</code> <code>kubectl</code> <code>kubelet</code> 等都移动至 <code>/usr/bin</code> 目录下。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master tmp]# mv kubernetes&#x2F;server&#x2F;bin&#x2F;kube&#123;adm,ctl,let&#125; &#x2F;usr&#x2F;bin&#x2F;</span><br><span class="line">[root@master tmp]# ls &#x2F;usr&#x2F;bin&#x2F;kube*</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;kubeadm  &#x2F;usr&#x2F;bin&#x2F;kubectl  &#x2F;usr&#x2F;bin&#x2F;kubelet</span><br><span class="line">[root@master tmp]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:59:42Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">[root@master tmp]# kubectl version --client</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">[root@master tmp]# kubelet --version</span><br><span class="line">Kubernetes v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到我们所需的组件，版本均为 <code>v1.11.3</code> 。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>为了在生产环境中保障各组件的稳定运行，同时也为了便于管理，我们增加对 <code>kubelet</code> 的 <code>systemd</code> 的配置，由 <code>systemd</code> 对服务进行管理。</p>
<h3 id="配置-kubelet"><a href="#配置-kubelet" class="headerlink" title="配置 kubelet"></a>配置 kubelet</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@master tmp]# cat &lt;&lt;&#39;EOF&#39; &gt; &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;kubelet: The Kubernetes Agent</span><br><span class="line">Documentation&#x3D;http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;kubelet</span><br><span class="line">Restart&#x3D;always</span><br><span class="line">StartLimitInterval&#x3D;0</span><br><span class="line">RestartSec&#x3D;10</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br><span class="line">EOF</span><br><span class="line">[root@master tmp]# mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d</span><br><span class="line">[root@master tmp]# cat &lt;&lt;&#39;EOF&#39; &gt; &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment&#x3D;&quot;KUBELET_KUBECONFIG_ARGS&#x3D;--bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf&quot;</span><br><span class="line">Environment&#x3D;&quot;KUBELET_CONFIG_ARGS&#x3D;--config&#x3D;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">EnvironmentFile&#x3D;-&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env</span><br><span class="line">EnvironmentFile&#x3D;-&#x2F;etc&#x2F;default&#x2F;kubelet</span><br><span class="line">ExecStart&#x3D;</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS</span><br><span class="line">EOF</span><br><span class="line">[root@master tmp]# systemctl enable kubelet</span><br><span class="line">Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;kubelet.service to &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.</span><br></pre></td></tr></table></figure></div>

<p>在这里我们添加了 <code>kubelet</code> 的 systemd 配置，然后添加了它的 <code>Drop-in</code> 文件，我们增加的这个 <code>kubeadm.conf</code> 文件，会被 systemd 自动解析，用于复写 <code>kubelet</code> 的基础 systemd 配置，可以看到我们增加了一系列的配置参数。在第 17 章中，我们会对 <code>kubelet</code> 做详细剖析，到时再进行解释。</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>此时，我们的前期准备已经基本完成，可以使用 <code>kubeadm</code> 来创建集群了。别着急，在此之前，我们还需要安装两个工具，名为<code>crictl</code> 和 <code>socat</code>。</p>
<h3 id="安装前置依赖-crictl"><a href="#安装前置依赖-crictl" class="headerlink" title="安装前置依赖 crictl"></a>安装前置依赖 crictl</h3><p><code>crictl</code> 包含在 <a href="https://github.com/kubernetes-sigs/cri-tools.git"><code>cri-tools</code></a> 项目中，这个项目中包含两个工具：</p>
<ul>
<li><code>crictl</code> 是 <code>kubelet</code> CRI (Container Runtime Interface) 的 CLI 。</li>
<li><code>critest</code> 是 <code>kubelet</code> CRI 的测试工具集。</li>
</ul>
<p>安装可以通过进入 <code>cri-tools</code> 项目的 <a href="https://github.com/kubernetes-sigs/cri-tools/releases">Release 页面</a> ，根据项目 <a href="https://github.com/kubernetes-sigs/cri-tools#current-status">README</a> 文件中的版本兼容关系，选择自己所需的安装包，下载即可，由于我们安装 K8S 1.11.3 所以选择最新的 v1.11.x 的安装包。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="800" height="600"></svg>)</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# wget https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;cri-tools&#x2F;releases&#x2F;download&#x2F;v1.11.1&#x2F;crictl-v1.11.1-linux-amd64.tar.gz</span><br><span class="line">[root@master ~]# echo &#39;ccf83574556793ceb01717dc91c66b70f183c60c2bbec70283939aae8fdef768 crictl-v1.11.1-linux-amd64.tar.gz&#39; | sha256sum -c -</span><br><span class="line">crictl-v1.11.1-linux-amd64.tar.gz: 确定</span><br><span class="line">[root@master ~]# tar zxvf crictl-v1.11.1-linux-amd64.tar.gz</span><br><span class="line">[root@master ~]# mv crictl &#x2F;usr&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure></div>

<h3 id="安装前置依赖-socat"><a href="#安装前置依赖-socat" class="headerlink" title="安装前置依赖 socat"></a>安装前置依赖 socat</h3><p><code>socat</code> 是一款很强大的命令行工具，可以建立两个双向字节流并在其中传输数据。这么说你也许不太理解，简单点说，它其中的一个功能是可以实现端口转发。</p>
<p>无论在 K8S 中，还是在 Docker 中，如果我们需要在外部访问服务，端口转发是个必不可少的部分。当然，你可能会问基本上没有任何地方提到说 <code>socat</code> 是一个依赖项啊，别急，我们来看下<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockershim/docker_streaming.go#L189-L192"> K8S 的源码</a>便知道了。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">func portForward(client libdocker.Interface, podSandboxID string, port int32, stream io.ReadWriteCloser) error &#123;</span><br><span class="line">    &#x2F;&#x2F; 省略了和 socat 无关的代码</span><br><span class="line"></span><br><span class="line">	socatPath, lookupErr :&#x3D; exec.LookPath(&quot;socat&quot;)</span><br><span class="line">	if lookupErr !&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;unable to do port forwarding: socat not found.&quot;)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	args :&#x3D; []string&#123;&quot;-t&quot;, fmt.Sprintf(&quot;%d&quot;, containerPid), &quot;-n&quot;, socatPath, &quot;-&quot;, fmt.Sprintf(&quot;TCP4:localhost:%d&quot;, port)&#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><code>socat</code> 的安装很简单 CentOS 下执行 <code>sudo yum install -y socat</code> ，Debian/Ubuntu 下执行 <code>sudo apt-get install -y socat</code> 即可完成安装。</p>
<h3 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h3><p>所有的准备工作已经完成，我们开始真正创建一个 K8S 集群。 <strong>注意：如果需要配置 Pod 网络方案，请先阅读本章最后的部分 <a href="https://juejin.im/book/5b9b2dc86fb9a05d0f16c8ac/section/5b9b8346f265da0af03375ed#配置集群网络" target="_blank" rel="noopener">配置集群网络</a></strong></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubeadm init                                                                                             </span><br><span class="line">[init] using Kubernetes version: v1.11.3               </span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">...</span><br><span class="line">I0920 01:09:09.602908   17966 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0920 01:09:09.603001   17966 kernel_validator.go:96] Validating kernel config</span><br><span class="line">        [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03</span><br><span class="line">[preflight&#x2F;images] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight&#x2F;images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight&#x2F;images] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">...</span><br><span class="line">[markmaster] Marking the node master as master by adding the label &quot;node-role.kubernetes.io&#x2F;master&#x3D;&#39;&#39;&quot;</span><br><span class="line">[markmaster] Marking the node master as master by adding the taints [node-role.kubernetes.io&#x2F;master:NoSchedule]</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 202.182.112.120:6443 --token t14kzc.vjurhx5k98dpzqdc --discovery-token-ca-cert-hash sha256:d64f7ce1af9f9c0c73d2d737fd0095456ad98a2816cb5527d55f984c8aa8a762</span><br></pre></td></tr></table></figure></div>

<p>以上省略了部分输出。</p>
<p>我们从以上日志中可以看到，创建集群时会检查内核版本，Docker 版本等信息，这里提示 Docker 版本较高，我们忽略这个提示。</p>
<p>然后会下载一些镜像，当然这里提示我们可以通过执行 <code>kubeadm config images pull</code> 提前去下载镜像。我们来看下</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubeadm config images pull</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-apiserver-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-controller-manager-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-scheduler-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;kube-proxy-amd64:v1.11.3</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;pause:3.1</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;etcd-amd64:3.2.18</span><br><span class="line">[config&#x2F;images] Pulled k8s.gcr.io&#x2F;coredns:1.1.3</span><br></pre></td></tr></table></figure></div>

<p>对于国内用户使用 <code>kubeadm</code> 创建集群时，可能遇到的问题便是这些镜像下载不下来，最终导致创建失败。所以我在国内的代码托管平台提供了一个<a href="https://gitee.com/K8S-release/kubeadm" target="_blank" rel="noopener">仓库</a> 可以 clone 该项目，进入 <code>v1.11.3</code> 目录，对每个 <code>tar</code> 文件执行 <code>sudo docker load -i xx.tar</code> 即可将镜像导入。</p>
<p>或者可使用<a href="https://dev.aliyun.com/list.html?namePrefix=google-containers" target="_blank" rel="noopener">阿里云提供的镜像</a>，只需要将 <code>k8s.gcr.io</code> 替换为 <code>registry.aliyuncs.com/google_containers</code> ，执行 <code>docker pull</code> 后再 <code>docker tag</code> 重 tag 即可。</p>
<p>继续看上面的日志，<code>kubeadm init</code> 执行起见生成了一些文件，而这些文件我们先前在 kubelet server 的 <code>Drop-in</code> 的配置中配置过。</p>
<p>生成这些配置文件后，将启动 <code>kubelet</code> 服务，生成一系列的证书和相关的配置之类的，并增加一些扩展。</p>
<p>最终集群创建成功，并提示可在任意机器上使用指定命令加入集群。</p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>在上面的步骤中，我们已经安装了 K8S 的 CLI 工具 <code>kubectl</code>，我们使用此工具查看集群信息：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl cluster-info</span><br><span class="line">Kubernetes master is running at http:&#x2F;&#x2F;localhost:8080</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure></div>

<p>使用 <code>kubectl cluster-info</code> 可查看集群 master 和集群服务的地址，但我们也注意到最后有一句报错 <code>connection ... refused</code> 很显然这里存在错误。</p>
<p><code>kubectl get nodes</code> 可查看集群中 <code>Node</code> 信息，同样报错。</p>
<p>在上面我们提到过，K8S 默认会监听一些端口，但并不是 <code>8080</code> 端口，由此可知，我们的 <code>kubectl</code> 配置有误。</p>
<h3 id="配置-kubectl"><a href="#配置-kubectl" class="headerlink" title="配置 kubectl"></a>配置 kubectl</h3><ul>
<li><p>使用 <code>kubectl</code> 的参数 <code>--kubeconfig</code> 或者环境变量 <code>KUBECONFIG</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl --kubeconfig &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf get nodes                                                </span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    13h       v1.11.3</span><br><span class="line">[root@master ~]#</span><br><span class="line">[root@master ~]# KUBECONFIG&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf kubectl get nodes                                                  </span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    13h       v1.11.3</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>使用传参的方式未免太繁琐，我们也可以更改默认配置文件</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# mkdir -p $HOME&#x2F;.kube</span><br><span class="line">[root@master ~]# sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@master ~]# sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@master ~]# kubectl get nodes                                                  </span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    13h       v1.11.3</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<h3 id="配置集群网络"><a href="#配置集群网络" class="headerlink" title="配置集群网络"></a>配置集群网络</h3><p>通过上面的配置，我们已经可以正常获取 <code>Node</code> 信息。但通过第 2 章，我们了解到 <code>Node</code> 都有 <code>status</code>，而此时我们唯一的 <code>Node</code> 是 <code>NotReady</code>。我们通过给 <code>kubectl</code> 传递 <code>-o</code> 参数更改输出格式，查看更详细的情况。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes -o yaml</span><br><span class="line">apiVersion: v1       </span><br><span class="line">items:                                       </span><br><span class="line">- apiVersion: v1                              </span><br><span class="line">  kind: Node</span><br><span class="line">  ...</span><br><span class="line">  status:</span><br><span class="line">    addresses:</span><br><span class="line">    - address: master</span><br><span class="line">      type: Hostname</span><br><span class="line">    ...</span><br><span class="line">    - lastHeartbeatTime: 2018-09-20T14:45:45Z</span><br><span class="line">      lastTransitionTime: 2018-09-20T01:09:48Z</span><br><span class="line">      message: &#39;runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady</span><br><span class="line">        message:docker: network plugin is not ready: cni config uninitialized&#39;</span><br><span class="line">      reason: KubeletNotReady</span><br><span class="line">      status: &quot;False&quot;</span><br><span class="line">      type: Ready</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></div>

<p>从以上输出中，我们可以看到 master 处于 <code>NotReady</code> 的原因是 <code>network plugin is not ready: cni config uninitialized</code> 那么 <code>CNI</code> 是什么呢？<code>CNI</code> 是 Container Network Interface 的缩写，是 K8S 用于配置 Linux 容器网络的接口规范。</p>
<p>关于网络的选择，我们此处不做过多介绍，我们暂时选择一个被广泛使用的方案 <code>flannel</code>。 但注意，如果要使用 <code>flannel</code> 需要在 <code>kubeadm init</code> 的时候，传递 <code>--pod-network-cidr=10.244.0.0/16</code> 参数。另外需要查看 <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> 是否已设置为 <code>1</code>。 可以通过 <code>sysctl net.bridge.bridge-nf-call-iptables=1</code> 更改配置。</p>
<p>我们在前面创建集群时，并没有传递任何参数。为了能使用 <code>flannel</code> , 所以我们需要先将集群重置。使用 <code>kubeadm reset</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubeadm reset </span><br><span class="line">[reset] WARNING: changes made to this host by &#39;kubeadm init&#39; or &#39;kubeadm join&#39; will be reverted.</span><br><span class="line">[reset] are you sure you want to proceed? [y&#x2F;N]: y</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">[reset] stopping the kubelet service</span><br><span class="line">[reset] unmounting mounted directories in &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&quot;</span><br><span class="line">[reset] removing kubernetes-managed containers</span><br><span class="line">[reset] cleaning up running containers using crictl with socket &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">[reset] failed to list running pods using crictl: exit status 1. Trying to use docker instead[reset] deleting contents of stateful directories: [&#x2F;var&#x2F;lib&#x2F;kubelet &#x2F;etc&#x2F;cni&#x2F;net.d &#x2F;var&#x2F;lib&#x2F;dockershim &#x2F;var&#x2F;run&#x2F;kubernetes &#x2F;var&#x2F;lib&#x2F;etcd]</span><br><span class="line">[reset] deleting contents of config directories: [&#x2F;etc&#x2F;kubernetes&#x2F;manifests &#x2F;etc&#x2F;kubernetes&#x2F;pki]</span><br><span class="line">[reset] deleting files: [&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf &#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf &#x2F;etc&#x2F;kubernetes&#x2F;scheduler.conf]</span><br></pre></td></tr></table></figure></div>

<p>重新初始化集群，并传递参数。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubeadm init --pod-network-cidr&#x3D;10.244.0.0&#x2F;16</span><br><span class="line">[init] using Kubernetes version: v1.11.3</span><br><span class="line">...</span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br></pre></td></tr></table></figure></div>

<p><strong>注意：这里会重新生成相应证书等配置，需要按上面的内容重新配置 kubectl。</strong></p>
<p>此时，<code>CNI</code> 也尚未初始化完成，我们还需完成以下的步骤。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 注意，这里的 flannel 配置仅适用于 1.11 版本的 K8S，若安装其他版本的 K8S 需要替换掉此链接</span><br><span class="line">[root@master ~]# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;v0.10.0&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;flannel created</span><br><span class="line">serviceaccount&#x2F;flannel created</span><br><span class="line">configmap&#x2F;kube-flannel-cfg created</span><br><span class="line">daemonset.extensions&#x2F;kube-flannel-ds created</span><br></pre></td></tr></table></figure></div>

<p>稍等片刻，再次查看 Node 状态：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    12m       v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到 status 已经是 <code>Ready</code> 状态。根据第 3 章的内容，我们知道 K8S 中最小的调度单元是 <code>Pod</code> 我们来看下集群中现有 <code>Pod</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods --all-namespaces</span><br><span class="line">NAMESPACE     NAME                             READY     STATUS              RESTARTS   AGE </span><br><span class="line">kube-system   coredns-78fcdf6894-h7pkc         0&#x2F;1       ContainerCreating   0          12m</span><br><span class="line">kube-system   coredns-78fcdf6894-lhlks         0&#x2F;1       ContainerCreating   0          12m</span><br><span class="line">kube-system   etcd-master                      1&#x2F;1       Running             0          5m</span><br><span class="line">kube-system   kube-apiserver-master            1&#x2F;1       Running             0          5m</span><br><span class="line">kube-system   kube-controller-manager-master   1&#x2F;1       Running             0          5m</span><br><span class="line">kube-system   kube-flannel-ds-tqvck            1&#x2F;1       Running             0          6m</span><br><span class="line">kube-system   kube-proxy-25tk2                 1&#x2F;1       Running             0          12m</span><br><span class="line">kube-system   kube-scheduler-master            1&#x2F;1       Running             0          5m</span><br></pre></td></tr></table></figure></div>

<p>我们发现有两个 <code>coredns</code> 的 <code>Pod</code> 是 <code>ContainerCreating</code> 的状态，但并未就绪。根据第 3 章的内容，我们知道 <code>Pod</code> 实际会有一个调度过程，此处我们暂且不论，后续章节再对此进行解释。</p>
<h3 id="新增-Node"><a href="#新增-Node" class="headerlink" title="新增 Node"></a>新增 Node</h3><p>我们按照刚才执行完 <code>kubeadm init</code> 后，给出的信息，在新的机器上执行 <code>kubeadm join</code> 命令。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubeadm join 202.182.112.120:6443 --token t14kzc.vjurhx5k98dpzqdc --discovery-token-ca-cert-hash sha256:d64f7ce1af9f9c0c73d2d737fd0095456ad98a2816cb5527d55f984c8aa8a762</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line"> 1. Run &#39;modprobe -- &#39; to load missing kernel modules;</span><br><span class="line">2. Provide the missing builtin kernel ipvs support</span><br><span class="line"></span><br><span class="line">I0921 04:00:54.805439   10677 kernel_validator.go:81] Validating kernel version                                                  </span><br><span class="line">I0921 04:00:54.805604   10677 kernel_validator.go:96] Validating kernel config                                                   </span><br><span class="line">        [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03</span><br><span class="line">[discovery] Trying to connect to API Server &quot;202.182.112.120:6443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https:&#x2F;&#x2F;202.182.112.120:6443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https:&#x2F;&#x2F;202.182.112.120:6443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;202.182.112.120:6443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;202.182.112.120:6443&quot;</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.11&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;&#x2F;var&#x2F;run&#x2F;dockershim.sock&quot; to the Node API object &quot;node1&quot; as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</span><br></pre></td></tr></table></figure></div>

<p>上面的命令执行完成，提示已经成功加入集群。 此时，我们在 master 上查看下当前集群状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    12m       v1.11.3</span><br><span class="line">node1     Ready     &lt;none&gt;    7m        v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到 node1 已经加入了集群。</p>
<h2 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们选择官方推荐的 <code>kubeadm</code> 工具在服务器上搭建了一套有两个节点的集群。</p>
<p><code>kubeadm</code> 可以自动的拉取相关组件的 Docker 镜像，并将其“组织”起来，免去了我们逐个部署相关组件的麻烦。</p>
<p>我们首先学习到了部署 K8S 时需要对系统做的基础配置，其次安装了一些必要的工具，以便 K8S 的功能可正常运行。</p>
<p>其次，我们学习到了 CNI 相关的知识，并在集群中部署了 <code>flannel</code> 网络方案。</p>
<p>最后，我们学习了增加 Node 的方法，以便后续扩展集群。</p>
<p>集群搭建方面的学习暂时告一段落，但这并不是结束，这才是真正的开始，从下一章开始，我们要学习集群管理相关的内容，学习如何真正使用 K8S 。</p>
<h1 id="集群管理：初识-kubectl"><a href="#集群管理：初识-kubectl" class="headerlink" title="集群管理：初识 kubectl"></a>集群管理：初识 kubectl</h1><p>从本节开始，我们来学习 K8S 集群管理相关的知识。通过前面的学习，我们知道 K8S 遵循 C/S 架构，官方也提供了 CLI 工具 <code>kubectl</code> 用于完成大多数集群管理相关的功能。当然凡是你可以通过 <code>kubectl</code> 完成的与集群交互的功能，都可以直接通过 API 完成。</p>
<p>对于我们来说 <code>kubectl</code> 并不陌生，在第 3 章讲 K8S 整体架构时，我们首次提到了它。在第 4 章和第 5 章介绍了两种安装 <code>kubectl</code> 的方式故而本章不再赘述安装的部分。</p>
<h2 id="整体概览"><a href="#整体概览" class="headerlink" title="整体概览"></a>整体概览</h2><p>首先我们在终端下执行下 <code>kubectl</code>:</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl                </span><br><span class="line">kubectl controls the Kubernetes cluster manager.</span><br><span class="line">...</span><br><span class="line">Usage:</span><br><span class="line">  kubectl [flags] [options]</span><br></pre></td></tr></table></figure></div>

<p><code>kubectl</code> 已经将命令做了基本的归类，同时显示了其一般的用法 <code>kubectl [flags] [options]</code> 。</p>
<p>使用 <code>kubectl options</code> 可以看到所有全局可用的配置项。</p>
<h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><p>在我们的用户家目录，可以看到一个名为 <code>.kube/config</code> 的配置文件，我们来看下其中的内容（此处以本地的 minikube 集群为例）。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ls $HOME&#x2F;.kube&#x2F;config </span><br><span class="line">&#x2F;home&#x2F;tao&#x2F;.kube&#x2F;config</span><br><span class="line">➜  ~ cat $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;ca.crt</span><br><span class="line">    server: https:&#x2F;&#x2F;192.168.99.101:8443</span><br><span class="line">  name: minikube</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: minikube</span><br><span class="line">    user: minikube</span><br><span class="line">  name: minikube</span><br><span class="line">current-context: minikube</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: minikube</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.crt</span><br><span class="line">    client-key: &#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.key</span><br></pre></td></tr></table></figure></div>

<p><code>$HOME/.kube/config</code> 中主要包含着：</p>
<ul>
<li>K8S 集群的 API 地址</li>
<li>用于认证的证书地址</li>
</ul>
<p>当然，我们在第 5 章时，也已经说过，也可以使用 <code>--kubeconfig</code> 或者环境变量 <code>KUBECONFIG</code> 来传递配置文件。</p>
<p>另外如果你并不想使用配置文件的话，你也可以通过使用直接传递相关参数来使用，例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl --client-key&#x3D;&#39;&#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.key&#39; --client-certificate&#x3D;&#39;&#x2F;home&#x2F;tao&#x2F;.minikube&#x2F;client.crt&#39; --server&#x3D;&#39;https:&#x2F;&#x2F;192.168.99.101:8443&#39;  get nodes</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br></pre></td></tr></table></figure></div>

<h2 id="从-get-说起"><a href="#从-get-说起" class="headerlink" title="从 get 说起"></a>从 <code>get</code> 说起</h2><p>无论是第 4 章还是第 5 章，当我们创建集群后，我们都做了两个相同的事情，一个是执行 <code>kubectl get nodes</code> 另一个则是 <code>kubectl cluster-info</code>，我们先从查看集群内 <code>Node</code> 开始。</p>
<p>这里我们使用了一个本地已创建好的 <code>minikube</code> 集群。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get nodes</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br><span class="line">➜  ~ kubectl get node</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br><span class="line">➜  ~ kubectl get no</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    2d        v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>可以看到以上三种“名称”均可获取当前集群内 <code>Node</code> 信息。这是为了便于使用而增加的别名和缩写。</p>
<p>如果我们想要看到更详细的信息呢？可以通过传递 <code>-o</code> 参数以得到不同格式的输出。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get nodes -o wide </span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE            KERNEL-VERSION   CONTAINER-RUNTIME</span><br><span class="line">minikube   Ready     master    2d        v1.11.3   10.0.2.15     &lt;none&gt;        Buildroot 2018.05   4.15.0           docker:&#x2F;&#x2F;17.12.1-ce</span><br></pre></td></tr></table></figure></div>

<p>当然也可以传递 <code>-o yaml</code> 或者 <code>-o json</code> 得到更加详尽的信息。</p>
<p>使用 <code>-o json</code> 将内容以 JSON 格式输出时，可以配合 <a href="https://stedolan.github.io/jq/" target="_blank" rel="noopener"><code>jq</code></a> 进行内容提取。例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get nodes -o json | jq &quot;.items[] | &#123;name: .metadata.name&#125; + .status.nodeInfo&quot;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;minikube&quot;,</span><br><span class="line">  &quot;architecture&quot;: &quot;amd64&quot;,</span><br><span class="line">  &quot;bootID&quot;: &quot;d675d75b-e58e-40db-8910-6e5dda9e7cf9&quot;,</span><br><span class="line">  &quot;containerRuntimeVersion&quot;: &quot;docker:&#x2F;&#x2F;17.12.1-ce&quot;,</span><br><span class="line">  &quot;kernelVersion&quot;: &quot;4.15.0&quot;,</span><br><span class="line">  &quot;kubeProxyVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;kubeletVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;machineID&quot;: &quot;078e2d22629747178397e29cf1c96cc7&quot;,</span><br><span class="line">  &quot;operatingSystem&quot;: &quot;linux&quot;,</span><br><span class="line">  &quot;osImage&quot;: &quot;Buildroot 2018.05&quot;,</span><br><span class="line">  &quot;systemUUID&quot;: &quot;4073906D-69A1-46EE-A08C-0252D9F79893&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>以此方法可得到 <code>Node</code> 的基础信息。</p>
<p>那么除了 <code>Node</code> 外我们还可以查看那些资源或别名呢？可以通过 <code>kubectl api-resources</code> 查看服务端支持的 API 资源及别名和描述等信息。</p>
<h2 id="答疑解惑-explain"><a href="#答疑解惑-explain" class="headerlink" title="答疑解惑 explain"></a>答疑解惑 <code>explain</code></h2><p>当通过上面的命令拿到所有支持的 API 资源列表后，虽然后面基本都有一个简单的说明，是不是仍然感觉一头雾水？</p>
<p>别担心，在我们使用 Linux 的时候，我们有 <code>man</code> ，在使用 <code>kubectl</code> 的时候，我们除了 <code>--help</code> 外还有 <code>explain</code> 可帮我们进行说明。 例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl explain node                                                                                                              </span><br><span class="line">KIND:     Node</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">DESCRIPTION:              </span><br><span class="line">     Node is a worker node in Kubernetes. Each node will have a unique</span><br><span class="line">     identifier in the cache (i.e. in etcd).</span><br><span class="line"></span><br><span class="line"># ... 省略输出</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h2><p>本节我们大致介绍了 <code>kubectl</code> 的基础使用，尤其是最常见的 <code>get</code> 命令。可通过传递不同参数获取不同格式的输出，配合 <code>jq</code> 工具可方便的进行内容提取。</p>
<p>以及关于 <code>kubectl</code> 的配置文件和无配置文件下通过传递参数直接使用等。</p>
<p>对应于我们前面提到的 K8S 架构，本节相当于 <code>CURD</code> 中的 <code>R</code> 即查询。查询对于我们来说，既是我们了解集群的第一步，同时也是后续验证操作结果或集群状态必不可少的技能。</p>
<p>当然，你在集群管理中可能会遇到各种各样的问题，单纯依靠 <code>get</code> 并不足以定位问题，我们在第 21 节中将介绍 Troubleshoot 的思路及方法。</p>
<p>下节我们来学习关于 <code>C</code> 的部分，即创建。</p>
<h1 id="集群管理：以-Redis-为例-部署及访问"><a href="#集群管理：以-Redis-为例-部署及访问" class="headerlink" title="集群管理：以 Redis 为例-部署及访问"></a>集群管理：以 Redis 为例-部署及访问</h1><p>上节我们已经学习了 <code>kubectl</code> 的基础使用，本节我们使用 <code>kubectl</code> 在 K8S 中进行部署。</p>
<p><strong>前面我们已经说过，Pod 是 K8S 中最小的调度单元，所以我们无法直接在 K8S 中运行一个 container 但是我们可以运行一个 Pod 而这个 Pod 中只包含一个 container 。</strong></p>
<h2 id="从-kubectl-run-开始"><a href="#从-kubectl-run-开始" class="headerlink" title="从 kubectl run 开始"></a>从 <code>kubectl run</code> 开始</h2><p><code>kubectl run</code> 的基础用法如下：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Usage:</span><br><span class="line">  kubectl run NAME --image&#x3D;image [--env&#x3D;&quot;key&#x3D;value&quot;] [--port&#x3D;port] [--replicas&#x3D;replicas] [--dry-run&#x3D;bool] [--overrides&#x3D;inline-json] [--command] -- [COMMAND] [args...] [options]</span><br></pre></td></tr></table></figure></div>

<p><code>NAME</code> 和 <code>--image</code> 是必需项。分别代表此次部署的名字及所使用的镜像，其余部分之后进行解释。当然，在我们实际使用时，推荐编写配置文件并通过 <code>kubectl create</code> 进行部署。</p>
<h2 id="使用最小的-Redis-镜像"><a href="#使用最小的-Redis-镜像" class="headerlink" title="使用最小的 Redis 镜像"></a>使用最小的 Redis 镜像</h2><p>在 Redis 的<a href="https://hub.docker.com/_/redis/" target="_blank" rel="noopener">官方镜像列表</a>可以看到有很多的 tag 可供选择，其中使用 <a href="https://alpinelinux.org/" target="_blank" rel="noopener">Alpine Linux</a> 作为基础的镜像体积最小，下载较为方便。我们选择 <code>redis:alpine</code> 这个镜像进行部署。</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>现在我们只部署一个 Redis 实例。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl run redis --image&#x3D;&#39;redis:alpine&#39;</span><br><span class="line">deployment.apps&#x2F;redis created</span><br></pre></td></tr></table></figure></div>

<p>可以看到提示 <code>deployment.apps/redis created</code> 这个稍后进行解释，我们使用 <code>kubectl get all</code> 来看看到底发生了什么。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get all</span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-7c7545cbcb-2m6rp   1&#x2F;1       Running   0          30s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   32s</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            1           30s</span><br><span class="line"></span><br><span class="line">NAME                               DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-7c7545cbcb   1         1         1         30s</span><br></pre></td></tr></table></figure></div>

<p>可以看到其中有我们刚才执行 <code>run</code> 操作后创建的 <code>deployment.apps/redis</code>，还有 <code>replicaset.apps/redis-7c7545cbcb</code>, <code>service/kubernetes</code> 以及 <code>pod/redis-7c7545cbcb-f984p</code>。</p>
<p>使用 <code>kubectl get all</code> 输出内容的格式 <code>/</code> 前代表类型，<code>/</code> 后是名称。</p>
<p>这些分别代表什么含义？</p>
<h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p><code>Deployment</code> 是一种高级别的抽象，允许我们进行扩容，滚动更新及降级等操作。我们使用 <code>kubectl run redis --image=&#39;redis:alpine</code> 命令便创建了一个名为 <code>redis</code> 的 <code>Deployment</code>，并指定了其使用的镜像为 <code>redis:alpine</code>。</p>
<p>同时 K8S 会默认为其增加一些标签（<code>Label</code>）。我们可以通过更改 <code>get</code> 的输出格式进行查看。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get deployment.apps&#x2F;redis -o wide </span><br><span class="line">NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">redis     1         1         1            1           40s       redis        redis:alpine   run&#x3D;redis</span><br><span class="line">➜  ~ kubectl get deploy redis -o wide          </span><br><span class="line">NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">redis     1         1         1            1           40s       redis        redis:alpine   run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>那么这些 <code>Label</code> 有什么作用呢？它们可作为选择条件进行使用。如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get deploy -l run&#x3D;redis -o wide </span><br><span class="line">NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">redis     1         1         1            1           11h       redis        redis:alpine   run&#x3D;redis</span><br><span class="line">➜  ~ kubectl get deploy -l run&#x3D;test -o wide  # 由于我们并没有创建过 test 所以查不到任何东西</span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure></div>

<p>我们在应用部署或更新时总是会考虑的一个问题是如何平滑升级，利用 <code>Deployment</code> 也能很方便的进行金丝雀发布（Canary deployments）。这主要也依赖 <code>Label</code> 和 <code>Selector</code>， 后面我们再详细介绍如何实现。</p>
<p><code>Deployment</code> 的创建除了使用我们这里提到的方式外，更推荐的方式便是使用 <code>yaml</code> 格式的配置文件。在配置文件中主要是声明一种预期的状态，而其他组件则负责协同调度并最终达成这种预期的状态。当然这也是它的关键作用之一，将 <code>Pod</code> 托管给下面将要介绍的 <code>ReplicaSet</code>。</p>
<h3 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h3><p><code>ReplicaSet</code> 是一种较低级别的结构，允许进行扩容。</p>
<p>我们上面已经提到 <code>Deployment</code> 主要是声明一种预期的状态，并且会将 <code>Pod</code> 托管给 <code>ReplicaSet</code>，而 <code>ReplicaSet</code> 则会去检查当前的 <code>Pod</code> 数量及状态是否符合预期，并尽量满足这一预期。</p>
<p><code>ReplicaSet</code> 可以由我们自行创建，但一般情况下不推荐这样去做，因为如果这样做了，那其实就相当于跳过了 <code>Deployment</code> 的部分，<code>Deployment</code> 所带来的功能或者特性我们便都使用不到了。</p>
<p>除了 <code>ReplicaSet</code> 外，我们还有一个选择名为 <code>ReplicationController</code>，这两者的主要区别更多的在选择器上，我们后面再做讨论。现在推荐的做法是 <code>ReplicaSet</code> 所以不做太多解释。</p>
<p><code>ReplicaSet</code> 可简写为 <code>rs</code>，通过以下命令查看：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get rs -o wide</span><br><span class="line">NAME               DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES         SELECTOR                           </span><br><span class="line">redis-7c7545cbcb   1         1         1         11h       redis        redis:alpine   pod-template-hash&#x3D;3731017676,run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>在输出结果中，我们注意到这里除了我们前面看到的 <code>run=redis</code> 标签外，还多了一个 <code>pod-template-hash=3731017676</code> 标签，这个标签是由 <code>Deployment controller</code> 自动添加的，目的是为了防止出现重复，所以将 <code>pod-template</code> 进行 hash 用作唯一性标识。</p>
<h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p><code>Service</code> 简单点说就是为了能有个稳定的入口访问我们的应用服务或者是一组 <code>Pod</code>。通过 <code>Service</code> 可以很方便的实现服务发现和负载均衡。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get service -o wide</span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE       SELECTOR</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   16m        &lt;none&gt;</span><br></pre></td></tr></table></figure></div>

<p>通过使用 <code>kubectl</code> 查看，能看到主要会显示 <code>Service</code> 的名称，类型，IP，端口及创建时间和选择器等。我们来具体拆解下。</p>
<h4 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h4><p><code>Service</code> 目前有 4 种类型：</p>
<ul>
<li><code>ClusterIP</code>： 是 K8S 当前默认的 <code>Service</code> 类型。将 service 暴露于一个仅集群内可访问的虚拟 IP 上。</li>
<li><code>NodePort</code>： 是通过在集群内所有 <code>Node</code> 上都绑定固定端口的方式将服务暴露出来，这样便可以通过 <code>:</code> 访问服务了。</li>
<li><code>LoadBalancer</code>： 是通过 <code>Cloud Provider</code> 创建一个外部的负载均衡器，将服务暴露出来，并且会自动创建外部负载均衡器路由请求所需的 <code>Nodeport</code> 或 <code>ClusterIP</code> 。</li>
<li><code>ExternalName</code>： 是通过将服务由 DNS CNAME 的方式转发到指定的域名上将服务暴露出来，这需要 <code>kube-dns</code> 1.7 或更高版本支持。</li>
</ul>
<h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><p>上面已经说完了 <code>Service</code> 的基本类型，而我们也已经部署了一个 Redis ,当还无法访问到该服务，接下来我们将刚才部署的 Redis 服务暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl expose deploy&#x2F;redis --port&#x3D;6379 --protocol&#x3D;TCP --target-port&#x3D;6379 --name&#x3D;redis-server  </span><br><span class="line">service&#x2F;redis-server exposed</span><br><span class="line">➜  ~ kubectl get svc -o wide                                                                       </span><br><span class="line">NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR</span><br><span class="line">kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443&#x2F;TCP    49m       &lt;none&gt;</span><br><span class="line">redis-server   ClusterIP   10.108.105.63   &lt;none&gt;        6379&#x2F;TCP   4s        run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>通过 <code>kubectl expose</code> 命令将 redis server 暴露出来，这里需要进行下说明：</p>
<ul>
<li><code>port</code>： 是 <code>Service</code> 暴露出来的端口，可通过此端口访问 <code>Service</code>。</li>
<li><code>protocol</code>： 是所用协议。当前 K8S 支持 TCP/UDP 协议，在 1.12 版本中实验性的加入了对 <a href="https://zh.wikipedia.org/zh-hans/流控制传输协议" target="_blank" rel="noopener">SCTP 协议</a>的支持。默认是 TCP 协议。</li>
<li><code>target-port</code>： 是实际服务所在的目标端口，请求由 <code>port</code> 进入通过上述指定 <code>protocol</code> 最终流向这里配置的端口。</li>
<li><code>name</code>： <code>Service</code> 的名字，它的用处主要在 dns 方面。</li>
<li><code>type</code>： 是前面提到的类型，如果没指定默认是 <code>ClusterIP</code>。</li>
</ul>
<p>现在我们的 redis 是使用的默认类型 <code>ClusterIP</code>，所以并不能直接通过外部进行访问，我们使用 <code>port-forward</code> 的方式让它可在集群外部访问。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl port-forward svc&#x2F;redis-server 6379:6379</span><br><span class="line">Forwarding from 127.0.0.1:6379 -&gt; 6379</span><br><span class="line">Forwarding from [::1]:6379 -&gt; 6379</span><br><span class="line">Handling connection for 6379</span><br></pre></td></tr></table></figure></div>

<p>在另一个本地终端内可通过 redis-cli 工具进行连接：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ redis-cli -h 127.0.0.1 -p 6379</span><br><span class="line">127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure></div>

<p>当然，我们也可以使用 <code>NodePort</code> 的方式对外暴露服务。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl expose deploy&#x2F;redis --port&#x3D;6379 --protocol&#x3D;TCP --target-port&#x3D;6379 --name&#x3D;redis-server-nodeport --type&#x3D;NodePort</span><br><span class="line">service&#x2F;redis-server-nodeport exposed</span><br><span class="line">➜  ~ kubectl get service&#x2F;redis-server-nodeport -o wide </span><br><span class="line">NAME                    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE       SELECTOR</span><br><span class="line">redis-server-nodeport   NodePort   10.109.248.204   &lt;none&gt;        6379:31913&#x2F;TCP   11s       run&#x3D;redis</span><br></pre></td></tr></table></figure></div>

<p>我们可以通过任意 <code>Node</code> 上的 31913 端口便可连接我们的 redis 服务。当然，这里需要注意的是这个端口范围其实是可以通过 <code>kube-apiserver</code> 的 <code>service-node-port-range</code> 进行配置的，默认是 <code>30000-32767</code>。</p>
<h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>第二节中，我们提到过 <code>Pod</code> 是 K8S 中的最小化部署单元。我们看下当前集群中 <code>Pod</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get pods</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">redis-7c7545cbcb-jwcf2   1&#x2F;1       Running   0          8h</span><br></pre></td></tr></table></figure></div>

<p>我们进行一次简单的扩容操作。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl scale deploy&#x2F;redis --replicas&#x3D;2</span><br><span class="line">deployment.extensions&#x2F;redis scaled</span><br><span class="line">➜  ~ kubectl get pods</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">redis-7c7545cbcb-jwcf2   1&#x2F;1       Running   0          8h</span><br><span class="line">redis-7c7545cbcb-wzh6w   1&#x2F;1       Running   0          4s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 数已经增加，并且也已经是 <code>Running</code> 的状态了。(当然在生产环境中 Redis 服务的扩容并不是使用这种方式进行扩容的，需要看实际的部署方式以及业务的使用姿势。)</p>
<h2 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h2><p>本节我们使用 Redis 作为例子，学习了集群管理相关的基础知识。学习了如何进行应用部署， <code>Service</code> 的基础类型以及如何通过 <code>port-forward</code> 或 <code>NodePort</code> 等方式将服务提供至集群的外部访问。</p>
<p>同时我们学习了应用部署中主要会涉及到的几类资源 <code>Deployment</code>，<code>Replicaset</code>，<code>Service</code> 和 <code>Pod</code> 等。对这些资源及它们之间关系的掌握，对于后续集群维护或定位问题有很大的帮助。</p>
<p>下节，我们开始学习在生产环境中使用 K8S 至关重要的一环，权限控制。</p>
<h1 id="安全重点-认证和授权"><a href="#安全重点-认证和授权" class="headerlink" title="安全重点: 认证和授权"></a>安全重点: 认证和授权</h1><p>本节我们将开始学习将 K8S 应用于生产环境中至关重要的一环，权限控制。当然，不仅是 K8S 对于任何应用于生产环境中的系统，权限管理或者说访问控制都是很重要的。</p>
<h2 id="整体概览-1"><a href="#整体概览-1" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们已经知道 K8S 中几乎所有的操作都需要经过 <code>kube-apiserver</code> 处理，所以为了安全起见，K8S 为它提供了三类安全访问的措施。分别是：用于识别用户身份的认证（Authentication），用于控制用户对资源访问的授权（Authorization）以及用于资源管理方面的准入控制（Admission Control）。</p>
<p>下面的图基本展示了这一过程。来自客户端的请求分别经过认证，授权，准入控制之后，才能真正执行。</p>
<p>当然，这里说<strong>基本展示</strong>是因为我们可以直接通过 <code>kubectl proxy</code> 的方式直接通过 HTTP 请求访问 <code>kube-apiserver</code> 而无需任何认证过程。</p>
<p>另外，也可通过在 <code>kube-apiserver</code> 所启动的机器上，直接访问启动时 <code>--insecure-port</code> 参数配置的端口进行绕过认证和授权，默认是 8080。为了避免安全问题，也可将此参数设置为 0 以规避问题。注意：这个参数和 <code>--insecure-bind-address</code> 都已过期，并将在未来的版本移除。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------------------------------------+</span><br><span class="line">|                                                                                                           |</span><br><span class="line">|               +---------------------------------------------------------------------------+    +--------+ |</span><br><span class="line">|               |                                                                           |    |        | |</span><br><span class="line">| +--------+    |   +------------------+   +----------------+   +--------------+   +------+ |    |        | |</span><br><span class="line">| |        |    |   |                  |   |                |   | Admission    |   |      | |    |        | |</span><br><span class="line">| | Client +------&gt; | Authentication   +-&gt; | Authorization  +-&gt; | Control      +-&gt; |Logic | +--&gt; | Others | |</span><br><span class="line">| |        |    |   |                  |   |                |   |              |   |      | |    |        | |</span><br><span class="line">| +--------+    |   +------------------+   +----------------+   +--------------+   +------+ |    |        | |</span><br><span class="line">|               |                                                                           |    |        | |</span><br><span class="line">|               |                                                                           |    |        | |</span><br><span class="line">|               |                          Kube-apiserver                                   |    |        | |</span><br><span class="line">|               +---------------------------------------------------------------------------+    +--------+ |</span><br><span class="line">|                                                                                                           |</span><br><span class="line">+-----------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<h2 id="认证（Authentication）"><a href="#认证（Authentication）" class="headerlink" title="认证（Authentication）"></a>认证（Authentication）</h2><p>认证，无非是判断当前发起请求的用户身份是否正确。例如，我们通常登录服务器时候需要输入用户名和密码，或者 SSH Keys 之类的。</p>
<p>在讲认证前，我们应该先理一下 K8S 中的用户。</p>
<p>K8S 中有两类用户，一般用户及 <code>Service Account</code>。</p>
<ul>
<li>一般用户：一般用户只能通过外部服务进行管理，由管理员进行私钥分发。这也意味着 K8S 中并没有任何表示一般用户的对象，所以一般用户是无法通过 API 直接添加到集群的。</li>
<li><code>Service Account</code>：由 K8S API 管理的用户，与特定的 <code>NameSpace</code>（命名空间）绑定。由 <code>API Server</code> 自动创建或者通过 API 手动进行创建。 同时，它会自动挂载到 <code>Pod</code> 中容器的 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code> 目录中，其中会包含 <code>NameSpace</code> <code>token</code> 等信息，并允许集群内进程与 <code>API Server</code> 进行交互。</li>
</ul>
<p>对集群操作的 API 都是与用户相关联的，或者被视为匿名请求。匿名请求可通过 <code>kube-apiserver</code> 的 <code>--anonymous-auth</code> 参数进行控制，默认是开启的，匿名用户默认的用户名为 <code>system:anonymous</code>，所属组为 <code>system:unauthenticated</code>。</p>
<p>理完 K8S 中的用户，我们来看下 K8S 中的认证机制。</p>
<p>K8S 支持以下认证机制：</p>
<ul>
<li>X509 客户端证书：这个认证机制我们并不陌生，我们前面搭建集群时，虽然没有指定配置文件，但 <code>kubeadm</code> 已经添加了默认参数 <code>--client-ca-file=/etc/kubernetes/pki/ca.crt</code> 而在进行认证时，将会使用客户端证书 subject 的 <code>CN</code> 域（Common Name）用作用户名，<code>O</code> 域（Organization）用作组名。</li>
<li>引导 Token：这个我们也不会陌生，前面我们搭建集群时，当集群通过 <code>kubeadm init</code> 初始化完成后，将会展示一行提示，其中便携带着引导 Token。如果不使用 <code>kubeadm</code> 时，需要设置 <code>--enable-bootstrap-token-auth=true</code>。</li>
<li>静态 Token 文件：启动 <code>Kube-apiserver</code> 时，设置 <code>--token-auth-file=SOMEFILE</code> 并在请求时，加上 <code>Authorization: Bearer TOKEN</code> 的请求头即可。</li>
<li>静态密码文件：与静态 Token 文件类似，设置 <code>--basic-auth-file=SOMEFILE</code> 并在请求时，加上 <code>Authorization: Basic BASE64ENCODED(USER:PASSWORD)</code> 的头即可。</li>
<li>Service Account Token：这是默认启用的机制，关于 <code>Service Account</code> 前面也已经介绍过了，不再赘述。</li>
<li>OpenID：其实是提供了 <a href="http://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html" target="_blank" rel="noopener">OAuth2</a> 的认证支持，像 Azure 或 Google 这类云厂商都提供了相关支持。</li>
<li>认证代理：主要是配合身份验证代理进行使用，比如提供一个通用的授权网关供用户使用。</li>
<li>Webhook：提供 Webhook 配合一个远端服务器使用。</li>
</ul>
<p>可选择同时开启多个认证机制。比如当我们使用 <code>kubeadm</code> 创建集群时，默认便会开启 X509 客户端证书和引导 Token 等认证机制。</p>
<h2 id="授权（Authorization）"><a href="#授权（Authorization）" class="headerlink" title="授权（Authorization）"></a>授权（Authorization）</h2><p>授权，也就是在验证当前发起请求的用户是否有相关的权限。例如，我们在 Linux 系统中常见的文件夹权限之类的。</p>
<p>授权是以认证的结果为基础的，授权机制检查用户通过认证后的请求中所包含的属性来进行判断。</p>
<p>K8S 支持多种授权机制，用户想要正确操作资源，则必须获得授权，所以 K8S 默认情况下的权限都是拒绝。当某种授权机制通过或者拒绝后，便会立即返回，不再去请求其他的授权机制；当所有授权机制都未通过时便会返回 403 错误了。</p>
<p>K8S 支持以下授权机制：</p>
<ul>
<li>ABAC(Attribute-Based Access Control)：基于属性的访问控制，在使用时需要先配置 <code>--authorization-mode=ABAC</code> 和 <code>--authorization-policy-file=SOME_FILENAME</code> 。ABAC 本身设计是非常好的，但是在 K8S 中使用却有点过于繁琐，这里不再赘述。</li>
<li>RBAC(Role-based access control)：基于角色的访问控制，自 K8S 1.6 开始 beta，1.8 进入稳定版，已被大量使用。而当我们使用 <code>kubeadm</code> 安装集群的时候，默认将会添加 <code>--authorization-mode=Node,RBAC</code> 的参数，表示同时开启 <code>Node</code> 和 <code>RBAC</code> 授权机制。当然，如果你对 <a href="https://www.mongodb.com/cn" target="_blank" rel="noopener">MongoDB</a> 有所了解或者比较熟悉的话，这部分的内容就会很容易理解，因为 MongoDB 的权限控制也使用了 <code>RBAC</code> （Role-based access control）。</li>
<li>Node：这是一种特殊用途的授权机制，专门用于对 <code>kubelet</code> 发出的 API 请求做授权验证。</li>
<li>Webhook：使用外部的 Server 通过 API 进行授权校验，需要在启动时候增加 <code>--authorization-webhook-config-file=SOME_FILENAME</code> 以及 <code>--authorization-mode=Webhook</code></li>
<li>AlwaysAllow：默认配置，允许全部。</li>
<li>AlwaysDeny：通常用于测试，禁止全部。</li>
</ul>
<h2 id="角色（Role）"><a href="#角色（Role）" class="headerlink" title="角色（Role）"></a>角色（Role）</h2><p>上面提到了 <code>RBAC</code>，为了能更好的理解，我们需要先认识下 K8S 中的角色。K8S 中的角色从类别上主要有两类，<code>Role</code> 和 <code>ClusterRole</code>。</p>
<ul>
<li><code>Role</code>：可以当作是一组权限的集合，但被限制在某个 <code>Namespace</code> 内（K8S 的 <code>Namespace</code>）。</li>
<li><code>ClusterRole</code>：对于集群级别的资源是不被 <code>Namespace</code> 所限制的，并且还有一些非资源类的请求，所以便产生了它。</li>
</ul>
<p>当已经了解到角色后，剩下给用户授权也就只是需要做一次绑定即可。在 K8S 中将这一过程称之为 binding，即 <code>rolebinding</code> 和 <code>clusterrolebinding</code>。 我们来看下集群刚初始化后的情况：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get roles --all-namespaces&#x3D;true</span><br><span class="line">NAMESPACE     NAME                                             AGE</span><br><span class="line">kube-public   kubeadm:bootstrap-signer-clusterinfo             1h</span><br><span class="line">kube-public   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   extension-apiserver-authentication-reader        1h</span><br><span class="line">kube-system   kube-proxy                                       1h</span><br><span class="line">kube-system   kubeadm:kubelet-config-1.12                      1h</span><br><span class="line">kube-system   kubeadm:nodes-kubeadm-config                     1h</span><br><span class="line">kube-system   system::leader-locking-kube-controller-manager   1h</span><br><span class="line">kube-system   system::leader-locking-kube-scheduler            1h</span><br><span class="line">kube-system   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   system:controller:cloud-provider                 1h</span><br><span class="line">kube-system   system:controller:token-cleaner                  1h</span><br><span class="line">➜  ~ kubectl get rolebindings --all-namespaces&#x3D;true</span><br><span class="line">NAMESPACE     NAME                                             AGE</span><br><span class="line">kube-public   kubeadm:bootstrap-signer-clusterinfo             1h</span><br><span class="line">kube-public   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   kube-proxy                                       1h</span><br><span class="line">kube-system   kubeadm:kubelet-config-1.12                      1h</span><br><span class="line">kube-system   kubeadm:nodes-kubeadm-config                     1h</span><br><span class="line">kube-system   system::leader-locking-kube-controller-manager   1h</span><br><span class="line">kube-system   system::leader-locking-kube-scheduler            1h</span><br><span class="line">kube-system   system:controller:bootstrap-signer               1h</span><br><span class="line">kube-system   system:controller:cloud-provider                 1h</span><br><span class="line">kube-system   system:controller:token-cleaner                  1h</span><br></pre></td></tr></table></figure></div>

<p>可以看到默认已经存在了一些 <code>role</code> 和 <code>rolebindings</code>。 对于这部分暂且不做过多说明，我们来看下对于集群全局有效的 <code>ClusterRole</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get clusterroles</span><br><span class="line">NAME                                                                   AGE</span><br><span class="line">admin                                                                  1h</span><br><span class="line">cluster-admin                                                          1h</span><br><span class="line">edit                                                                   1h</span><br><span class="line">flannel                                                                1h</span><br><span class="line">system:aggregate-to-admin                                              1h</span><br><span class="line">system:aggregate-to-edit                                               1h</span><br><span class="line">system:aggregate-to-view                                               1h</span><br><span class="line">system:auth-delegator                                                  1h</span><br><span class="line">system:aws-cloud-provider                                              1h</span><br><span class="line">system:basic-user                                                      1h</span><br><span class="line">system:certificates.k8s.io:certificatesigningrequests:nodeclient       1h</span><br><span class="line">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   1h</span><br><span class="line">system:controller:attachdetach-controller                              1h</span><br><span class="line">system:controller:certificate-controller                               1h</span><br><span class="line">system:controller:clusterrole-aggregation-controller                   1h</span><br><span class="line">system:controller:cronjob-controller                                   1h</span><br><span class="line">system:controller:daemon-set-controller                                1h</span><br><span class="line">system:controller:deployment-controller                                1h</span><br><span class="line">system:controller:disruption-controller                                1h</span><br><span class="line">system:controller:endpoint-controller                                  1h</span><br><span class="line">system:controller:expand-controller                                    1h</span><br><span class="line">system:controller:generic-garbage-collector                            1h</span><br><span class="line">system:controller:horizontal-pod-autoscaler                            1h</span><br><span class="line">system:controller:job-controller                                       1h</span><br><span class="line">system:controller:namespace-controller                                 1h</span><br><span class="line">system:controller:node-controller                                      1h</span><br><span class="line">system:controller:persistent-volume-binder                             1h</span><br><span class="line">system:controller:pod-garbage-collector                                1h</span><br><span class="line">system:controller:pv-protection-controller                             1h</span><br><span class="line">system:controller:pvc-protection-controller                            1h</span><br><span class="line">system:controller:replicaset-controller                                1h</span><br><span class="line">system:controller:replication-controller                               1h</span><br><span class="line">system:controller:resourcequota-controller                             1h</span><br><span class="line">system:controller:route-controller                                     1h</span><br><span class="line">system:controller:service-account-controller                           1h</span><br><span class="line">system:controller:service-controller                                   1h</span><br><span class="line">system:controller:statefulset-controller                               1h</span><br><span class="line">system:controller:ttl-controller                                       1h</span><br><span class="line">system:coredns                                                         1h</span><br><span class="line">system:csi-external-attacher                                           1h</span><br><span class="line">system:csi-external-provisioner                                        1h</span><br><span class="line">system:discovery                                                       1h</span><br><span class="line">system:heapster                                                        1h</span><br><span class="line">system:kube-aggregator                                                 1h</span><br><span class="line">system:kube-controller-manager                                         1h</span><br><span class="line">system:kube-dns                                                        1h</span><br><span class="line">system:kube-scheduler                                                  1h</span><br><span class="line">system:kubelet-api-admin                                               1h</span><br><span class="line">system:node                                                            1h</span><br><span class="line">system:node-bootstrapper                                               1h</span><br><span class="line">system:node-problem-detector                                           1h</span><br><span class="line">system:node-proxier                                                    1h</span><br><span class="line">system:persistent-volume-provisioner                                   1h</span><br><span class="line">system:volume-scheduler                                                1h</span><br><span class="line">view                                                                   1h</span><br><span class="line">➜  ~ kubectl get clusterrolebindings</span><br><span class="line">NAME                                                   AGE</span><br><span class="line">cluster-admin                                          1h</span><br><span class="line">flannel                                                1h</span><br><span class="line">kubeadm:kubelet-bootstrap                              1h</span><br><span class="line">kubeadm:node-autoapprove-bootstrap                     1h</span><br><span class="line">kubeadm:node-autoapprove-certificate-rotation          1h</span><br><span class="line">kubeadm:node-proxier                                   1h</span><br><span class="line">system:aws-cloud-provider                              1h</span><br><span class="line">system:basic-user                                      1h</span><br><span class="line">system:controller:attachdetach-controller              1h</span><br><span class="line">system:controller:certificate-controller               1h</span><br><span class="line">system:controller:clusterrole-aggregation-controller   1h</span><br><span class="line">system:controller:cronjob-controller                   1h</span><br><span class="line">system:controller:daemon-set-controller                1h</span><br><span class="line">system:controller:deployment-controller                1h</span><br><span class="line">system:controller:disruption-controller                1h</span><br><span class="line">system:controller:endpoint-controller                  1h</span><br><span class="line">system:controller:expand-controller                    1h</span><br><span class="line">system:controller:generic-garbage-collector            1h</span><br><span class="line">system:controller:horizontal-pod-autoscaler            1h</span><br><span class="line">system:controller:job-controller                       1h</span><br><span class="line">system:controller:namespace-controller                 1h</span><br><span class="line">system:controller:node-controller                      1h</span><br><span class="line">system:controller:persistent-volume-binder             1h</span><br><span class="line">system:controller:pod-garbage-collector                1h</span><br><span class="line">system:controller:pv-protection-controller             1h</span><br><span class="line">system:controller:pvc-protection-controller            1h</span><br><span class="line">system:controller:replicaset-controller                1h</span><br><span class="line">system:controller:replication-controller               1h</span><br><span class="line">system:controller:resourcequota-controller             1h</span><br><span class="line">system:controller:route-controller                     1h</span><br><span class="line">system:controller:service-account-controller           1h</span><br><span class="line">system:controller:service-controller                   1h</span><br><span class="line">system:controller:statefulset-controller               1h</span><br><span class="line">system:controller:ttl-controller                       1h</span><br><span class="line">system:coredns                                         1h</span><br><span class="line">system:discovery                                       1h</span><br><span class="line">system:kube-controller-manager                         1h</span><br><span class="line">system:kube-dns                                        1h</span><br><span class="line">system:kube-scheduler                                  1h</span><br><span class="line">system:node                                            1h</span><br><span class="line">system:node-proxier                                    1h</span><br><span class="line">system:volume-scheduler                                1h</span><br></pre></td></tr></table></figure></div>

<p>可以看到 K8S 中默认已经有很多的 <code>ClusterRole</code> 和 <code>clusterrolebindings</code> 了，我们选择其中一个做下探究。</p>
<h2 id="查看用户权限"><a href="#查看用户权限" class="headerlink" title="查看用户权限"></a>查看用户权限</h2><p>我们一直都在使用 <code>kubectl</code> 对集群进行操作，那么当前用户是什么权限呢？ 对应于 <code>RBAC</code> 中又是什么情况呢？</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl config current-context   # 获取当前上下文</span><br><span class="line">kubernetes-admin@kubernetes  # 名为 kubernetes-admin 的用户，在名为 kubernetes 的 cluster 上</span><br><span class="line">➜  ~ kubectl config view users -o yaml  # 查看 user 配置，以下省略了部分内容</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:                                   </span><br><span class="line">- cluster:                            </span><br><span class="line">    ...</span><br><span class="line">contexts:                    </span><br><span class="line">- context:  </span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kubernetes-admin</span><br><span class="line">  name: kubernetes-admin@kubernetes</span><br><span class="line">current-context: kubernetes-admin@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kubernetes-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: REDACTED</span><br><span class="line">    client-key-data: REDACTED</span><br></pre></td></tr></table></figure></div>

<p><code>client-certificate-data</code> 的部分默认是不显示的，而它的<strong>内容实际是通过 <code>base64</code> 加密后的证书内容</strong>。我们可以通过通过以下方式进行查看</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl config view users --raw -o jsonpath&#x3D;&#39;&#123; .users[?(@.name &#x3D;&#x3D; &quot;kubernetes-admin&quot;)].user.client-certificate-data&#125;&#39; |base64 -d  </span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIIC8jCCAdqgAwIBAgIIGuC27C9B8LIwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE</span><br><span class="line">...</span><br><span class="line">kae1A&#x2F;d4D5Cm5Qt7M5gr3SxqE5t+O7DP0YhuEPlfY7RzYDksYa8&#x3D;</span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line">➜  ~ kubectl config view users --raw -o jsonpath&#x3D;&#39;&#123; .users[?(@.name &#x3D;&#x3D; &quot;kubernetes-admin&quot;)].user.client-certificate-data&#125;&#39; |base64 -d |openssl x509 -text -noout  # 限于篇幅 省略部分输出</span><br><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 3 (0x2)</span><br><span class="line">        Serial Number: 1936748965290700978 (0x1ae0b6ec2f41f0b2)</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: CN&#x3D;kubernetes</span><br><span class="line">        Subject: O&#x3D;system:masters, CN&#x3D;kubernetes-admin</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></div>

<p>根据前面认证部分的内容，我们知道当前的用户是 <code>kubernetes-admin</code> （CN 域），所属组是 <code>system:masters</code> （O 域） 。</p>
<p>我们看下 <code>clusterrolebindings</code> 中的 <code>cluster-admin</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get clusterrolebindings  cluster-admin  -o yaml                         </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io&#x2F;autoupdate: &quot;true&quot;</span><br><span class="line">  ...</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io&#x2F;bootstrapping: rbac-defaults</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  resourceVersion: &quot;116&quot;</span><br><span class="line">  selfLink: &#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterrolebindings&#x2F;cluster-admin</span><br><span class="line">  uid: 71c550f1-e0e4-11e8-866a-fa163e938a99</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:masters</span><br></pre></td></tr></table></figure></div>

<p>重点内容在 <code>roleRef</code> 和 <code>subjects</code> 中，名为 <code>cluster-admin</code> 的 <code>ClusterRole</code> 与名为 <code>system:masters</code> 的 <code>Group</code> 相绑定。我们继续探究下它们所代表的含义。</p>
<p>先看看这个 <code>ClusterRole</code> 的实际内容：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get clusterrole cluster-admin -o yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io&#x2F;autoupdate: &quot;true&quot;</span><br><span class="line">  ...</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io&#x2F;bootstrapping: rbac-defaults</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  resourceVersion: &quot;58&quot;</span><br><span class="line">  selfLink: &#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterroles&#x2F;cluster-admin</span><br><span class="line">  uid: 71307108-e0e4-11e8-866a-fa163e938a99</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">  resources:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">  verbs:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">- nonResourceURLs:</span><br><span class="line">  - &#39;*&#39;</span><br><span class="line">  verbs:</span><br><span class="line">  - &#39;*&#39;</span><br></pre></td></tr></table></figure></div>

<p><code>rules</code> 中定义了它所能操作的资源及对应动作，<code>*</code> 是通配符。</p>
<p>到这里，我们就可以得出结论了，当前用户 <code>kubernetes-admin</code> 属于 <code>system:masters</code> 组，而这个组与 <code>cluster-admin</code> 这个 <code>ClusterRole</code> 所绑定，所以用户也就继承了其权限。具备了对多种资源和 API 的相关操作权限。</p>
<h2 id="实践：创建权限可控的用户"><a href="#实践：创建权限可控的用户" class="headerlink" title="实践：创建权限可控的用户"></a>实践：创建权限可控的用户</h2><p>前面是通过实际用户来反推它所具备的权限，接下来我们开始实践的部分，创建用户并为它进行授权。</p>
<p>我们要创建的用户名为 <code>backend</code> 所属组为 <code>dev</code>。</p>
<h3 id="创建-NameSpace"><a href="#创建-NameSpace" class="headerlink" title="创建 NameSpace"></a>创建 NameSpace</h3><p>为了演示，这里创建一个新的 <code>NameSpace</code> ，名为 <code>work</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl create namespace work</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">➜  ~ kubectl get ns work</span><br><span class="line">NAME   STATUS   AGE</span><br><span class="line">work   Active   14s</span><br></pre></td></tr></table></figure></div>

<h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><h4 id="创建私钥"><a href="#创建私钥" class="headerlink" title="创建私钥"></a>创建私钥</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ mkdir work</span><br><span class="line">➜  ~ cd work</span><br><span class="line">➜  work openssl genrsa -out backend.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">..........................................+++                            </span><br><span class="line">........................+++                   </span><br><span class="line">e is 65537 (0x10001)            </span><br><span class="line">➜  work ls                                       </span><br><span class="line">backend.key </span><br><span class="line"></span><br><span class="line">➜  work cat backend.key                                              </span><br><span class="line">-----BEGIN RSA PRIVATE KEY-----                                          </span><br><span class="line">MIIEpAIBAAKCAQEAzk7blZthwSzachPxrk6pHsuaImTVh6Iw8mNDmtn6sqOqBfZS</span><br><span class="line">...</span><br><span class="line">bNKDWDk8HZREugaOAwjt7xaWOlr9SPCCoXrWoaA1z2215IC4qSA2Nw&#x3D;&#x3D;</span><br><span class="line">-----END RSA PRIVATE KEY-----</span><br></pre></td></tr></table></figure></div>

<h4 id="使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定-subject-信息，传递用户名和组名"><a href="#使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定-subject-信息，传递用户名和组名" class="headerlink" title="使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定 subject 信息，传递用户名和组名"></a>使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定 <code>subject</code> 信息，传递用户名和组名</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  work openssl req -new -key backend.key -out backend.csr -subj &quot;&#x2F;CN&#x3D;backend&#x2F;O&#x3D;dev&quot;</span><br><span class="line">➜  work ls</span><br><span class="line">backend.csr  backend.key</span><br><span class="line">➜  work cat backend.csr</span><br><span class="line">-----BEGIN CERTIFICATE REQUEST-----</span><br><span class="line">MIICZTCCAU0CAQAwIDEQMA4GA1UEAwwHYmFja2VuZDEMMAoGA1UECgwDZGV2MIIB</span><br><span class="line">...</span><br><span class="line">lpoSVlNA0trJoiEiZjUqMfXX6ogBhQC4aeRfmbXkW2ZCNxsIm3PDk1Y&#x3D;</span><br><span class="line">-----END CERTIFICATE REQUEST-----</span><br></pre></td></tr></table></figure></div>

<h4 id="使用-CA-进行签名。K8S-默认的证书目录为-etc-kubernetes-pki。"><a href="#使用-CA-进行签名。K8S-默认的证书目录为-etc-kubernetes-pki。" class="headerlink" title="使用 CA 进行签名。K8S 默认的证书目录为 /etc/kubernetes/pki。"></a>使用 CA 进行签名。K8S 默认的证书目录为 <code>/etc/kubernetes/pki</code>。</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  work openssl x509 -req -in backend.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -out backend.crt -days 365</span><br><span class="line">Signature ok</span><br><span class="line">subject&#x3D;&#x2F;CN&#x3D;backend&#x2F;O&#x3D;dev</span><br><span class="line">Getting CA Private Key</span><br><span class="line">➜  work ls</span><br><span class="line">backend.crt  backend.csr  backend.key</span><br></pre></td></tr></table></figure></div>

<p>查看生成的证书文件</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  work openssl x509 -in backend.crt -text -noout</span><br><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 1 (0x0)</span><br><span class="line">        Serial Number:</span><br><span class="line">            d9:7f:62:f7:38:66:2a:7b</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: CN&#x3D;kubernetes</span><br><span class="line">        Subject: CN&#x3D;backend, O&#x3D;dev</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>CN</code> 域和 <code>O</code> 域已经正确设置</p>
<h4 id="添加-context"><a href="#添加-context" class="headerlink" title="添加 context"></a>添加 context</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  work kubectl config set-credentials backend --client-certificate&#x3D;&#x2F;root&#x2F;work&#x2F;backend.crt  --client-key&#x3D;&#x2F;root&#x2F;work&#x2F;backend.key</span><br><span class="line">User &quot;backend&quot; set.</span><br><span class="line">➜  work kubectl config set-context backend-context --cluster&#x3D;kubernetes --namespace&#x3D;work --user&#x3D;backend</span><br><span class="line">Context &quot;backend-context&quot; created.</span><br></pre></td></tr></table></figure></div>

<h4 id="使用新用户测试访问"><a href="#使用新用户测试访问" class="headerlink" title="使用新用户测试访问"></a>使用新用户测试访问</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">➜  work kubectl --context&#x3D;backend-context get pods</span><br><span class="line">Error from server (Forbidden): pods is forbidden: User &quot;backend&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;work&quot;</span><br><span class="line"># 可能看得不够清楚，我们添加 &#96;-v&#96; 参数来显示详情</span><br><span class="line">➜  work kubectl --context&#x3D;backend-context get pods -n work -v 5</span><br><span class="line">I1109 05:35:11.870639   18626 helpers.go:201] server response object: [&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;&#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;pods is forbidden: User \&quot;backend\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;work\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;pods&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;]</span><br><span class="line">F1109 05:35:11.870688   18626 helpers.go:119] Error from server (Forbidden): pods is forbidden: User &quot;backend&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;work&quot;</span><br></pre></td></tr></table></figure></div>

<p>可以看到已经使用了新的 <code>backend</code> 用户，并且默认的 <code>Namespace</code> 设置成了 <code>work</code>。</p>
<h4 id="创建-Role"><a href="#创建-Role" class="headerlink" title="创建 Role"></a>创建 Role</h4><p>我们想要让这个用户只具备查看 <code>Pod</code> 的权限。先来创建一个配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜  work cat &lt;&lt;EOF &gt; backend-role.yaml </span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: work</span><br><span class="line">  name: backend-role</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></div>

<p>创建并查看已生成的 <code>Role</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  work kubectl create -f backend-role.yaml </span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;backend-role created</span><br><span class="line">➜  work kubectl get roles  -n work -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">  kind: Role</span><br><span class="line">  metadata:</span><br><span class="line">    ...</span><br><span class="line">    name: backend-role</span><br><span class="line">    namespace: work</span><br><span class="line">    selfLink: &#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;namespaces&#x2F;work&#x2F;roles&#x2F;backend-role</span><br><span class="line">  rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">    - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">    - pods</span><br><span class="line">    verbs:</span><br><span class="line">    - get</span><br><span class="line">    - list</span><br><span class="line">    - watch</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure></div>

<h4 id="创建-Rolebinding"><a href="#创建-Rolebinding" class="headerlink" title="创建 Rolebinding"></a>创建 Rolebinding</h4><p>先创建一个配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  work cat &lt;&lt;EOF &gt; backend-rolebind.yaml</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: backend-rolebinding          </span><br><span class="line">  namespace: work</span><br><span class="line">subjects:      </span><br><span class="line">- kind: User</span><br><span class="line">  name: backend</span><br><span class="line">  apiGroup: &quot;&quot;     </span><br><span class="line">roleRef:    </span><br><span class="line">  kind: Role </span><br><span class="line">  name: backend-role</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></div>

<p>创建并查看已生成的 Rolebinding 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  work kubectl create -f backend-rolebind.yaml</span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;backend-rolebinding created</span><br><span class="line">➜  work kubectl get rolebinding -o yaml -n work</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">  kind: RoleBinding</span><br><span class="line">  metadata:</span><br><span class="line">    name: backend-rolebinding</span><br><span class="line">    namespace: work</span><br><span class="line">    ...</span><br><span class="line">  roleRef:</span><br><span class="line">    apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: Role</span><br><span class="line">    name: backend-role</span><br><span class="line">  subjects:</span><br><span class="line">  - apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: User</span><br><span class="line">    name: backend</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure></div>

<h4 id="测试用户权限"><a href="#测试用户权限" class="headerlink" title="测试用户权限"></a>测试用户权限</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  work kubectl --context&#x3D;backend-context get pods -n work</span><br><span class="line">No resources found.</span><br><span class="line">➜  work kubectl --context&#x3D;backend-context get ns</span><br><span class="line">Error from server (Forbidden): namespaces is forbidden: User &quot;backend&quot; cannot list resource &quot;namespaces&quot; in API group &quot;&quot; at the cluster scope</span><br><span class="line">➜  work kubectl --context&#x3D;backend-context get deploy -n work</span><br><span class="line">Error from server (Forbidden): deployments.extensions is forbidden: User &quot;backend&quot; cannot list resource &quot;deployments&quot; in API group &quot;extensions&quot; in the namespace &quot;work&quot;</span><br></pre></td></tr></table></figure></div>

<p>可以看到用户已经具备查看 <code>Pod</code> 的权限，但并不能查看 <code>Namespace</code> 或者 <code>deployment</code> 等其他资源。当然，K8S 也内置了一种很方便的调试机制。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  work kubectl auth can-i list pods -n work --as&#x3D;&quot;backend&quot;</span><br><span class="line">yes</span><br><span class="line">➜  work kubectl auth can-i list deploy -n work --as&#x3D;&quot;backend&quot;</span><br><span class="line">no - no RBAC policy matched</span><br></pre></td></tr></table></figure></div>

<p><code>--as</code> 是一种建立在 K8S 认证机制之上的机制，可以便于系统管理员验证授权情况，或进行调试。</p>
<p>你也可以仿照 <code>~/.kube/config</code> 文件的内容，将当前生成的证书及私钥文件等写入到配置文件中，通过指定 <code>KUBECONFIG</code> 的环境变量，或者给 <code>kubectl</code> 传递 <code>--kubeconfig</code> 参数来使用。</p>
<h2 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h2><p>本节中，我们学习了 K8S 的认证及授权逻辑，K8S 支持多种认证及授权模式，可按需使用。通过 X509 客户端证书认证的方式使用比较方便也比较推荐，在客户端证书的 <code>CN</code> 域和 <code>O</code> 域可以指定用户名和所属组名。</p>
<p>RBAC 的授权模式现在使用最多，可以通过对 <code>Role</code> 和 <code>subjects</code> (可以是用户或组) 进行绑定，以达到授权的目的。</p>
<p>最后，我们实际新创建了一个用户，并对其授予了预期的权限。在此过程中也涉及到了 <code>openssl</code> 客户端的常规操作，在之后也会常常用到。</p>
<p>下节，我们将开始部署实际的项目到 K8S 中，逐步掌握生成环境中对 K8S 的使用实践。</p>
<p>PS: 也许你会觉得切换 <code>Namespace</code> 之类的操作很繁琐，有一个项目：<a href="https://github.com/ahmetb/kubectx">kubectx</a> 可帮你简化这些步骤，推荐尝试。</p>
<p>留言</p>
<h1 id="应用发布-部署实际项目"><a href="#应用发布-部署实际项目" class="headerlink" title="应用发布: 部署实际项目"></a>应用发布: 部署实际项目</h1><p>本节我们开始学习如何将实际项目部署至 K8S 中，开启生产实践之路。</p>
<h2 id="整体概览-2"><a href="#整体概览-2" class="headerlink" title="整体概览"></a>整体概览</h2><p>本节所用示例项目是一个混合了 Go，NodeJS，Python 等语言的项目，灵感来自于知名程序员 <a href="https://github.com/kennethreitz">Kenneth Reitz</a> 的 <a href="https://saythanks.io/" target="_blank" rel="noopener">Say Thanks</a> 项目。本项目实现的功能主要有两个：1. 用户通过前端发送感谢消息 2. 有个工作进程会持续的计算收到感谢消息的排行榜。项目代码可在 <a href="https://github.com/tao12345666333/saythx">GitHub 上获得</a>。接下来几节中如果需要用到此项目我会统一称之为 saythx 项目。</p>
<p>saythx 项目的基础结构如下图：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/11/18/16722d23f282fa8b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h2><h3 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h3><p>我们使用了前端框架 <a href="https://vuejs.org/" target="_blank" rel="noopener">Vue</a>，所以在做生产部署时，需要先在 <a href="http://nodejs.org/" target="_blank" rel="noopener">Node JS</a> 的环境下进行打包构建。包管理器使用的是 <a href="https://yarnpkg.com/" target="_blank" rel="noopener">Yarn</a>。然后使用 <a href="http://nginx.com/" target="_blank" rel="noopener">Nginx</a> 提供服务，并进行反向代理，将请求正确的代理至后端。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FROM node:10.13 as builder</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;app</span><br><span class="line">COPY . &#x2F;app</span><br><span class="line"></span><br><span class="line">RUN yarn install \</span><br><span class="line">        &amp;&amp; yarn build</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FROM nginx:1.15</span><br><span class="line"></span><br><span class="line">COPY nginx.conf &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf</span><br><span class="line">COPY --from&#x3D;builder &#x2F;app&#x2F;dist &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;</span><br><span class="line"></span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure></div>

<p>Nginx 的配置文件如下：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">upstream backend-up &#123;</span><br><span class="line">    server saythx-backend:8080;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    charset     utf-8;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">        try_files $uri $uri&#x2F; &#x2F;index.html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ ^&#x2F;(api) &#123;</span><br><span class="line">        proxy_pass   http:&#x2F;&#x2F;backend-up;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>将 API 的请求反向代理到后端服务上。其余请求全部留给前端进行处理。</p>
<h3 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h3><p>后端是使用 <a href="http://golang.org/" target="_blank" rel="noopener">Golang</a> 编写的 API 服务，对请求进行相应处理，并将数据存储至 <a href="http://redis.io/" target="_blank" rel="noopener">Redis</a> 当中。依赖管理使用的是 <a href="https://github.com/golang/dep">dep</a>。由于 Golang 是编译型语言，编译完成后会生成一个二进制文件，为了让镜像尽可能小，所以 Dockerfile 和前端的差不多，都使用了<a href="https://docs.docker.com/develop/develop-images/multistage-build/" target="_blank" rel="noopener">多阶段构建</a>的特性。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.11.1 as builder</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;be</span><br><span class="line">COPY . &#x2F;go&#x2F;src&#x2F;be</span><br><span class="line">RUN go get -u github.com&#x2F;golang&#x2F;dep&#x2F;cmd&#x2F;dep \</span><br><span class="line">        &amp;&amp; dep ensure \</span><br><span class="line">        &amp;&amp; go build</span><br><span class="line"></span><br><span class="line">FROM debian:stretch-slim</span><br><span class="line">COPY --from&#x3D;builder &#x2F;go&#x2F;src&#x2F;be&#x2F;be &#x2F;usr&#x2F;bin&#x2F;be</span><br><span class="line">ENTRYPOINT [&quot;&#x2F;usr&#x2F;bin&#x2F;be&quot;]</span><br><span class="line">EXPOSE 8080</span><br></pre></td></tr></table></figure></div>

<p>注意这里会暴露出来后端服务所监听的端口。</p>
<h3 id="Work"><a href="#Work" class="headerlink" title="Work"></a>Work</h3><p>Work 端使用的是 <a href="http://python.org/" target="_blank" rel="noopener">Python</a>，用于计算已经存储至 Redis 当中的数据，并生成排行榜。依赖使用 <a href="https://github.com/pypa/pip">pip</a> 进行安装。对于 Python 的镜像选择，我做了一组<a href="http://moelove.info/docker-python-perf/" target="_blank" rel="noopener">性能对比的测试</a> 有兴趣可以了解下。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM python:3.7-slim</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;app</span><br><span class="line">COPY . &#x2F;app</span><br><span class="line">RUN pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [&quot;python&quot;, &quot;work.py&quot;]</span><br></pre></td></tr></table></figure></div>

<h3 id="构建发布"><a href="#构建发布" class="headerlink" title="构建发布"></a>构建发布</h3><p>接下来，我们只要在对应项目目录中，执行 <code>docker build [OPTIONS] PATH</code> 即可。一般我们会使用 <code>-t name:tag</code> 的方式打 tag。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 以下分别是在各模块自己的目录内</span><br><span class="line">➜  be docker build -t  taobeier&#x2F;saythx-be .</span><br><span class="line">➜  fe docker build -t  taobeier&#x2F;saythx-fe .</span><br><span class="line">➜  work docker build -t  taobeier&#x2F;saythx-work .</span><br></pre></td></tr></table></figure></div>

<p>需要注意的是，前端项目由于目录内包含开发时的 <code>node_modules</code> 等文件，需要注意添加 <code>.dockerignore</code> 文件，忽略一些非预期的文件。关于 Docker 的 build 原理，有想深入理解的，可参考我之前写的 <a href="http://moelove.info/2018/09/04/Docker-深入篇之-Build-原理/" target="_blank" rel="noopener">Docker 深入篇之 Build 原理</a> 。 当镜像构建完成后，我们需要将它们发布至镜像仓库。这里我们直接使用官方的 <a href="http://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>，执行 <code>docker login</code> 输入用户名密码验证成功后便可进行发布（需要先去 Docker Hub 注册帐号）。</p>
<p>登录成功后，默认情况下在 <code>$HOME/.docker/config.json</code> 中会存储用户相关凭证。</p>
<p>接下来进行发布只需要执行 <code>docker push</code> 即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker push taobeier&#x2F;saythx-be</span><br><span class="line">➜  ~ docker push taobeier&#x2F;saythx-fe</span><br><span class="line">➜  ~ docker push taobeier&#x2F;saythx-work</span><br></pre></td></tr></table></figure></div>

<h2 id="容器编排-Docker-Compose"><a href="#容器编排-Docker-Compose" class="headerlink" title="容器编排 Docker Compose"></a>容器编排 Docker Compose</h2><p><a href="https://docs.docker.com/compose/overview/" target="_blank" rel="noopener">Docker Compose</a> 是一种较为简单的可进行容器编排的技术，需要创建一个配置文件，通常情况下为 <code>docker-compose.yml</code> 。在 saythx 项目的根目录下我已经创建好了 <code>docker-compose.yml</code> 的配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">version: &#39;3&#39;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  saythx-frontend:</span><br><span class="line">    build:</span><br><span class="line">      context: fe&#x2F;.</span><br><span class="line">    image: taobeier&#x2F;saythx-fe</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8088:80&quot;</span><br><span class="line">    depends_on:</span><br><span class="line">      - saythx-backend</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line"></span><br><span class="line">  saythx-backend:</span><br><span class="line">    build:</span><br><span class="line">      context: be&#x2F;.</span><br><span class="line">    image: taobeier&#x2F;saythx-be</span><br><span class="line">    depends_on:</span><br><span class="line">      - saythx-redis</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line">    environment:</span><br><span class="line">      - REDIS_HOST&#x3D;saythx-redis</span><br><span class="line">    </span><br><span class="line">  saythx-work:</span><br><span class="line">    build:</span><br><span class="line">      context: work&#x2F;.</span><br><span class="line">    image: taobeier&#x2F;saythx-work</span><br><span class="line">    depends_on:</span><br><span class="line">      - saythx-redis</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line">    environment:</span><br><span class="line">      - REDIS_HOST&#x3D;saythx-redis</span><br><span class="line">      - REDIS_PORT&#x3D;6379</span><br><span class="line"></span><br><span class="line">  saythx-redis:</span><br><span class="line">    image: &quot;redis:5&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - saythx</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  saythx:</span><br></pre></td></tr></table></figure></div>

<p>在项目的根目录下执行 <code>docker-compose up</code> 即可启动该项目。在浏览器中访问 <a href="http://127.0.0.1:8088/" target="_blank" rel="noopener">http://127.0.0.1:8088/</a> 即可看到项目的前端界面。如下图：</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="628"></svg>)</p>
<p>打开另外的终端，进入项目根目录内，执行 <code>docker-compose ps</code> 命令即可看到当前的服务情况。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  saythx git:(master) ✗ docker-compose ps</span><br><span class="line"></span><br><span class="line">          Name                        Command               State          Ports</span><br><span class="line">----------------------------------------------------------------------------------------</span><br><span class="line">saythx_saythx-backend_1    &#x2F;usr&#x2F;bin&#x2F;be                      Up      8080&#x2F;tcp</span><br><span class="line">saythx_saythx-frontend_1   nginx -g daemon off;             Up      0.0.0.0:8088-&gt;80&#x2F;tcp</span><br><span class="line">saythx_saythx-redis_1      docker-entrypoint.sh redis ...   Up      6379&#x2F;tcp</span><br><span class="line">saythx_saythx-work_1       python work.py                   Up</span><br></pre></td></tr></table></figure></div>

<p>可以看到各组件均是 <code>Up</code> 的状态，相关端口也已经暴露出来。</p>
<p>可在浏览器直接访问体验。由于 <code>Docker Compose</code> 并非本册的重点，故不做太多介绍，可参考官方文档进行学习。接下来进入本节的重点内容，将项目部署至 K8S 中。</p>
<h2 id="编写配置文件并部署"><a href="#编写配置文件并部署" class="headerlink" title="编写配置文件并部署"></a>编写配置文件并部署</h2><p>在 K8S 中进行部署或者说与 K8S 交互的方式主要有三种：</p>
<ul>
<li>命令式</li>
<li>命令式对象配置</li>
<li>声明式对象配置</li>
</ul>
<p>第 7 节介绍过的 <code>kubectl run redis --image=&#39;redis:alpine&#39;</code> 这种方式便是命令式，这种方式很简单，但是可重用性低。毕竟你的命令执行完后，其他人也并不清楚到底发生了什么。</p>
<p>命令式对象配置，主要是编写配置文件，但是通过类似 <code>kubectl create</code> 之类命令式的方式进行操作。</p>
<p>再有一种便是声明式对象配置，主要也是通过编写配置文件，但是使用 <code>kubectl apply</code> 之类的放好似进行操作。与第二种命令式对象配置的区别主要在于对对象的操作将会得到保留。但同时这种方式有时候也并不好进行调试。</p>
<p>接下来，为 saythx 项目编写配置文件，让它可以部署至 K8S 中。当然，这里我们已经创建过了 <code>docker-compose.yml</code> 的配置文件，并且也验证了其可用性，可以直接使用 <a href="https://github.com/kubernetes/kompose">Kompose</a> 工具将 <code>docker-compose.yml</code> 的配置文件进行转换。</p>
<p>但这里采用直接编写的方式。同时，我们部署至一个新的名为 <code>work</code> 的 <code>Namespace</code> 中。</p>
<h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: work</span><br></pre></td></tr></table></figure></div>

<p>指定了 <code>Namespace</code> name 为 <code>work</code>。然后进行部署</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f namespace.yaml </span><br><span class="line">namespace&#x2F;work created</span><br></pre></td></tr></table></figure></div>

<h3 id="Redis-资源"><a href="#Redis-资源" class="headerlink" title="Redis 资源"></a>Redis 资源</h3><p>从前面的 <code>docker-compose.yml</code> 中也能发现，saythx 中各个组件，只有 Redis 是无任何依赖的。我们先对它进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: redis</span><br><span class="line">  name: saythx-redis</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: redis</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: redis</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: redis:5</span><br><span class="line">        name: redis</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 6379</span><br></pre></td></tr></table></figure></div>

<p>由于这是本册内第一次出现完整的 <code>Deployment</code> 配置文件，故而进行重点介绍。</p>
<ul>
<li><code>apiVersion</code> ：指定了 API 的版本号，当前我们使用的 K8S 中， <code>Deployment</code> 的版本号为 <code>apps/v1</code>，而在 1.9 之前使用的版本则为 <code>apps/v1beta2</code>，在 1.8 之前的版本使用的版本为 <code>extensions/v1beta1</code>。在编写配置文件时需要格外注意。</li>
<li><code>kind</code> ：指定了资源的类型。这里指定为 <code>Deployment</code> 说明是一次部署。</li>
<li><code>metadata</code> ：指定了资源的元信息。例如其中的 <code>name</code> 和 <code>namespace</code> 分别表示资源名称和所归属的 <code>Namespace</code>。</li>
<li><code>spec</code> ：指定了对资源的配置信息。例如其中的 <code>replicas</code> 指定了副本数当前指定为 1 。<code>template.spec</code> 则指定了 <code>Pod</code> 中容器的配置信息，这里的 <code>Pod</code> 中只部署了一个容器。</li>
</ul>
<p>配置文件已经生产，现在对它进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">No resources found.</span><br><span class="line">➜  conf git:(master) ✗ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-79d8f9864d-x8fp9   1&#x2F;1       Running   0          4s</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            1           4s</span><br><span class="line"></span><br><span class="line">NAME                                      DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-79d8f9864d   1         1         1         4s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 已经在正常运行了。我们进入 <code>Pod</code> 内进行测试。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl -n work exec -it saythx-redis-79d8f9864d-x8fp9 bash</span><br><span class="line">root@saythx-redis-79d8f9864d-x8fp9:&#x2F;data# redis-cli</span><br><span class="line">127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure></div>

<p>响应正常。</p>
<h3 id="Redis-service"><a href="#Redis-service" class="headerlink" title="Redis service"></a>Redis service</h3><p>由于 <code>Redis</code> 是后端服务的依赖，我们将它作为 <code>Service</code> 暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: redis</span><br><span class="line">  name: saythx-redis</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 6379</span><br><span class="line">    targetPort: 6379</span><br><span class="line">  selector:</span><br><span class="line">    app: redis</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>关于 <code>Service</code> 的内容，可参考第 7 节，我们详细做过解释。这里直接使用配置文件进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br><span class="line">➜  conf git:(master) ✗ kubectl get svc -n work</span><br><span class="line">NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">saythx-redis   NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   1m</span><br></pre></td></tr></table></figure></div>

<h3 id="后端服务"><a href="#后端服务" class="headerlink" title="后端服务"></a>后端服务</h3><p>接下来，我们对后端服务进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: backend</span><br><span class="line">  name: saythx-backend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: backend</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: backend</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - env:</span><br><span class="line">        - name: REDIS_HOST</span><br><span class="line">          value: saythx-redis</span><br><span class="line">        image: taobeier&#x2F;saythx-be</span><br><span class="line">        name: backend</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br></pre></td></tr></table></figure></div>

<p>可以看到这里通过环境变量的方式，将 <code>REDIS_HOST</code> 传递给了后端服务。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f backend-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-backend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                 READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-backend-c5f9f6d95-lmtxn   0&#x2F;1       ContainerCreating   0          5s</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-kcmzk     1&#x2F;1       Running             0          17m</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   1m</span><br><span class="line"></span><br><span class="line">NAME                             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-backend   1         1         1            0           5s</span><br><span class="line">deployment.apps&#x2F;saythx-redis     1         1         1            1           17m</span><br><span class="line"></span><br><span class="line">NAME                                       DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-backend-c5f9f6d95   1         1         0         5s</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d     1         1         1         17m</span><br></pre></td></tr></table></figure></div>

<h3 id="后端-Service"><a href="#后端-Service" class="headerlink" title="后端 Service"></a>后端 Service</h3><p>后端服务是前端项目的依赖，故而我们也将其作为 <code>Service</code> 暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: backend</span><br><span class="line">  name: saythx-backend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: backend</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>通过配置文件进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f backend-service.yaml</span><br><span class="line">service&#x2F;saythx-backend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl get svc -n work</span><br><span class="line">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-backend   NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   8s</span><br><span class="line">service&#x2F;saythx-redis     NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   3m</span><br></pre></td></tr></table></figure></div>

<p>我们同样使用 <code>NodePort</code> 将其暴露出来，并在本地进行测试。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ curl http:&#x2F;&#x2F;127.0.0.1:32051&#x2F;api&#x2F;v1&#x2F;list</span><br><span class="line">&#123;&quot;HonorDatas&quot;:null&#125;</span><br></pre></td></tr></table></figure></div>

<p>服务可正常响应。</p>
<h3 id="前端-1"><a href="#前端-1" class="headerlink" title="前端"></a>前端</h3><p>接下来我们编写前端的配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: frontend</span><br><span class="line">  name: saythx-frontend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: frontend</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: frontend</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: taobeier&#x2F;saythx-fe</span><br><span class="line">        name: frontend</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure></div>

<p>需要注意的是，我们必须在后端 <code>Service</code> 暴露出来后才能进行前端的部署，因为前端镜像中 Nginx 的反向代理配置中会去检查后端是否可达。使用配置文件进行部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f frontend-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-frontend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-backend-c5f9f6d95-lmtxn     1&#x2F;1       Running   0          16m</span><br><span class="line">pod&#x2F;saythx-frontend-678d544b86-wp9gr   1&#x2F;1       Running   0          30s</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-kcmzk       1&#x2F;1       Running   0          34m</span><br><span class="line"></span><br><span class="line">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-backend   NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   15m</span><br><span class="line">service&#x2F;saythx-redis     NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   18m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-backend    1         1         1            1           16m</span><br><span class="line">deployment.apps&#x2F;saythx-frontend   1         1         1            1           30s</span><br><span class="line">deployment.apps&#x2F;saythx-redis      1         1         1            1           34m</span><br><span class="line"></span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-backend-c5f9f6d95     1         1         1         16m</span><br><span class="line">replicaset.apps&#x2F;saythx-frontend-678d544b86   1         1         1         30s</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d       1         1         1         34m</span><br></pre></td></tr></table></figure></div>

<h3 id="前端-Service"><a href="#前端-Service" class="headerlink" title="前端 Service"></a>前端 Service</h3><p>接下来，我们需要让前端可以被直接访问到，同样的需要将它以 <code>Service</code> 的形式暴露出来。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: frontend</span><br><span class="line">  name: saythx-frontend</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: &quot;80&quot;</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: frontend</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>创建 <code>Service</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f frontend-service.yaml</span><br><span class="line">service&#x2F;saythx-frontend created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get svc</span><br><span class="line">NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">saythx-backend    NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   17m</span><br><span class="line">saythx-frontend   NodePort   10.96.221.71    &lt;none&gt;        80:32682&#x2F;TCP     11s</span><br><span class="line">saythx-redis      NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   20m</span><br></pre></td></tr></table></figure></div>

<p>我们可以直接通过 Node 的 32682 端口进行访问。</p>
<h3 id="Work-1"><a href="#Work-1" class="headerlink" title="Work"></a>Work</h3><p>最后，是我们的 Work 组件，为它编写配置文件。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: work</span><br><span class="line">  name: saythx-work</span><br><span class="line">  namespace: work</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: work</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: work</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - env:</span><br><span class="line">        - name: REDIS_HOST</span><br><span class="line">          value: saythx-redis</span><br><span class="line">        - name: REDIS_PORT</span><br><span class="line">          value: &quot;6379&quot;</span><br><span class="line">        image: taobeier&#x2F;saythx-work</span><br><span class="line">        name: work</span><br></pre></td></tr></table></figure></div>

<p>同样的，我们通过环境变量的方式传递了 Redis 相关的配置进去。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl apply -f work-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-work created</span><br><span class="line">➜  conf git:(master) ✗ kubectl -n work get all</span><br><span class="line">NAME                                   READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-backend-c5f9f6d95-lmtxn     1&#x2F;1       Running             0          22m</span><br><span class="line">pod&#x2F;saythx-frontend-678d544b86-wp9gr   1&#x2F;1       Running             0          5m</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-kcmzk       1&#x2F;1       Running             0          39m</span><br><span class="line">pod&#x2F;saythx-work-6b9958dc47-hh9td       0&#x2F;1       ContainerCreating   0          7s</span><br><span class="line"></span><br><span class="line">NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-backend    NodePort   10.104.0.47     &lt;none&gt;        8080:32051&#x2F;TCP   20m</span><br><span class="line">service&#x2F;saythx-frontend   NodePort   10.96.221.71    &lt;none&gt;        80:32682&#x2F;TCP     3m</span><br><span class="line">service&#x2F;saythx-redis      NodePort   10.107.31.242   &lt;none&gt;        6379:31467&#x2F;TCP   23m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-backend    1         1         1            1           22m</span><br><span class="line">deployment.apps&#x2F;saythx-frontend   1         1         1            1           5m</span><br><span class="line">deployment.apps&#x2F;saythx-redis      1         1         1            1           39m</span><br><span class="line">deployment.apps&#x2F;saythx-work       1         1         1            0           7s</span><br><span class="line"></span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-backend-c5f9f6d95     1         1         1         22m</span><br><span class="line">replicaset.apps&#x2F;saythx-frontend-678d544b86   1         1         1         5m</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d       1         1         1         39m</span><br><span class="line">replicaset.apps&#x2F;saythx-work-6b9958dc47       1         1         0         7s</span><br></pre></td></tr></table></figure></div>

<p>现在均已经部署完成。并且可直接通过 Node 端口进行访问。</p>
<h2 id="扩缩容"><a href="#扩缩容" class="headerlink" title="扩缩容"></a>扩缩容</h2><p>如果我们觉得排行榜生成效率较低，则可通过扩容 Work 来得到解决。具体做法是可修改 work 的 <code>Deployment</code> 配置文件，将 <code>spec.replicas</code> 设置为预期的数值，之后执行 <code>kubectl -f work-deployment.yaml</code> 即可。</p>
<p>或者可直接通过命令行进行操作</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  conf git:(master) ✗ kubectl -n work scale --replicas&#x3D;2  deployment&#x2F;saythx-work</span><br></pre></td></tr></table></figure></div>

<p>上面的命令是将 <code>saythx-work</code> 的部署副本数设置为 2 。缩容也差不多是类似的操作。</p>
<h2 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h2><p>通过本节的学习，我们已经学习到了如何将项目实际部署至 K8S 中，可以手动编写配置也可以利用一些工具进行辅助。同时也了解到了如何应对应用的扩缩容。</p>
<p>但如果应用需要进行升级的话，则需要去更改配置文件中相关的配置，这个过程会较为繁琐，并且整体项目线上的版本管理也是个问题：比如组件的个人升级，回滚之间如果有依赖的话，会比较麻烦。我们在接下来的两节来学习如何解决这个问题。</p>
<h1 id="应用管理-初识-Helm"><a href="#应用管理-初识-Helm" class="headerlink" title="应用管理: 初识 Helm"></a>应用管理: 初识 Helm</h1><h2 id="整体概览-3"><a href="#整体概览-3" class="headerlink" title="整体概览"></a>整体概览</h2><p>上节，我们已经学习了如何通过编写配置文件的方式部署项目。而在实际生产环境中，项目所包含组件可能不止 3 个，并且可能项目数会很多，如果每个项目的发布，更新等都通过手动去编写配置文件的方式，实在不利于管理。</p>
<p>并且，当线上出现个别组件升级回滚之类的操作，如果组件之间有相关版本依赖等情况，那事情会变得复杂的多。我们需要有更简单的机制来辅助我们完成这些事情。</p>
<h2 id="Helm-介绍"><a href="#Helm-介绍" class="headerlink" title="Helm 介绍"></a>Helm 介绍</h2><p><a href="https://www.helm.sh/" target="_blank" rel="noopener">Helm</a> 是构建于 K8S 之上的包管理器，可与我们平时接触到的 <code>Yum</code>，<code>APT</code>，<code>Homebrew</code> 或者 <code>Pip</code> 等包管理器相类比。</p>
<p>使用 Helm 可简化包分发，安装，版本管理等操作流程。同时它也是 CNCF 孵化项目。</p>
<h2 id="Helm-安装"><a href="#Helm-安装" class="headerlink" title="Helm 安装"></a>Helm 安装</h2><p>Helm 是 C/S 架构，主要分为客户端 <code>helm</code> 和服务端 <code>Tiller</code>。安装时可直接在 <a href="https://github.com/helm/helm/releases">Helm 仓库的 Release 页面</a> 下载所需二进制文件或者源码包。</p>
<p>由于当前项目的二进制文件存储已切换为 GCS，我已经为国内用户准备了最新版本的二进制包，可通过以下链接进行下载。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1n1zj3rlv2NyfiA6kRGrHfg 提取码: 5huw</span><br></pre></td></tr></table></figure></div>

<p>下载后对文件进行解压，我这里以 Linux amd64 为例。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">➜  &#x2F;tmp tar -zxvf helm-v2.11.0-linux-amd64.tar.gz</span><br><span class="line">linux-amd64&#x2F;</span><br><span class="line">linux-amd64&#x2F;tiller</span><br><span class="line">linux-amd64&#x2F;README.md</span><br><span class="line">linux-amd64&#x2F;helm</span><br><span class="line">linux-amd64&#x2F;LICENSE</span><br><span class="line">➜  &#x2F;tmp tree linux-amd64 </span><br><span class="line">linux-amd64</span><br><span class="line">├── helm</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">└── tiller</span><br><span class="line"></span><br><span class="line">0 directories, 4 files</span><br></pre></td></tr></table></figure></div>

<p>解压完成后，可看到其中包含 <code>helm</code> 和 <code>tiller</code> 二进制文件。</p>
<h3 id="客户端-helm"><a href="#客户端-helm" class="headerlink" title="客户端 helm"></a>客户端 helm</h3><p><code>helm</code> 是个二进制文件，直接将它移动至 <code>/usr/bin</code> 目录下即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  &#x2F;tmp sudo mv linux-amd64&#x2F;helm &#x2F;usr&#x2F;bin&#x2F;helm</span><br></pre></td></tr></table></figure></div>

<p>这时候便可直接通过 <code>helm</code> 命令使用了。比如，我们验证当前使用的版本。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  &#x2F;tmp helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Error: Get http:&#x2F;&#x2F;localhost:8080&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;pods?labelSelector&#x3D;app%3Dhelm%2Cname%3Dtiller: dial tcp 127.0.0.1:8080: connect: connection refused</span><br></pre></td></tr></table></figure></div>

<p>可以看到上面有明显的报错，并且很像 <code>kubectl</code> 未正确配置时的错误。这是因为 <code>helm</code> 默认会去读取 <code>$HOME/.kube/config</code> 的配置文件，用于正确的连接至目标集群。</p>
<p>当我们正确的配置好 <code>$HOME/.kube/config</code> 文件时，再次执行：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  &#x2F;tmp helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Error: could not find tiller</span><br></pre></td></tr></table></figure></div>

<p>这次报错是因为找不到服务端 <code>Tiller</code>，接下来我们部署服务端。</p>
<h3 id="服务端-Tiller"><a href="#服务端-Tiller" class="headerlink" title="服务端 Tiller"></a>服务端 Tiller</h3><p>以下讨论中，前提都是 <code>$HOME/.kube/config</code> 已正确配置，并且 <code>kebectl</code> 有操作集群的权限。</p>
<h4 id="本地安装"><a href="#本地安装" class="headerlink" title="本地安装"></a>本地安装</h4><p>刚才我们解压的文件中，还有一个二进制文件 <code>tiller</code> 。我们可以直接执行它，用于在本地启动服务。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  &#x2F;tmp .&#x2F;linux-amd64&#x2F;tiller</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Starting Tiller v2.11.0 (tls&#x3D;false)</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 GRPC listening on :44134</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Probes listening on :44135</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Storage driver is ConfigMap</span><br><span class="line">[main] 2018&#x2F;11&#x2F;18 23:47:10 Max history per release is 0</span><br></pre></td></tr></table></figure></div>

<p>直接执行时，默认会监听 <code>44134</code> 和 <code>44135</code> 端口，<code>44134</code> 端口用于和 <code>helm</code> 进行通信，而 <code>44135</code> 主要是用于做探活的，在部署至 K8S 时使用。</p>
<p>当我们使用客户端连接时，只需设置 <code>HELM_HOST</code> 环境变量即可。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ export HELM_HOST&#x3D;localhost:44134</span><br><span class="line">➜  ~ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>注意</strong> 一定要正确配置 <code>$HOME/.kube/config</code> 文件，否则会影响正常功能使用。</p>
<h4 id="默认安装"><a href="#默认安装" class="headerlink" title="默认安装"></a>默认安装</h4><p>官方提供了一种一键式安装的方式。那便是 <code>helm init</code> 执行这条命令后，会同时在 K8S 中部署服务端 Tiller 和初始化 helm 的默认目录 <code>$HELM_HOME</code> 默认值为 <code>$HOME/.helm</code>。</p>
<p>这种方式下会默认使用官方镜像 <code>gcr.io/kubernetes-helm/tiller</code> 网络原因可能会导致安装失败。所以我已将官方镜像进行同步。可使用以下方式进行使用：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ helm init --tiller-image taobeier&#x2F;tiller:v2.11.0 </span><br><span class="line">Creating &#x2F;root&#x2F;.helm</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;cache</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;local</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;plugins</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;starters</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;cache&#x2F;archive</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;repositories.yaml</span><br><span class="line">Adding stable repo with URL: https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com</span><br><span class="line">Adding local repo with URL: http:&#x2F;&#x2F;127.0.0.1:8879&#x2F;charts</span><br><span class="line">$HELM_HOME has been configured at &#x2F;root&#x2F;.helm.</span><br><span class="line"></span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line"></span><br><span class="line">Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy.</span><br><span class="line">To prevent this, run &#96;helm init&#96; with the --tiller-tls-verify flag.</span><br><span class="line">For more information on securing your installation see: https:&#x2F;&#x2F;docs.helm.sh&#x2F;using_helm&#x2F;#securing-your-helm-installation</span><br><span class="line">Happy Helming!</span><br><span class="line">➜  ~ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>$HELM_HOME</code> 目录已经初始化完成，客户端与服务端已可以正常通信。查看下当前 K8S 集群中的情况：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl -n kube-system get deploy tiller-deploy</span><br><span class="line">NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">tiller-deploy   1         1         1            1           6m</span><br></pre></td></tr></table></figure></div>

<p>可以看到已正常部署。</p>
<h4 id="手动安装"><a href="#手动安装" class="headerlink" title="手动安装"></a>手动安装</h4><p>通过上面的描述，可能你已经发现，安装服务端，其实也就是一次普通的部署，我们可以通过以下方式来自行通过 <code>kubectl</code> 完成部署。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ helm init --dry-run --debug  # 篇幅原因，以下内容进行了省略</span><br><span class="line">---                               </span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment                                    </span><br><span class="line">metadata:                            </span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:         </span><br><span class="line">    app: helm              </span><br><span class="line">    name: tiller       </span><br><span class="line">  name: tiller-deploy           </span><br><span class="line">  namespace: kube-system   </span><br><span class="line">spec:               </span><br><span class="line">  replicas: 1 </span><br><span class="line">  strategy: &#123;&#125;                </span><br><span class="line">  ...</span><br><span class="line">status: &#123;&#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: helm</span><br><span class="line">    name: tiller</span><br><span class="line">  name: tiller-deploy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: tiller</span><br><span class="line">    port: 44134</span><br><span class="line">    targetPort: tiller</span><br><span class="line">  selector:</span><br><span class="line">    app: helm</span><br><span class="line">    name: tiller</span><br><span class="line">  type: ClusterIP</span><br><span class="line">status:</span><br><span class="line">  loadBalancer: &#123;&#125;</span><br></pre></td></tr></table></figure></div>

<p>将输出内容保存至文件中，自行修改后，通过 <code>kubectl</code> 进行部署即可。建议在修改过程中，尽量不要去更改标签及选择器。</p>
<h4 id="RBAC-使用"><a href="#RBAC-使用" class="headerlink" title="RBAC 使用"></a>RBAC 使用</h4><p>上面的内容中，均未提及到权限控制相关的内容，但是在生产环境中使用，我们一般都是会进行权限控制的。</p>
<p>在第 8 节中，我们已经详细的解释了认证授权相关的内容。所以下面的内容不做太多详细解释。</p>
<p>这里我们创建一个 <code>ServiceAccount</code> 命名为 <code>tiller</code>，为了简单，我们直接将它与 <code>cluster-admin</code> 进行绑定。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: tiller</span><br><span class="line">    namespace: kube-system</span><br></pre></td></tr></table></figure></div>

<p>将此内容保存为 <code>tiller-rbac.yaml</code>，开始进行部署操作。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f tiller-rbac.yaml</span><br><span class="line">serviceaccount&#x2F;tiller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;tiller created</span><br><span class="line">➜  ~ helm init --service-account tiller</span><br><span class="line">Creating &#x2F;root&#x2F;.helm</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;cache</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;local</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;plugins</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;starters</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;cache&#x2F;archive</span><br><span class="line">Creating &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;repositories.yaml</span><br><span class="line">Adding stable repo with URL: https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com</span><br><span class="line">Adding local repo with URL: http:&#x2F;&#x2F;127.0.0.1:8879&#x2F;charts</span><br><span class="line">$HELM_HOME has been configured at &#x2F;root&#x2F;.helm.</span><br><span class="line"></span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line"></span><br><span class="line">Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy.</span><br><span class="line">To prevent this, run &#96;helm init&#96; with the --tiller-tls-verify flag.</span><br><span class="line">For more information on securing your installation see: https:&#x2F;&#x2F;docs.helm.sh&#x2F;using_helm&#x2F;#securing-your-helm-installation</span><br><span class="line">Happy Helming!</span><br><span class="line">➜  ~ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>以此方式完成部署。</p>
<h2 id="Helm-概念"><a href="#Helm-概念" class="headerlink" title="Helm 概念"></a>Helm 概念</h2><h3 id="Chart"><a href="#Chart" class="headerlink" title="Chart"></a>Chart</h3><p><code>chart</code> 就是 Helm 所管理的包，类似于 <code>Yum</code> 所管理的 <code>rpm</code> 包或是 <code>Homebrew</code> 管理的 <code>Formulae</code>。它包含着一个应用要部署至 K8S 上所必须的所有资源。</p>
<h3 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h3><p><code>Release</code> 就是 <code>chart</code> 在 K8S 上部署后的实例。<code>chart</code> 的每次部署都将产生一次 <code>Release</code>。这和上面类比的包管理器就有所不同了，多数的系统级包管理器所安装的包只会在系统中存在一份。我们可以以 <code>Pip</code> 在虚拟环境下的包安装，或者 <code>Npm</code> 的 local install 来进行类比。</p>
<h3 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h3><p><code>Repository</code> 就是字面意思，存储 <code>chart</code> 的仓库。还记得我们上面执行 <code>helm init</code> 时的输出吗？默认情况下，初始化 Helm 的时候，会添加两个仓库，一个是 <code>stable</code> 仓库 <a href="https://kubernetes-charts.storage.googleapis.com/" target="_blank" rel="noopener">kubernetes-charts.storage.googleapis.com</a> 另一个则是 <code>local</code> 仓库，地址是 <a href="http://127.0.0.1:8879/charts" target="_blank" rel="noopener">http://127.0.0.1:8879/charts</a> 。</p>
<h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><p>前面提到了 <code>chart</code> 是应用程序所必须的资源，当然我们实际部署的时候，可能就需要有些自定义的配置了。<code>Config</code> 便是用于完成此项功能的，在部署时候，会将 <code>config</code> 与 <code>chart</code> 进行合并，共同构成我们将部署的应用。</p>
<h2 id="Helm-的工作原理"><a href="#Helm-的工作原理" class="headerlink" title="Helm 的工作原理"></a>Helm 的工作原理</h2><p><code>helm</code> 通过 <code>gRPC</code> 将 <code>chart</code> 发送至 <code>Tiller</code> ，<code>Tiller</code> 则通过内置的 <code>kubernetes</code> 客户端库与 K8S 的 API server 进行交流，将 <code>chart</code> 进行部署，并生成 <code>Release</code> 用于管理。</p>
<p>前面只说到了 <code>helm</code> 与 <code>Tiller</code> 交互的协议，但尚未说其数据链路。</p>
<p>我们来看看 <code>Tiller</code> 的部署情况。主要看 <code>Service</code>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl -n kube-system get svc</span><br><span class="line">NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kube-dns        ClusterIP   10.96.0.10       &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP   1h</span><br><span class="line">tiller-deploy   ClusterIP   10.107.204.164   &lt;none&gt;        44134&#x2F;TCP       33m</span><br></pre></td></tr></table></figure></div>

<p><code>Tiller</code> 默认采用 <code>ClusterIP</code> 类型的 <code>Service</code> 进行部署。而我们知道的 <code>ClusterIP</code> 类型的 <code>Service</code> 是仅限集群内访问的。</p>
<p>在这里所依赖的技术，便是在第 5 节，我们提到的 <code>socat</code> 。<code>helm</code> 通过 <code>socat</code> 的端口转发（或者说 K8S 的代理），进而实现了本地与 <code>Tiller</code> 的通信。</p>
<p>当然，以上内容均以当前最新版本 <code>2.11.0</code> 为例。当下一个大版本 Helm v3 出现时， <code>Tiller</code> 将不复存在，通信机制和工作原理也将发生变化。</p>
<h2 id="总结-8"><a href="#总结-8" class="headerlink" title="总结"></a>总结</h2><p>通过本节，我们已经学习到了 Helm 的基础知识和工作原理，了解到了 Helm 的用途以及如何在本地和 K8S 中部署它。需要注意的是 <code>$HOME/.kube/config</code> 需要提前配置好，以及 <code>socat</code> 工具需要提前安装，可参考第 5 节的内容。</p>
<p>接下来，我们将上节中的示例项目使用 Helm 部署至 K8S 集群中。</p>
<h1 id="部署实践-以-Helm-部署项目"><a href="#部署实践-以-Helm-部署项目" class="headerlink" title="部署实践: 以 Helm 部署项目"></a>部署实践: 以 Helm 部署项目</h1><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>上节，我们学习到了 Helm 的基础概念和工作原理，本节我们将 Helm 用于我们的实际项目，编写 Helm <code>chart</code> 以及通过 Helm 进行部署。</p>
<h2 id="Helm-chart"><a href="#Helm-chart" class="headerlink" title="Helm chart"></a>Helm chart</h2><p>上节我们解释过 <code>chart</code> 的含义，现在我们要将项目使用 Helm 部署，那么首先，我们需要创建一个 <code>chart</code>。</p>
<h3 id="Chart-结构"><a href="#Chart-结构" class="headerlink" title="Chart 结构"></a>Chart 结构</h3><p>在我们项目的根目录下，通过以下命令创建一个 <code>chart</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  saythx git:(master) helm create saythx</span><br><span class="line">Creating saythx</span><br><span class="line">➜  saythx git:(master) ✗ tree -a saythx</span><br><span class="line">saythx</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── .helmignore</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">2 directories, 8 files</span><br></pre></td></tr></table></figure></div>

<p>创建完成后，我们可以看到默认创建的 <code>chart</code> 中包含了几个文件和目录。我们先对其进行解释。</p>
<h4 id="Chart-yaml"><a href="#Chart-yaml" class="headerlink" title="Chart.yaml"></a>Chart.yaml</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  saythx git:(master) ✗ cat saythx&#x2F;Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for Kubernetes</span><br><span class="line">name: saythx</span><br><span class="line">version: 0.1.0</span><br></pre></td></tr></table></figure></div>

<p>这个文件是每个 <code>chart</code> 必不可少的一个文件，其中包含着几个重要的属性，如：</p>
<ul>
<li><code>apiVersion</code>：目前版本都为 <code>v1</code></li>
<li><code>appVersion</code>：这是应用的版本号，需要与 <code>apiVersion</code>， <code>version</code> 等字段注意区分</li>
<li><code>name</code>: 通常要求 <code>chart</code> 的名字必须和它所在目录保持一致，且此字段必须</li>
<li><code>version</code>：表明当前 <code>chart</code> 的版本号，会直接影响 <code>Release</code> 的记录，且此字段必须</li>
<li><code>description</code>：描述</li>
</ul>
<h4 id="charts"><a href="#charts" class="headerlink" title="charts"></a>charts</h4><p><code>charts</code> 文件夹是用于存放依赖的 <code>chart</code> 的。当有依赖需要管理时，可添加 <code>requirements.yaml</code> 文件，可用于管理项目内或者外部的依赖。</p>
<h4 id="helmignore"><a href="#helmignore" class="headerlink" title=".helmignore"></a>.helmignore</h4><p><code>.helmignore</code> 类似于 <code>.gitignore</code> 和 <code>.dockerignore</code> 之类的，用于忽略掉一些不想包含在 <code>chart</code> 内的文件。</p>
<h4 id="templates"><a href="#templates" class="headerlink" title="templates"></a>templates</h4><p><code>templates</code> 文件夹内存放着 <code>chart</code> 所使用的模板文件，也是 <code>chart</code> 的实际执行内容。在使用 <code>chart</code> 进行安装的时候，会将 下面介绍的 <code>values.yaml</code> 中的配置项与 <code>templates</code> 中的模板进行组装，生成最终要执行的配置文件。</p>
<p><code>templates</code> 中，推荐命名应该清晰，如 <code>xx-deployment.yaml</code>，中间使用 <code>-</code> 进行分割，避免使用驼峰式命名。</p>
<p><code>Notes.txt</code> 文件在 <code>helm install</code> 完成后，会进行回显，可用于解释说明如何访问服务等。</p>
<h4 id="values-yaml"><a href="#values-yaml" class="headerlink" title="values.yaml"></a>values.yaml</h4><p><code>values.yaml</code> 存放着项目的一些可配置项，如镜像的名称或者 tag 之类的。作用就是用于和模板进行组装。</p>
<h3 id="编写-chart"><a href="#编写-chart" class="headerlink" title="编写 chart"></a>编写 chart</h3><p>了解完结构之后，我们来实际编写我们的 chart 。所有完整的代码可在 <a href="https://github.com/tao12345666333/saythx">SayThx 项目</a> 获取。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for SayThx.</span><br><span class="line">name: saythx</span><br><span class="line">version: 0.1.0</span><br><span class="line">maintainers:</span><br><span class="line">  - name: Jintao Zhang</span><br></pre></td></tr></table></figure></div>

<p>可添加 <code>maintainers</code> 字段，表示维护者。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># values.yaml</span><br><span class="line"></span><br><span class="line"># backend is the values for backend</span><br><span class="line">backend:</span><br><span class="line">  image: taobeier&#x2F;saythx-be</span><br><span class="line">  tag: &quot;1.0&quot;</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  replicas: 1</span><br><span class="line"></span><br><span class="line"># namespace is the values for deploy namespace</span><br><span class="line">namespace: work</span><br><span class="line"></span><br><span class="line"># service.type is the values for service type</span><br><span class="line">service:</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p><code>values.yaml</code> 文件中定义了我们预期哪些东西是可配置的，比如 <code>namespace</code> 以及镜像名称 tag 等。这里只是贴出了部分内容，仅做说明使用，完整内容可查看我们的<a href="https://github.com/tao12345666333/saythx">示例项目</a> 。</p>
<p>写 <code>values.yaml</code> 文件的时候，由于是使用 <code>YAML</code> 格式的配置，所以它非常的灵活，即可以使用如上面例子中的 <code>backend</code> 那种字典类型的， 也可以写成简单的 k-v 形式。但通常来讲，应该尽可能的将它写的清晰明确。并且容易被替换。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># templates&#x2F;backend-service.yaml </span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: backend</span><br><span class="line">  name: saythx-backend</span><br><span class="line">  namespace: &#123;&#123; .Values.namespace &#125;&#125;</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: backend</span><br><span class="line">  type: &#123;&#123; .Values.service.type &#125;&#125;</span><br></pre></td></tr></table></figure></div>

<p>将我们之前写的部署文件模板化，与配置项进行组装。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">&#123;&#123;- if contains &quot;NodePort&quot; .Values.service.type &#125;&#125;</span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace &#123;&#123; .Values.namespace &#125;&#125; -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services saythx-frontend)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace &#123;&#123; .Values.namespace &#125;&#125; -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br><span class="line">&#123;&#123;- else if contains &quot;ClusterIP&quot; .Values.service.type &#125;&#125;</span><br><span class="line">  export POD_NAME&#x3D;$(kubectl get pods --namespace &#123;&#123; .Values.namespace &#125;&#125; -l &quot;app&#x3D;frontend&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http:&#x2F;&#x2F;127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl --namespace &#123;&#123; .Values.namespace &#125;&#125; port-forward $POD_NAME 8080:80</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure></div>

<p>上面这是 <code>NOTES.txt</code> 文件内的内容。 这些内容会在 <code>helm install</code> 执行成功后显示在终端，用于说明服务如何访问或者其他注意事项等。</p>
<p>当然，这里的内容主要是为了说明如何编写 <code>chart</code> ，在实践中，尽量避免硬编码配置在里面。</p>
<h2 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h2><h3 id="直接部署"><a href="#直接部署" class="headerlink" title="直接部署"></a>直接部署</h3><p>Helm 的 <code>chart</code> 可以直接在源码目录下通过 <code>helm install</code> 完成部署。例如：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">➜  saythx helm install saythx</span><br><span class="line">NAME:   handy-seastar</span><br><span class="line">LAST DEPLOYED: Tue Nov 20 23:33:42 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Namespace</span><br><span class="line">NAME  STATUS  AGE</span><br><span class="line">work  Active  1s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME             TYPE      CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE</span><br><span class="line">saythx-backend   NodePort  10.102.206.213  &lt;none&gt;       8080:30663&#x2F;TCP  0s</span><br><span class="line">saythx-frontend  NodePort  10.96.109.45    &lt;none&gt;       80:30300&#x2F;TCP    0s</span><br><span class="line">saythx-redis     NodePort  10.97.174.8     &lt;none&gt;       6379:30589&#x2F;TCP  0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Deployment</span><br><span class="line">NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">saythx-backend   1        1        1           0          0s</span><br><span class="line">saythx-frontend  1        1        1           0          0s</span><br><span class="line">saythx-redis     1        1        1           0          0s</span><br><span class="line">saythx-work      1        1        1           0          0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                              READY  STATUS             RESTARTS  AGE</span><br><span class="line">saythx-backend-7f6d86d9c8-xqttg   0&#x2F;1    ContainerCreating  0         0s</span><br><span class="line">saythx-frontend-777fc64997-9zmq6  0&#x2F;1    Pending            0         0s</span><br><span class="line">saythx-redis-8558c7d7d-lh5df      0&#x2F;1    ContainerCreating  0         0s</span><br><span class="line">saythx-work-9b4446d84-c2pr4       0&#x2F;1    ContainerCreating  0         0s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace work -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services saythx-frontend)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace work -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br></pre></td></tr></table></figure></div>

<h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><p>当然，我们也可以将 <code>chart</code> 打包，以便于分发。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  saythx helm package saythx </span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;root&#x2F;saythx&#x2F;saythx-0.1.0.tgz</span><br></pre></td></tr></table></figure></div>

<p>可以看到打包时是按照 <code>chart</code> 的名字加版本号进行命名的。</p>
<p>至于部署，和前面没什么太大区别， <code>helm install saythx-0.1.0.tgz</code> 即可。</p>
<h3 id="访问服务"><a href="#访问服务" class="headerlink" title="访问服务"></a>访问服务</h3><p>前面在部署完成后，有一些返回信息，我们来按照其内容访问我们的服务：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  saythx export NODE_PORT&#x3D;$(kubectl get --namespace work -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services saythx-frontend)</span><br><span class="line">➜  saythx export NODE_IP&#x3D;$(kubectl get nodes --namespace work -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">➜  saythx echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br><span class="line">http:&#x2F;&#x2F;172.17.0.5:30300</span><br><span class="line">➜  saythx curl http:&#x2F;&#x2F;172.17.0.5:30300</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;en&gt;&lt;head&gt;&lt;meta charset&#x3D;utf-8&gt;&lt;meta http-equiv&#x3D;X-UA-Compatible content&#x3D;&quot;IE&#x3D;edge&quot;&gt;&lt;meta name&#x3D;viewport content&#x3D;&quot;width&#x3D;device-width,initial-scale&#x3D;1&quot;&gt;&lt;link rel&#x3D;icon href&#x3D;&#x2F;favicon.ico&gt;&lt;title&gt;fe&lt;&#x2F;title&gt;&lt;link href&#x3D;&#x2F;css&#x2F;app.0a6f0b04.css rel&#x3D;preload as&#x3D;style&gt;&lt;link href&#x3D;&#x2F;css&#x2F;chunk-vendors.ea3fa8e3.css rel&#x3D;preload as&#x3D;style&gt;&lt;link href&#x3D;&#x2F;js&#x2F;app.ee469174.js rel&#x3D;preload as&#x3D;script&gt;&lt;link href&#x3D;&#x2F;js&#x2F;chunk-vendors.14b9b088.js rel&#x3D;preload as&#x3D;script&gt;&lt;link href&#x3D;&#x2F;css&#x2F;chunk-vendors.ea3fa8e3.css rel&#x3D;stylesheet&gt;&lt;link href&#x3D;&#x2F;css&#x2F;app.0a6f0b04.css rel&#x3D;stylesheet&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;noscript&gt;&lt;strong&gt;We&#39;re sorry but fe doesn&#39;t work properly without JavaScript enabled. Please enable it to continue.&lt;&#x2F;strong&gt;&lt;&#x2F;noscript&gt;&lt;div id&#x3D;app&gt;&lt;&#x2F;div&gt;&lt;script src&#x3D;&#x2F;js&#x2F;chunk-vendors.14b9b088.js&gt;&lt;&#x2F;script&gt;&lt;script src&#x3D;&#x2F;js&#x2F;app.ee469174.js&gt;&lt;&#x2F;script&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure></div>

<p>服务可以正常访问。</p>
<h2 id="总结-9"><a href="#总结-9" class="headerlink" title="总结"></a>总结</h2><p>通过本节我们学习到了 <code>chart</code> 的实际结构，及编写方式。以及编写了我们自己的 <code>chart</code> 并使用该 <code>chart</code> 部署了服务。</p>
<p>示例项目还仅仅是个小项目，试想当我们需要部署一个大型项目，如果不通过类似 Helm 这样的软件进行管理，每次的更新发布，维护 <code>YAML</code> 的配置文件就会很繁琐了。</p>
<p>另外，Helm 的功能还不仅限于此，使用 Helm 我们还可以管理 <code>Release</code> ，并进行更新回滚等操作。以及，我们可以搭建自己的私有 <code>chart</code> 仓库等。</p>
<p>下节开始，我们将进入深入学习阶段，逐个讲解 K8S 的核心组件，以便后续遇到问题时，可快速定位和解决。</p>
<h1 id="庖丁解牛：kube-apiserver"><a href="#庖丁解牛：kube-apiserver" class="headerlink" title="庖丁解牛：kube-apiserver"></a>庖丁解牛：kube-apiserver</h1><h2 id="整体概览-4"><a href="#整体概览-4" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们初次认识到了 <code>kube-apiserver</code> 的存在（以下内容中将统一称之为 <code>kube-apiserver</code>），知道了它作为集群的统一入口，接收来自外部的信号和请求，并将一些信息存储至 <code>etcd</code> 中。</p>
<p>但这只是一种很模糊的说法，本节我们来具体看看 <code>kube-apiserver</code> 的关键功能以及它的工作原理。</p>
<p>注意：本节所有的源码均以 <code>v1.11.3</code> 为准 commit id <code>a4529464e4629c21224b3d52edfe0ea91b072862</code>。</p>
<h2 id="REST-API-Server"><a href="#REST-API-Server" class="headerlink" title="REST API Server"></a>REST API Server</h2><p>先来说下 <code>kube-apiserver</code> 作为整个集群的入口，接受外部的信号和请求所应该具备的基本功能。</p>
<p>首先，它对外提供接口，可处理来自客户端（无论我们在用的 <code>kubeclt</code> 或者 <code>curl</code> 或者其他语言实现的客户端）的请求，并作出响应。</p>
<p>在第 5 节搭建集群时，我们提到要先去检查 <code>6443</code> 端口是否被占用。这样检查的原因在于 <code>kube-apiserver</code> 有个 <code>--secure-port</code> 的参数，通过这个参数来配置它将要监听在哪个端口，默认情况下是 <code>6443</code>。</p>
<p>当然，它还有另一个参数 <code>--insecure-port</code> ，这个参数可将 <code>kube-apiserver</code> 绑定到其指定的端口上，且通过该端口访问时无需认证。</p>
<p>在生产环境中，建议将其设置为 <code>0</code> 以禁用该功能。另外，这个参数也已经被标记为废弃，将在之后版本中移除。如果未禁用该功能，建议通过防火墙策略禁止从外部访问该端口。该端口会绑定在 <code>--insecure-bind-address</code> 参数所设置的地址上，默认为 <code>127.0.0.1</code>。</p>
<p>那么 <code>secure</code> 和 <code>insecure</code> 最主要的区别是什么呢？ 这就引出来了 <code>kube-apiserver</code> 作为 API Server 的一个最主要功能：认证。</p>
<h3 id="认证（Authentication）-1"><a href="#认证（Authentication）-1" class="headerlink" title="认证（Authentication）"></a>认证（Authentication）</h3><p>在第 8 节《认证和授权》中，我们已经讲过认证相关的机制。这里，我们以最简单的获取集群版本号为例。</p>
<p>通常，我们使用 <code>kubeclt version</code> 来获取集群和当前客户端的版本号。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:53:03Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>获取集群版本号的时候，其实也是向 <code>kube-apiserver</code> 发送了一个请求进行查询的，我们可以通过传递 <code>-v</code> 参数来改变 log level 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl version -v 8</span><br><span class="line">I1202 03:15:06.360838   13581 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:15:06.362106   13581 round_trippers.go:383] GET https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;version?timeout&#x3D;32s</span><br><span class="line">I1202 03:15:06.362130   13581 round_trippers.go:390] Request Headers:</span><br><span class="line">I1202 03:15:06.362139   13581 round_trippers.go:393]     Accept: application&#x2F;json, *&#x2F;*</span><br><span class="line">I1202 03:15:06.362146   13581 round_trippers.go:393]     User-Agent: kubectl&#x2F;v1.11.3 (linux&#x2F;amd64) kubernetes&#x2F;a452946</span><br><span class="line">I1202 03:15:06.377653   13581 round_trippers.go:408] Response Status: 200 OK in 15 milliseconds</span><br><span class="line">I1202 03:15:06.377678   13581 round_trippers.go:411] Response Headers:</span><br><span class="line">I1202 03:15:06.377686   13581 round_trippers.go:414]     Content-Type: application&#x2F;json</span><br><span class="line">I1202 03:15:06.377693   13581 round_trippers.go:414]     Content-Length: 263</span><br><span class="line">I1202 03:15:06.377699   13581 round_trippers.go:414]     Date: Sun, 02 Dec 2018 03:15:06 GMT</span><br><span class="line">I1202 03:15:06.379314   13581 request.go:897] Response Body: &#123;</span><br><span class="line">  &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;minor&quot;: &quot;11&quot;,</span><br><span class="line">  &quot;gitVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;gitCommit&quot;: &quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;,</span><br><span class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">  &quot;buildDate&quot;: &quot;2018-09-09T17:53:03Z&quot;,</span><br><span class="line">  &quot;goVersion&quot;: &quot;go1.10.3&quot;,</span><br><span class="line">  &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">  &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">&#125;</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:53:03Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure></div>

<p>通过日志就可以很明显看到，首先会加载 <code>$HOME/.kube/config</code> 下的配置，获的集群地址，进而请求 <code>/version</code> 接口，最后格式化输出。</p>
<p>我们使用 <code>curl</code> 去请求同样的接口：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">master $ curl -k https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;version</span><br><span class="line">&#123;</span><br><span class="line">  &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;minor&quot;: &quot;11&quot;,</span><br><span class="line">  &quot;gitVersion&quot;: &quot;v1.11.3&quot;,</span><br><span class="line">  &quot;gitCommit&quot;: &quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;,</span><br><span class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">  &quot;buildDate&quot;: &quot;2018-09-09T17:53:03Z&quot;,</span><br><span class="line">  &quot;goVersion&quot;: &quot;go1.10.3&quot;,</span><br><span class="line">  &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">  &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>得到了相同的结果。你可能会有些奇怪，使用 <code>curl -k</code> 相当于忽略了认证的过程，为何还能拿到正确的信息。别急，我们来看下一个例子：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl get ns  -v 8</span><br><span class="line">I1202 03:25:40.607886   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.608862   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.611187   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.622737   16620 loader.go:359] Config loaded from file &#x2F;root&#x2F;.kube&#x2F;config</span><br><span class="line">I1202 03:25:40.623495   16620 round_trippers.go:383] GET https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;api&#x2F;v1&#x2F;namespaces?limit&#x3D;500</span><br><span class="line">I1202 03:25:40.623650   16620 round_trippers.go:390] Request Headers:</span><br><span class="line">I1202 03:25:40.623730   16620 round_trippers.go:393]     Accept: application&#x2F;json;as&#x3D;Table;v&#x3D;v1beta1;g&#x3D;meta.k8s.io, application&#x2F;json</span><br><span class="line">I1202 03:25:40.623820   16620 round_trippers.go:393]     User-Agent: kubectl&#x2F;v1.11.3 (linux&#x2F;amd64) kubernetes&#x2F;a452946</span><br><span class="line">I1202 03:25:40.644280   16620 round_trippers.go:408] Response Status: 200 OK in 20 milliseconds</span><br><span class="line">I1202 03:25:40.644308   16620 round_trippers.go:411] Response Headers:</span><br><span class="line">I1202 03:25:40.644327   16620 round_trippers.go:414]     Content-Type: application&#x2F;json</span><br><span class="line">I1202 03:25:40.644334   16620 round_trippers.go:414]     Content-Length: 2061</span><br><span class="line">I1202 03:25:40.644338   16620 round_trippers.go:414]     Date: Sun, 02 Dec 2018 03:25:40 GMT</span><br><span class="line">I1202 03:25:40.644398   16620 request.go:897] Response Body: &#123;&quot;kind&quot;:&quot;Table&quot;,&quot;apiVersion&quot;:&quot;meta.k8s.io&#x2F;v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;&#x2F;api&#x2F;v1&#x2F;namespaces&quot;,&quot;resourceVersion&quot;:&quot;3970&quot;&#125;,&quot;columnDefinitions&quot;:[&#123;&quot;name&quot;:&quot;Name&quot;,&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;name&quot;,&quot;description&quot;:&quot;Name must be unique within anamespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;user-guide&#x2F;identifiers#names&quot;,&quot;priority&quot;:0&#125;,&#123;&quot;name&quot;:&quot;Status&quot;,&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;&quot;,&quot;description&quot;:&quot;The status of the namespace&quot;,&quot;priority&quot;:0&#125;,&#123;&quot;name&quot;:&quot;Age&quot;,&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;&quot;,&quot;description&quot;:&quot;CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\n\nPopulated by the system. Read-only. [truncated 1037 chars]</span><br><span class="line">I1202 03:25:40.645111   16620 get.go:443] no kind is registered for the type v1beta1.Table</span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">default       Active    45m</span><br><span class="line">kube-public   Active    45m</span><br><span class="line">kube-system   Active    45m</span><br></pre></td></tr></table></figure></div>

<p>使用 <code>curl</code> 去请求：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">master $ curl -k  https:&#x2F;&#x2F;172.17.0.99:6443&#x2F;api&#x2F;v1&#x2F;namespaces</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;namespaces is forbidden: User \&quot;system:anonymous\&quot; cannot list namespaces at the cluster scope&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;namespaces&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>看到这里，应该就很明显了，当前忽略掉认证过程的 <code>curl</code> 被判定为 <code>system:anonymous</code> 用户，而此用户不具备列出 <code>namespace</code> 的权限。</p>
<p>那我们是否有其他办法使用 <code>curl</code> 获取资源呢？ 当然有，使用 <code>kubectl proxy</code> 可以在本地和集群之间创建一个代理，就像这样：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl proxy &amp;</span><br><span class="line">[1] 22205</span><br><span class="line">master $ Starting to serve on 127.0.0.1:8001</span><br><span class="line"></span><br><span class="line">master $ curl http:&#x2F;&#x2F;127.0.0.1:8001&#x2F;api&#x2F;v1&#x2F;namespaces</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;NamespaceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;5363&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;a5124131-f5db-11e8-9237-0242ac110063&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;4&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2018-12-02T02:40:35Z&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">          &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;kube-public&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-public&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;a5153f73-f5db-11e8-9237-0242ac110063&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;10&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2018-12-02T02:40:35Z&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">          &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;kube-system&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;a514ad25-f5db-11e8-9237-0242ac110063&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;9&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2018-12-02T02:40:35Z&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">          &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>可以看到已经能正确的获取资源了，这是因为 <code>kubectl proxy</code> 使用了 <code>$HOME/.kube/config</code> 中的配置。</p>
<p>在 <code>staging/src/k8s.io/client-go/tools/clientcmd/loader.go</code> 中，有一个名为 <code>LoadFromFile</code> 的函数用来提供加载配置文件的功能。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">func LoadFromFile(filename string) (*clientcmdapi.Config, error) &#123;</span><br><span class="line">	kubeconfigBytes, err :&#x3D; ioutil.ReadFile(filename)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	config, err :&#x3D; Load(kubeconfigBytes)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	glog.V(6).Infoln(&quot;Config loaded from file&quot;, filename)</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; set LocationOfOrigin on every Cluster, User, and Context</span><br><span class="line">	for key, obj :&#x3D; range config.AuthInfos &#123;</span><br><span class="line">		obj.LocationOfOrigin &#x3D; filename</span><br><span class="line">		config.AuthInfos[key] &#x3D; obj</span><br><span class="line">	&#125;</span><br><span class="line">	for key, obj :&#x3D; range config.Clusters &#123;</span><br><span class="line">		obj.LocationOfOrigin &#x3D; filename</span><br><span class="line">		config.Clusters[key] &#x3D; obj</span><br><span class="line">	&#125;</span><br><span class="line">	for key, obj :&#x3D; range config.Contexts &#123;</span><br><span class="line">		obj.LocationOfOrigin &#x3D; filename</span><br><span class="line">		config.Contexts[key] &#x3D; obj</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if config.AuthInfos &#x3D;&#x3D; nil &#123;</span><br><span class="line">		config.AuthInfos &#x3D; map[string]*clientcmdapi.AuthInfo&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	if config.Clusters &#x3D;&#x3D; nil &#123;</span><br><span class="line">		config.Clusters &#x3D; map[string]*clientcmdapi.Cluster&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	if config.Contexts &#x3D;&#x3D; nil &#123;</span><br><span class="line">		config.Contexts &#x3D; map[string]*clientcmdapi.Context&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return config, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>逻辑其实很简单，读取指定的文件（一般在调用此函数前，都会先去检查是否有 <code>KUBECONFIG</code> 的环境变量或 <code>--kubeconfig</code>，如果没有才会使用默认的 <code>$HOME/.kube/config</code> 作为文件名）。</p>
<p>从以上的例子中，使用当前配置的用户可以获取资源，而 <code>system:anonymous</code> 不可以。可以得出 <code>kube-apiserver</code> 又一个重要的功能：授权。</p>
<h3 id="授权（Authorization）-1"><a href="#授权（Authorization）-1" class="headerlink" title="授权（Authorization）"></a>授权（Authorization）</h3><p>在第 8 节中，我们也已经讲过，K8S 支持多种授权机制，现在多数都在使用 <code>RBAC</code> ，我们之前使用 <code>kubeadm</code> 创建集群时，默认会开启 <code>RBAC</code>。如何创建权限可控的用户在第 8 节也已经说过。所以本节中不过多赘述了，直接看授权后的处理逻辑。</p>
<h3 id="准入控制（Admission-Control）"><a href="#准入控制（Admission-Control）" class="headerlink" title="准入控制（Admission Control）"></a>准入控制（Admission Control）</h3><p>在请求进来时，会先经过认证、授权接下来会进入准入控制环节。准入控制和前两项内容不同，它不只是关注用户和行为，它还会处理请求的内容。不过它对读操作无效。</p>
<p>准入控制与我们前面说提到的认证、授权插件类似，支持同时开启多个。在 <code>v1.11.3</code> 中，默认开启的准入控制插件有：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeClaimResize,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,Priority</span><br></pre></td></tr></table></figure></div>

<p>相关的代码可查看 <code>pkg/kubeapiserver/options/plugins.go</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">func DefaultOffAdmissionPlugins() sets.String &#123;</span><br><span class="line">	defaultOnPlugins :&#x3D; sets.NewString(</span><br><span class="line">		lifecycle.PluginName,                &#x2F;&#x2F;NamespaceLifecycle</span><br><span class="line">		limitranger.PluginName,              &#x2F;&#x2F;LimitRanger</span><br><span class="line">		serviceaccount.PluginName,           &#x2F;&#x2F;ServiceAccount</span><br><span class="line">		setdefault.PluginName,               &#x2F;&#x2F;DefaultStorageClass</span><br><span class="line">		resize.PluginName,                   &#x2F;&#x2F;PersistentVolumeClaimResize</span><br><span class="line">		defaulttolerationseconds.PluginName, &#x2F;&#x2F;DefaultTolerationSeconds</span><br><span class="line">		mutatingwebhook.PluginName,          &#x2F;&#x2F;MutatingAdmissionWebhook</span><br><span class="line">		validatingwebhook.PluginName,        &#x2F;&#x2F;ValidatingAdmissionWebhook</span><br><span class="line">		resourcequota.PluginName,            &#x2F;&#x2F;ResourceQuota</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	if utilfeature.DefaultFeatureGate.Enabled(features.PodPriority) &#123;</span><br><span class="line">		defaultOnPlugins.Insert(podpriority.PluginName) &#x2F;&#x2F;PodPriority</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return sets.NewString(AllOrderedPlugins...).Difference(defaultOnPlugins)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在这里写了一些默认开启的配置。事实上，在早之前，<code>PersistentVolumeClaimResize</code> 默认是不开启的，并且开启了 <code>PersistentVolumeLabel</code>，对于移除 <code>Persistentvolumelabel</code> 感兴趣的朋友可以参考下 <a href="https://github.com/kubernetes/kubernetes/issues/52617">Remove the PersistentVolumeLabel Admission Controller</a> 。</p>
<p>这里对几个比较常见的插件做下说明：</p>
<ul>
<li><p>NamespaceLifecycle：它可以保证正在终止的 <code>Namespace</code> 不允许创建对象，不允许请求不存在的 <code>Namespace</code> 以及保证默认的 <code>default</code>, <code>kube-system</code> 之类的命名空间不被删除。核心的代码是：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if a.GetOperation() &#x3D;&#x3D; admission.Delete &amp;&amp; a.GetKind().GroupKind() &#x3D;&#x3D; v1.SchemeGroupVersion.WithKind(&quot;Namespace&quot;).GroupKind() &amp;&amp; l.immortalNamespaces.Has(a.GetName()) &#123;</span><br><span class="line">	return errors.NewForbidden(a.GetResource().GroupResource(), a.GetName(), fmt.Errorf(&quot;this namespace may not be deleted&quot;))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>如果删除默认的 <code>Namespace</code> 则会得到下面的异常：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl delete ns kube-system</span><br><span class="line">Error from server (Forbidden): namespaces &quot;kube-system&quot; is forbidden: this namespace may not be deleted</span><br><span class="line">master $ kubectl delete ns kube-public</span><br><span class="line">Error from server (Forbidden): namespaces &quot;kube-public&quot; is forbidden: this namespace may not be deleted</span><br><span class="line">master $ kubectl delete ns default</span><br><span class="line">Error from server (Forbidden): namespaces &quot;default&quot; is forbidden: this namespace may not be deleted</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>LimitRanger：为 <code>Pod</code> 设置默认请求资源的限制。</p>
</li>
<li><p>ServiceAccount：可按照预设规则创建 <code>Serviceaccount</code> 。比如都有统一的前缀：<code>system:serviceaccount:</code>。</p>
</li>
<li><p>DefaultStorageClass：为 <code>PVC</code> 设置默认 <code>StorageClass</code>。</p>
</li>
<li><p>DefaultTolerationSeconds：设置 <code>Pod</code> 的默认 forgiveness toleration 为 5 分钟。这个可能常会看到。</p>
</li>
<li><p>MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook：这两个都是通过 Webhook 验证或者修改请求，唯一的区别是一个是顺序进行，一个是并行进行的。</p>
</li>
<li><p>ResourceQuota：限制 <code>Pod</code> 请求配额。</p>
</li>
<li><p>AlwaysPullImages：总是拉取镜像。</p>
</li>
<li><p>AlwaysAdmit：总是接受所有请求。</p>
</li>
</ul>
<h3 id="处理请求"><a href="#处理请求" class="headerlink" title="处理请求"></a>处理请求</h3><p>前面已经说到，一个请求依次会经过认证，授权，准入控制等环节，当这些环节都已经通过后，该请求便到了 <code>kube-apiserver</code> 的实际处理逻辑中了。</p>
<p>其实和普通的 Web server 类似，<code>kube-apiserver</code> 提供了 <code>restful</code> 的接口，增删改查等基本功能都基本类似。这里先暂时不再深入。</p>
<h2 id="总结-10"><a href="#总结-10" class="headerlink" title="总结"></a>总结</h2><p>通过本节，我们学习到了 <code>kube-apiserver</code> 的基本工作逻辑，各类 API 请求先后通过认证，授权，准入控制等一系列环节后，进入到 <code>kube-apiserver</code> 的 <code>Registry</code> 进行相关逻辑处理。</p>
<p>至于需要进行持久化或者需要与后端存储交互的部分，我们在下节会介绍 <code>etcd</code> 到时再看 K8S 是如何将后端存储抽象化，从 <code>etcd</code> v2 升级至 v3 的。</p>
<p><code>kube-apiserver</code> 包含的东西有很多，当你在终端下执行 <code>./kube-apiserver -h</code> 时，会发现有大量的参数。</p>
<p>这些参数除了认证，授权，准入控制相关功能</p>
<h1 id="庖丁解牛：etcd"><a href="#庖丁解牛：etcd" class="headerlink" title="庖丁解牛：etcd"></a>庖丁解牛：etcd</h1><h2 id="整体概览-5"><a href="#整体概览-5" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>etcd</code> 的存在，知道了 Master 是 K8S 是集群的大脑，而 <code>etcd</code> 则是大脑的核心。为什么这么说？本节我们一同来看看 <code>etcd</code> 为何如此重要。</p>
<h2 id="etcd-是什么"><a href="#etcd-是什么" class="headerlink" title="etcd 是什么"></a><code>etcd</code> 是什么</h2><p>先摘录<a href="https://etcd.readthedocs.io/en/latest/faq.html#what-is-etcd" target="_blank" rel="noopener">官方文档</a>的一句说明:</p>
<blockquote>
<p>etcd is a consistent distributed key-value store. Mainly used as a separate coordination service, in distributed systems. And designed to hold small amounts of data that can fit entirely in memory.</p>
</blockquote>
<p><code>etcd</code> 是由 CoreOS 团队发起的一个分布式，强一致的键值存储。它用 Go 语言编写，使用 <code>Raft</code> 协议作为一致性算法。多数情况下会用于分布式系统中的服务注册发现，或是用于存储系统的关键数据。</p>
<h2 id="etcd-有什么作用"><a href="#etcd-有什么作用" class="headerlink" title="etcd 有什么作用"></a><code>etcd</code> 有什么作用</h2><p><code>etcd</code> 在 K8S 中，最主要的作用便是其高可用，强一致的键值存储以及监听机制。</p>
<p>在 <code>kube-apiserver</code> 收到对应请求经过一系列的处理后，最终如果是集群所需要存储的数据，便会存储至 <code>etcd</code> 中。主部分主要是集群状态信息和元信息。</p>
<p>我们来实际操作 K8S 集群中的 <code>etcd</code> 看下真实情况。</p>
<ul>
<li>查找集群中的 <code>etcd</code> Pod</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 默认集群中的 etcd 会放到 kube-system Namespace 中</span><br><span class="line">master $ kubectl -n kube-system get pods | grep etcd</span><br><span class="line">etcd-master                      1&#x2F;1       Running   0          1h</span><br></pre></td></tr></table></figure></div>

<ul>
<li>进入该 Pod 并查看 <code>etcd</code> 集群的 <code>member</code></li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system exec -it etcd-master sh</span><br><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt member list</span><br><span class="line">a874c87fd42044f, started, master, https:&#x2F;&#x2F;127.0.0.1:2380, https:&#x2F;&#x2F;127.0.0.1:2379</span><br></pre></td></tr></table></figure></div>

<p>这里由于在 K8S 1.11.3 中默认使用的是 <code>etcd</code> 3.2 版本，所以需要加入 <code>ETCDCTL_API=3</code> 的环境变量，且 <code>etcd</code> 从 2 到 3 很明显的一个变化也是使用上的变化，在 2 中是 HTTP 接口的。</p>
<p>我们通过传递证书等相关参数进去，完成校验，查看 <code>member</code> 。</p>
<ul>
<li>查看存储的元信息</li>
</ul>
<p><code>etcd</code> 中存储的 K8S 集群元信息基本都是 <code>/registry</code> 下，我们可通过下面的方式进行查看</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt get &#x2F;registry --prefix --keys-only</span><br><span class="line">&#x2F;registry&#x2F;apiregistration.k8s.io&#x2F;apiservices&#x2F;v1.</span><br><span class="line">&#x2F;registry&#x2F;clusterroles&#x2F;system:aggregate-to-admin</span><br><span class="line">&#x2F;registry&#x2F;configmaps&#x2F;kube-public&#x2F;cluster-info</span><br><span class="line">&#x2F;registry&#x2F;masterleases&#x2F;172.17.0.53</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;default</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-public</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-system</span><br><span class="line">&#x2F;registry&#x2F;pods&#x2F;kube-system&#x2F;etcd-master</span><br><span class="line">&#x2F;registry&#x2F;pods&#x2F;kube-system&#x2F;kube-apiserver-master</span><br><span class="line">&#x2F;registry&#x2F;ranges&#x2F;serviceips</span><br><span class="line">&#x2F;registry&#x2F;ranges&#x2F;servicenodeports</span><br><span class="line">...</span><br><span class="line"># 篇幅原因，删掉了很多输出</span><br></pre></td></tr></table></figure></div>

<p>可以看到有各种类型的资源。我们直接以 Namespaces 为例。</p>
<ul>
<li>查看 Namespaces 信息</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt get &#x2F;registry&#x2F;namespaces --prefix --keys-only</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;default</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-public</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-system</span><br></pre></td></tr></table></figure></div>

<ul>
<li>使用 <code>kubectl</code> 创建名为 <code>moelove</code> 的 Namespaces</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl create  ns moelove</span><br><span class="line">namespace&#x2F;moelove created</span><br></pre></td></tr></table></figure></div>

<ul>
<li>查看 Namespaces 信息</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F; # ETCDCTL_API&#x3D;3 etcdctl --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key  --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt  --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt get &#x2F;registry&#x2F;namespaces --prefix --keys-only</span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;default</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-public</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;kube-system</span><br><span class="line"></span><br><span class="line">&#x2F;registry&#x2F;namespaces&#x2F;moelove</span><br></pre></td></tr></table></figure></div>

<p>发现刚创建的 <code>moelove</code> 的 Namespaces 已经在 <code>etcd</code> 中了。</p>
<h2 id="etcd-是如何被使用的"><a href="#etcd-是如何被使用的" class="headerlink" title="etcd 是如何被使用的"></a><code>etcd</code> 是如何被使用的</h2><p>首先，在 <code>staging/src/k8s.io/apiserver/pkg/server/options/etcd.go</code> 中，存在一处声明：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">var storageTypes &#x3D; sets.NewString(</span><br><span class="line">	storagebackend.StorageTypeUnset,</span><br><span class="line">	storagebackend.StorageTypeETCD2,</span><br><span class="line">	storagebackend.StorageTypeETCD3,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<p><strong>在 1.13 发布时，<code>etcd 2</code> 的相关代码已经移除，其中就包含上面声明中的 <code>storagebackend.StorageTypeETCD2</code></strong></p>
<p>这里是在过渡期间为了同时兼容 <code>etcd</code> 2 和 3 而增加的。</p>
<p>我们来看下实际对各类资源的操作，还是以对 <code>Namespace</code> 的操作为例：代码在 <code>pkg/registry/core/namespace/storage/storage.go</code> 中，</p>
<p>比如，<code>Get</code>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">func (r *REST) Get(ctx context.Context, name string, options *metav1.GetOptions) (runtime.Object, error) &#123;</span><br><span class="line">	return r.store.Get(ctx, name, options)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>而 <code>REST</code> 则是这样定义的：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">type REST struct &#123;</span><br><span class="line">	store  *genericregistry.Store</span><br><span class="line">	status *genericregistry.Store</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>通过 <code>REST</code> 实现了一个 <code>RESTStorage</code>，实际使用时，也是调用了 <code>staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go</code> 对接口的封装。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">func (e *Store) Get(ctx context.Context, name string, options *metav1.GetOptions) (runtime.Object, error) &#123;</span><br><span class="line">	obj :&#x3D; e.NewFunc()</span><br><span class="line">	key, err :&#x3D; e.KeyFunc(ctx, name)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return nil, err</span><br><span class="line">	&#125;</span><br><span class="line">	if err :&#x3D; e.Storage.Get(ctx, key, options.ResourceVersion, obj, false); err !&#x3D; nil &#123;</span><br><span class="line">		return nil, storeerr.InterpretGetError(err, e.qualifiedResourceFromContext(ctx), name)</span><br><span class="line">	&#125;</span><br><span class="line">	if e.Decorator !&#x3D; nil &#123;</span><br><span class="line">		if err :&#x3D; e.Decorator(obj); err !&#x3D; nil &#123;</span><br><span class="line">			return nil, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	return obj, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在该处实现了对各类基本方法的封装，各种资源类型都会统一去调用。而更上层则是对 <code>storagebackend</code> 的统一封装，最终会调用 <code>etcd</code> 客户端的实现完成想对应的操作，这里就不再多展开了。</p>
<h2 id="总结-11"><a href="#总结-11" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>etcd</code> 以及它在 K8S 中的作用以及如何使用。我们还介绍了如何通过 <code>etcdctl</code> 工具去操作 <code>etcd</code>。</p>
<p>在某些极端情况下，也许你需要通过直接操作 <code>etcd</code> 集群去变更数据，这里没有介绍所有的操作命令，感兴趣的可以自行通过下方的链接看官方文档进行学习。</p>
<p>但通常情况下，不建议直接操作 <code>etcd</code> ，除非你已经明确自己在做什么。</p>
<p>另外，由于 <code>etcd</code> 集群使用 <code>Raft</code> 一致性算法，通常情况下 <code>etcd</code> 集群需要部署奇数个节点，如 3，5，7 等。<code>etcd</code> 集群维护也相对容易，很容易可以做成高可用集群。（这也是保障 K8S 集群高可用的重要一环）</p>
<p>学习了 <code>etcd</code> 之后，下节我们来学习同样很重要的一个组件 <code>Controller Manager</code>，学习它是如何保障节群符合预期状态的。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://etcd.readthedocs.io/en/latest/faq.html#what-is-etcd" target="_blank" rel="noopener">What is etcd</a></li>
</ul>
<h1 id="庖丁解牛：controller-manager"><a href="#庖丁解牛：controller-manager" class="headerlink" title="庖丁解牛：controller-manager"></a>庖丁解牛：controller-manager</h1><h2 id="整体概览-6"><a href="#整体概览-6" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>Controller Manager</code> 的存在，知道了 Master 是 K8S 是集群的大脑，而它则是 Master 中最繁忙的部分。为什么这么说？本节我们一同来看看它为何如此繁忙。</p>
<p><strong>注意：<code>Controller Manager</code> 实际由 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code> 两部分组成，<code>cloud-controller-manager</code> 则是为各家云厂商提供了一个抽象的封装，便于让各厂商使用各自的 <code>provide</code>。本文只讨论 <code>kube-controller-manager</code>，为了避免混淆，下文统一使用 <code>kube-controller-manager</code>。</strong></p>
<h2 id="kube-controller-manager-是什么"><a href="#kube-controller-manager-是什么" class="headerlink" title="kube-controller-manager 是什么"></a><code>kube-controller-manager</code> 是什么</h2><p>一句话来讲 <code>kube-controller-manager</code> 是一个嵌入了 K8S 核心控制循环的守护进程。</p>
<p>这里的重点是</p>
<ul>
<li>嵌入：它已经内置了相关逻辑，可独立进行部署。我们在第 5 节下载 K8S 服务端二进制文件解压后，便可以看到 <code>kube-controller-manager</code> 的可执行文件，不过我们使用的是 <code>kubeadm</code> 进行的部署，它会默认使用 <code>k8s.gcr.io/kube-controller-manager</code> 的镜像。我们直接来看下实际情况：</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system describe pods -l component&#x3D;kube-controller-manager</span><br><span class="line">Name:               kube-controller-manager-master</span><br><span class="line">Namespace:          kube-system</span><br><span class="line">Priority:           2000000000</span><br><span class="line">PriorityClassName:  system-cluster-critical</span><br><span class="line">Node:               master&#x2F;172.17.0.35</span><br><span class="line">Start Time:         Mon, 10 Dec 2018 07:14:21 +0000</span><br><span class="line">Labels:             component&#x3D;kube-controller-manager</span><br><span class="line">                    tier&#x3D;control-plane</span><br><span class="line">Annotations:        kubernetes.io&#x2F;config.hash&#x3D;c7ed7a8fa5c430410e84970f8ee7e067</span><br><span class="line">                    kubernetes.io&#x2F;config.mirror&#x3D;c7ed7a8fa5c430410e84970f8ee7e067</span><br><span class="line">                    kubernetes.io&#x2F;config.seen&#x3D;2018-12-10T07:14:21.685626322Z</span><br><span class="line">                    kubernetes.io&#x2F;config.source&#x3D;file</span><br><span class="line">                    scheduler.alpha.kubernetes.io&#x2F;critical-pod&#x3D;</span><br><span class="line">Status:             Running</span><br><span class="line">IP:                 172.17.0.35</span><br><span class="line">Containers:</span><br><span class="line">  kube-controller-manager:</span><br><span class="line">    Container ID:  docker:&#x2F;&#x2F;0653e71ae4287608726490b724c3d064d5f1556dd89b7d3c618e97f0e7f2a533</span><br><span class="line">    Image:         k8s.gcr.io&#x2F;kube-controller-manager-amd64:v1.11.3</span><br><span class="line">    Image ID:      docker-pullable:&#x2F;&#x2F;k8s.gcr.io&#x2F;kube-controller-manager-amd64@sha256:a6d115bb1c0116036ac6e6e4d504665bc48879c421a450566c38b3b726f0a123</span><br><span class="line">    Port:          &lt;none&gt;</span><br><span class="line">    Host Port:     &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      kube-controller-manager</span><br><span class="line">      --address&#x3D;127.0.0.1</span><br><span class="line">      --cluster-signing-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt</span><br><span class="line">      --cluster-signing-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key</span><br><span class="line">      --controllers&#x3D;*,bootstrapsigner,tokencleaner</span><br><span class="line">      --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf</span><br><span class="line">      --leader-elect&#x3D;true</span><br><span class="line">      --root-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt</span><br><span class="line">      --service-account-private-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sa.key</span><br><span class="line">      --use-service-account-credentials&#x3D;true</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Mon, 10 Dec 2018 07:14:24 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        200m</span><br><span class="line">    Liveness:     http-get http:&#x2F;&#x2F;127.0.0.1:10252&#x2F;healthz delay&#x3D;15s timeout&#x3D;15s period&#x3D;10s #success&#x3D;1 #failure&#x3D;8</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      &#x2F;etc&#x2F;ca-certificates from etc-ca-certificates (ro)</span><br><span class="line">      &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf from kubeconfig (ro)</span><br><span class="line">      &#x2F;etc&#x2F;kubernetes&#x2F;pki from k8s-certs (ro)</span><br><span class="line">      &#x2F;etc&#x2F;ssl&#x2F;certs from ca-certs (ro)</span><br><span class="line">      &#x2F;usr&#x2F;libexec&#x2F;kubernetes&#x2F;kubelet-plugins&#x2F;volume&#x2F;exec from flexvolume-dir (rw)</span><br><span class="line">      &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates from usr-local-share-ca-certificates (ro)</span><br><span class="line">      &#x2F;usr&#x2F;share&#x2F;ca-certificates from usr-share-ca-certificates (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             True</span><br><span class="line">  ContainersReady   True</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  usr-share-ca-certificates:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;usr&#x2F;share&#x2F;ca-certificates</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  usr-local-share-ca-certificates:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  etc-ca-certificates:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;ca-certificates</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  k8s-certs:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;kubernetes&#x2F;pki</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  ca-certs:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;ssl&#x2F;certs</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  kubeconfig:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf</span><br><span class="line">    HostPathType:  FileOrCreate</span><br><span class="line">  flexvolume-dir:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          &#x2F;usr&#x2F;libexec&#x2F;kubernetes&#x2F;kubelet-plugins&#x2F;volume&#x2F;exec</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">QoS Class:         Burstable</span><br><span class="line">Node-Selectors:    &lt;none&gt;</span><br><span class="line">Tolerations:       :NoExecute</span><br><span class="line">Events:            &lt;none&gt;</span><br><span class="line">master</span><br></pre></td></tr></table></figure></div>

<p>这是使用 <code>kubeadm</code> 搭建的集群中的 <code>kube-controller-manager</code> 的 <code>Pod</code>，首先可以看到它所使用的镜像，其次可以看到它使用的一系列参数，最后它在 <code>10252</code> 端口提供了健康检查的接口。稍后我们再展开。</p>
<ul>
<li>控制循环：这里拆解为两部分： <strong>控制</strong> 和 <strong>循环</strong> ，它所控制的是集群的状态；至于循环它当然是会有个循环间隔的，这里有个参数可以进行控制。</li>
<li>守护进程：这个就不单独展开了。</li>
</ul>
<h2 id="kube-controller-manager-有什么作用"><a href="#kube-controller-manager-有什么作用" class="headerlink" title="kube-controller-manager 有什么作用"></a><code>kube-controller-manager</code> 有什么作用</h2><p>前面已经说了它一个很关键的点 “控制”：它通过 <code>kube-apiserver</code> 提供的信息持续的监控集群状态，并尝试将集群调整至预期的状态。由于访问 <code>kube-apiserver</code> 也需要通过认证，授权等过程，所以可以看到上面启动 <code>kube-controller-manager</code> 时提供了一系列的参数。</p>
<p>比如，当我们创建了一个 <code>Deployment</code>，默认副本数为 1 ，当我们把 <code>Pod</code> 删除后，<code>kube-controller-manager</code> 会按照原先的预期，重新创建一个 <code>Pod</code> 。下面举个例子：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl run redis --image&#x3D;&#39;redis&#39;</span><br><span class="line">deployment.apps&#x2F;redis created</span><br><span class="line">master $ kubectl get all</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-bb7894d65-w2rsp   1&#x2F;1       Running   0          3m</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   18m</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            1           3m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-bb7894d65   1         1         1         3m</span><br><span class="line">master $ kubectl delete pod&#x2F;redis-bb7894d65-w2rsp</span><br><span class="line">pod &quot;redis-bb7894d65-w2rsp&quot; deleted</span><br><span class="line">master $ kubectl get all  # 可以看到已经重新运行了一个 Pod</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-bb7894d65-62ftk   1&#x2F;1       Running   0          16s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   19m</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            1           4m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-bb7894d65   1         1         1         4m</span><br></pre></td></tr></table></figure></div>

<p>我们来看下 <code>kube-controller-manager</code> 的日志：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system logs -l component&#x3D;kube-controller-manager --tail&#x3D;5</span><br><span class="line">I1210 09:30:17.125377       1 node_lifecycle_controller.go:945] Controller detected that all Nodes are not-Ready. Entering master disruption mode.</span><br><span class="line">I1210 09:31:07.140539       1 node_lifecycle_controller.go:972] Controller detected that some Nodes are Ready. Exiting master disruption mode.</span><br><span class="line">I1210 09:43:30.377649       1 event.go:221] Event(v1.ObjectReference&#123;Kind:&quot;Deployment&quot;, Namespace:&quot;default&quot;, Name:&quot;redis&quot;, UID:&quot;0d1cb2d7-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps&#x2F;v1&quot;, ResourceVersion:&quot;1494&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;ScalingReplicaSet&#39; Scaled up replica setredis-bb7894d65 to 1</span><br><span class="line">I1210 09:43:30.835149       1 event.go:221] Event(v1.ObjectReference&#123;Kind:&quot;ReplicaSet&quot;, Namespace:&quot;default&quot;, Name:&quot;redis-bb7894d65&quot;, UID:&quot;0d344d15-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps&#x2F;v1&quot;, ResourceVersion:&quot;1495&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;SuccessfulCreate&#39; Created pod:redis-bb7894d65-w2rsp</span><br><span class="line">I1210 09:47:41.658781       1 event.go:221] Event(v1.ObjectReference&#123;Kind:&quot;ReplicaSet&quot;, Namespace:&quot;default&quot;, Name:&quot;redis-bb7894d65&quot;, UID:&quot;0d344d15-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps&#x2F;v1&quot;, ResourceVersion:&quot;1558&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;SuccessfulCreate&#39; Created pod:redis-bb7894d65-62ftk</span><br></pre></td></tr></table></figure></div>

<p>可以看到它先观察到有 <code>Deployment</code> 的事件，然后 <code>ScalingReplicaSet</code> 进而创建了对应的 <code>Pod</code>。 而当我们删掉正在运行的 <code>Pod</code> 后，它便会重新创建 <code>Pod</code> 使集群状态符合原先的预期状态。</p>
<p>同时，注意 <code>Pod</code> 的名字已经发生了变化。</p>
<h2 id="kube-controller-manager-是如何工作的"><a href="#kube-controller-manager-是如何工作的" class="headerlink" title="kube-controller-manager 是如何工作的"></a><code>kube-controller-manager</code> 是如何工作的</h2><p>在 <code>cmd/kube-controller-manager/app/controllermanager.go</code> 中列出了大多数的 <code>controllermanager</code>，他们对 <code>controllermanager</code> 函数的实际调用都在 <code>cmd/kube-controller-manager/app/core.go</code> 中，我们以 <code>PodGC</code> 为例：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">func startPodGCController(ctx ControllerContext) (bool, error) &#123;</span><br><span class="line">	go podgc.NewPodGC(</span><br><span class="line">		ctx.ClientBuilder.ClientOrDie(&quot;pod-garbage-collector&quot;),</span><br><span class="line">		ctx.InformerFactory.Core().V1().Pods(),</span><br><span class="line">		int(ctx.ComponentConfig.PodGCController.TerminatedPodGCThreshold),</span><br><span class="line">	).Run(ctx.Stop)</span><br><span class="line">	return true, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在前两节中我们已经对 <code>kube-apiserver</code> 和 <code>etcd</code> 有了一些基本的认识，这里它主要会去 watch 相关的资源，但是出于性能上的考虑，也不能过于频繁的去请求 <code>kube-apiserver</code> 或者永久 watch ，所以在实现上借助了 <a href="https://github.com/kubernetes/client-go">client-go</a> 的 <code>informer</code> 包，相当于是实现了一个本地的二级缓存。这里不做过多展开。</p>
<p>它最终会调用 <code>PodGC</code> 的具体实现，位置在 <code>pkg/controller/podgc/gc_controller.go</code> 中：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">func NewPodGC(kubeClient clientset.Interface, podInformer coreinformers.PodInformer, terminatedPodThreshold int) *PodGCController &#123;</span><br><span class="line">	if kubeClient !&#x3D; nil &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() !&#x3D; nil &#123;</span><br><span class="line">		metrics.RegisterMetricAndTrackRateLimiterUsage(&quot;gc_controller&quot;, kubeClient.CoreV1().RESTClient().GetRateLimiter())</span><br><span class="line">	&#125;</span><br><span class="line">	gcc :&#x3D; &amp;PodGCController&#123;</span><br><span class="line">		kubeClient:             kubeClient,</span><br><span class="line">		terminatedPodThreshold: terminatedPodThreshold,</span><br><span class="line">		deletePod: func(namespace, name string) error &#123;</span><br><span class="line">			glog.Infof(&quot;PodGC is force deleting Pod: %v:%v&quot;, namespace, name)</span><br><span class="line">			return kubeClient.CoreV1().Pods(namespace).Delete(name, metav1.NewDeleteOptions(0))</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	gcc.podLister &#x3D; podInformer.Lister()</span><br><span class="line">	gcc.podListerSynced &#x3D; podInformer.Informer().HasSynced</span><br><span class="line"></span><br><span class="line">	return gcc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>代码也比较直观，不过这里可以看到有一个注册 <code>metrics</code> 的过程，实际上 <code>kube-controller-manager</code> 在前面的 <code>10252</code> 端口上不仅暴露出来了一个 <code>/healthz</code> 接口，还暴露出了一个 <code>/metrics</code> 的接口，可用于进行监控之类的。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system get pod -l component&#x3D;kube-controller-manager</span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">kube-controller-manager-master   1&#x2F;1       Running   1          2m</span><br><span class="line">master $ kubectl -n kube-system exec -it kube-controller-manager-master sh</span><br><span class="line">&#x2F; # wget -qO- http:&#x2F;&#x2F;127.0.0.1:10252&#x2F;metrics|grep gc_controller</span><br><span class="line"># HELP gc_controller_rate_limiter_use A metric measuring the saturation of the rate limiter for gc_controller</span><br><span class="line"># TYPE gc_controller_rate_limiter_use gauge</span><br><span class="line">gc_controller_rate_limiter_use 0</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-12"><a href="#总结-12" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>kube-controller-manager</code> 以及它在 K8S 中主要是将集群调节至预期的状态，并提供出了 <code>/metrics</code> 的接口可供监控。</p>
<p><code>kube-controller-manager</code> 中有很多的 controller 大多数是默认开启的，当然也有默认关闭的，比如 <code>bootstrapsigner</code> 和 <code>tokencleaner</code>，在我们启动 <code>kube-controller-manager</code> 的时候，可通过 <code>--controllers</code> 的参数进行控制，就比如上面例子中 <code>--controllers=*,bootstrapsigner,tokencleaner</code> 表示开启所有默认开启的以及 <code>bootstrapsigner</code> 和 <code>tokencleaner</code> 。</p>
<p>下节，我们将学习另一个与资源调度有关的组件 <code>kube-scheduler</code>，了解下它对我们使用集群所带来的意义。</p>
<p>留言</p>
<ul>
<li><p><a href="https://juejin.im/user/5937807c2f301e006b268c6c" target="_blank" rel="noopener">楊歌</a></p>
<p>Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。</p>
</li>
</ul>
<h1 id="庖丁解牛：kube-scheduler"><a href="#庖丁解牛：kube-scheduler" class="headerlink" title="庖丁解牛：kube-scheduler"></a>庖丁解牛：kube-scheduler</h1><h2 id="整体概览-7"><a href="#整体概览-7" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------+          </span><br><span class="line">| Master                                                   |          </span><br><span class="line">|              +-------------------------+                 |          </span><br><span class="line">|     +-------&gt;|        API Server       |&lt;--------+       |          </span><br><span class="line">|     |        |                         |         |       |          </span><br><span class="line">|     v        +-------------------------+         v       |          </span><br><span class="line">|   +----------------+     ^      +--------------------+   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   |   Scheduler    |     |      | Controller Manager |   |          </span><br><span class="line">|   |                |     |      |                    |   |          </span><br><span class="line">|   +----------------+     v      +--------------------+   |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| |                Cluster state store                   | |          </span><br><span class="line">| |                                                      | |          </span><br><span class="line">| +------------------------------------------------------+ |          </span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>Scheduler</code> 的存在，知道了 Master 是 K8S 是集群的大脑，<code>Controller Manager</code> 负责将集群调整至预期的状态，而 <code>Scheduler</code> 则是集群调度器，将预期的 <code>Pod</code> 资源调度到正确的 <code>Node</code> 节点上，进而令该 <code>Pod</code> 可完成启动。本节我们一同来看看它如何发挥如此大的作用。</p>
<p><strong>下文统一使用 <code>kube-scheduler</code> 进行表述</strong></p>
<h2 id="kube-scheduler-是什么"><a href="#kube-scheduler-是什么" class="headerlink" title="kube-scheduler 是什么"></a><code>kube-scheduler</code> 是什么</h2><p>引用<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" rel="noopener">官方文档</a>一句话：</p>
<blockquote>
<p>The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity.</p>
</blockquote>
<p><code>kube-scheduler</code> 是一个策略丰富，拓扑感知的调度程序，会显著影响可用性，性能和容量。</p>
<p>我们知道资源调度本就是 K8S 这类系统中的一个很复杂的事情，既要能满足系统对资源利用率的需要，同样还需要避免资源竞争，比如说端口冲突之类的。</p>
<p>为了能完成这样的需求，<code>kube-scheduler</code> 便在不断的迭代和发展，通过支持多种策略满足各类需求，通过感知拓扑避免资源竞争和保障系统的可用性及容量等。</p>
<p>我们在第 5 节下载服务端二进制文件解压后，便可看到 <code>kube-scheduler</code> 的可执行文件。当给它传递 <code>--help</code> 查看其支持参数的时候，便可以看到它支持使用 <code>--address</code> 或者 <code>--bind-address</code> 等参数指定所启动的 HTTP server 所绑定的地址之类的。</p>
<p>它和 <code>kube-controller-manager</code> 有点类似，同样是通过定时的向 <code>kube-apiserver</code> 请求获取信息，并进行处理。而他们所起到的作用并不相同。</p>
<h2 id="kube-scheduler-有什么作用"><a href="#kube-scheduler-有什么作用" class="headerlink" title="kube-scheduler 有什么作用"></a><code>kube-scheduler</code> 有什么作用</h2><p>从上层的角度来看，<code>kube-scheduler</code> 的作用就是将待调度的 <code>Pod</code> 调度至最佳的 <code>Node</code> 上，而这个过程中则需要根据不同的策略，考虑到 <code>Node</code> 的资源使用情况，比如端口，内存，存储等。</p>
<h2 id="kube-scheduler-是如何工作的"><a href="#kube-scheduler-是如何工作的" class="headerlink" title="kube-scheduler 是如何工作的"></a><code>kube-scheduler</code> 是如何工作的</h2><p>整体的过程可通过 <code>pkg/scheduler/core/generic_scheduler.go</code> 的代码来看</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) &#123;</span><br><span class="line">	trace :&#x3D; utiltrace.New(fmt.Sprintf(&quot;Scheduling %s&#x2F;%s&quot;, pod.Namespace, pod.Name))</span><br><span class="line">	defer trace.LogIfLong(100 * time.Millisecond)</span><br><span class="line"></span><br><span class="line">	if err :&#x3D; podPassesBasicChecks(pod, g.pvcLister); err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	nodes, err :&#x3D; nodeLister.List()</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line">	if len(nodes) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">		return &quot;&quot;, ErrNoNodesAvailable</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	err &#x3D; g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	trace.Step(&quot;Computing predicates&quot;)</span><br><span class="line">	startPredicateEvalTime :&#x3D; time.Now()</span><br><span class="line">	filteredNodes, failedPredicateMap, err :&#x3D; g.findNodesThatFit(pod, nodes)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if len(filteredNodes) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">		return &quot;&quot;, &amp;FitError&#123;</span><br><span class="line">			Pod:              pod,</span><br><span class="line">			NumAllNodes:      len(nodes),</span><br><span class="line">			FailedPredicates: failedPredicateMap,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime))</span><br><span class="line">	metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime))</span><br><span class="line"></span><br><span class="line">	trace.Step(&quot;Prioritizing&quot;)</span><br><span class="line">	startPriorityEvalTime :&#x3D; time.Now()</span><br><span class="line">	&#x2F;&#x2F; When only one node after predicate, just use it.</span><br><span class="line">	if len(filteredNodes) &#x3D;&#x3D; 1 &#123;</span><br><span class="line">		metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))</span><br><span class="line">		return filteredNodes[0].Name, nil</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	metaPrioritiesInterface :&#x3D; g.priorityMetaProducer(pod, g.cachedNodeInfoMap)</span><br><span class="line">	priorityList, err :&#x3D; PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line">	metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))</span><br><span class="line">	metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime))</span><br><span class="line"></span><br><span class="line">	trace.Step(&quot;Selecting host&quot;)</span><br><span class="line">	return g.selectHost(priorityList)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>它的输入有两个：</p>
<ul>
<li><code>pod</code>：待调度的 <code>Pod</code> 对象；</li>
<li><code>nodeLister</code>：所有可用的 <code>Node</code> 列表</li>
</ul>
<p>备注：<code>nodeLister</code> 的实现稍微用了点技巧，返回的是 <code>[]*v1.Node</code> 而不是 <code>v1.NodeList</code> 可避免拷贝带来的性能损失。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type NodeLister interface &#123;</span><br><span class="line">	List() ([]*v1.Node, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="处理阶段"><a href="#处理阶段" class="headerlink" title="处理阶段"></a>处理阶段</h3><p><code>kube-scheduler</code> 将处理阶段主要分为三个阶段 <code>Computing predicates</code>，<code>Prioritizing</code>和 <code>Selecting host</code>：</p>
<ul>
<li><p><code>Computing predicates</code>：主要解决的问题是 <code>Pod</code> 能否调度到集群的 <code>Node</code> 上；</p>
<p>主要是通过一个名为 <code>podFitsOnNode</code> 的函数进行实现，在检查的过程中也会先去检查下是否已经有已缓存的判断结果， 当然也会检查 <code>Pod</code> 是否是可调度的，以防有 <code>Pod Affinity</code> (亲合性) 之类的存在。</p>
</li>
<li><p><code>Prioritizing</code>：主要解决的问题是在上个阶段通过 <code>findNodesThatFit</code> 得到了 <code>filteredNodes</code> 的基础之上解决哪些 <code>Node</code> 是最优的，得到一个优先级列表 <code>priorityList</code>;</p>
<p>至于优先级的部分，主要是通过下面的代码：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for i :&#x3D; range nodes &#123;</span><br><span class="line">	result &#x3D; append(result, schedulerapi.HostPriority&#123;Host: nodes[i].Name, Score: 0&#125;)</span><br><span class="line">	for j :&#x3D; range priorityConfigs &#123;</span><br><span class="line">		result[i].Score +&#x3D; results[j][i].Score * priorityConfigs[j].Weight</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>给每个经过第一步筛选出来的 <code>Node</code> 一个 <code>Score</code>，再按照各种条件进行打分，最终得到一个优先级列表。</p>
</li>
<li><p><code>Selecting host</code>：则是最终选择 <code>Node</code> 调度到哪台机器上。</p>
<p>最后，则是通过 <code>selectHost</code> 选择出最终要调度到哪台机器上。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) &#123;</span><br><span class="line">    if len(priorityList) &#x3D;&#x3D; 0 &#123;</span><br><span class="line">        return &quot;&quot;, fmt.Errorf(&quot;empty priorityList&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sort.Sort(sort.Reverse(priorityList))</span><br><span class="line">    maxScore :&#x3D; priorityList[0].Score</span><br><span class="line">    firstAfterMaxScore :&#x3D; sort.Search(len(priorityList), func(i int) bool &#123; return priorityList[i].Score &lt; maxScore &#125;)</span><br><span class="line"></span><br><span class="line">    g.lastNodeIndexLock.Lock()</span><br><span class="line">    ix :&#x3D; int(g.lastNodeIndex % uint64(firstAfterMaxScore))</span><br><span class="line">    g.lastNodeIndex++</span><br><span class="line">    g.lastNodeIndexLock.Unlock()</span><br><span class="line"></span><br><span class="line">    return priorityList[ix].Host, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<h2 id="总结-13"><a href="#总结-13" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>kube-scheduler</code> 以及它在调度 <code>Pod</code> 的过程中的大致步骤。</p>
<p>不过它实际使用的各种策略及判断条件很多，无法在一节中完全都详细介绍，感兴趣的朋友可以按照本节中提供的思路大致去看看它的实现。</p>
<p>我们通过前面几节的介绍，已经知道了当实际进行部署操作的时候，首先会通过 <code>kubectl</code> 之类的客户端工具与 <code>kube-apiserver</code> 进行交互，在经过一系列的处理后，数据将持久化到 <code>etcd</code> 中；</p>
<p>此时，<code>kube-controller-manager</code> 通过持续的观察，开始按照我们的配置，将集群的状态调整至预期状态；</p>
<p>而 <code>kube-scheduler</code> 也在发挥作用，决定 <code>Pod</code> 应该调度至哪个或者哪些 <code>Node</code> 上；之后则通过其他组件的协作，最总将该 <code>Pod</code> 在相应的 <code>Node</code> 上部署启动。</p>
<p>我们在下节将要介绍的 <code>kubelet</code> 便是后面这部分“实际部署动作”相关的组件中尤为重要的一个，下节我们再详细介绍它是如何完成这些功能的。</p>
<h1 id="庖丁解牛：kubelet"><a href="#庖丁解牛：kubelet" class="headerlink" title="庖丁解牛：kubelet"></a>庖丁解牛：kubelet</h1><h2 id="整体概览-8"><a href="#整体概览-8" class="headerlink" title="整体概览"></a>整体概览</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">+--------------------------------------------------------+       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| |      kubelet        |        |     kube-proxy      | |       </span><br><span class="line">| |                     |        |                     | |       </span><br><span class="line">| +---------------------+        +---------------------+ |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">| | Container Runtime (Docker)                         | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| | |Pod                  |    |Pod                  | | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | | |C1   | |C2   |     |    ||C1   ||C2   ||C3   || | |       </span><br><span class="line">| | | |     | |     |     |    ||     ||     ||     || | |       </span><br><span class="line">| | | +-----+ +-----+     |    |+-----++-----++-----+| | |       </span><br><span class="line">| | +---------------------+    +---------------------+ | |       </span><br><span class="line">| +----------------------------------------------------+ |       </span><br><span class="line">+--------------------------------------------------------+</span><br></pre></td></tr></table></figure></div>

<p>在第 3 节《宏观认识：整体架构》 中，我们知道了 K8S 中 Node 由一些必要的组件构成，而其中最为核心的当属 <code>kubelet</code> 了，如果没有 <code>kubelet</code> 的存在，那我们预期的各类资源就只能存在于 <code>Master</code> 的相关组件中了，而 K8S 也很能只是一个 CRUD 的普通程序了。本节，我们来介绍下 <code>kubelet</code> 及它是如何完成这一系列任务的。</p>
<h2 id="kubelet-是什么"><a href="#kubelet-是什么" class="headerlink" title="kubelet 是什么"></a><code>kubelet</code> 是什么</h2><p>按照一般架构设计上的习惯，<code>kubelet</code> 所承担的角色一般会被叫做 <code>agent</code>，这里叫做 <code>kubelet</code> 很大程度上受 <code>Borg</code> 的命名影响，<code>Borg</code> 里面也有一个 <code>Borglet</code> 的组件存在。<code>kubelet</code> 便是 K8S 中的 <code>agent</code>，负责 <code>Node</code> 和 <code>Pod</code> 相关的管理任务。</p>
<p>同样的，在我们下载 K8S 二进制文件解压后，便可以得到 <code>kubelet</code> 的可执行文件。在第 5 节中，我们也完成了 <code>kubelet</code> 以 <code>systemd</code> 进行启动和管理的相关配置。</p>
<h2 id="kubelet-有什么作用"><a href="#kubelet-有什么作用" class="headerlink" title="kubelet 有什么作用"></a><code>kubelet</code> 有什么作用</h2><p>通常来讲 <code>agent</code> 这样的角色起到的作用首先便是要能够注册，让 <code>server</code> 端知道它的存在，所以这便是它的第一个作用：节点管理。</p>
<h3 id="节点管理"><a href="#节点管理" class="headerlink" title="节点管理"></a>节点管理</h3><p>当我们执行 <code>kubelet --help</code> 的时候，会看到它所支持的可配置参数，其中有一个 <code>--register-node</code> 参数便是用于控制是否向 <code>kube-apiserver</code> 注册节点的，默认是开启的。</p>
<p>我们在第 5 节中还介绍了如何新增一个 <code>Node</code>，当 <code>kubeadm join</code> 执行成功后，你便可以通过 <code>kubectl get node</code> 查看到新加入集群中的 <code>Node</code>，与此同时，你也可以在该节点上通过以下命令查看 <code>kubelet</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">master $ systemctl status kubelet</span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Agent</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d</span><br><span class="line">           └─kubeadm.conf</span><br><span class="line">   Active: active (running) since Thu 2018-12-13 07:49:51 UTC; 32min ago</span><br><span class="line">     Docs: http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;</span><br><span class="line"> Main PID: 3876259 (kubelet)</span><br><span class="line">   Memory: 66.3M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;kubelet.service</span><br><span class="line">           └─3876259 &#x2F;usr&#x2F;bin&#x2F;kubelet --bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernete...</span><br></pre></td></tr></table></figure></div>

<p>当我们查看 <code>Node</code> 信息时，也能得到如下输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl get nodes node01 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubeadm.alpha.kubernetes.io&#x2F;cri-socket: &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">    node.alpha.kubernetes.io&#x2F;ttl: &quot;0&quot;</span><br><span class="line">    volumes.kubernetes.io&#x2F;controller-managed-attach-detach: &quot;true&quot;</span><br><span class="line">  creationTimestamp: 2018-12-13T07:50:47Z</span><br><span class="line">  labels:</span><br><span class="line">    beta.kubernetes.io&#x2F;arch: amd64</span><br><span class="line">    beta.kubernetes.io&#x2F;os: linux</span><br><span class="line">    kubernetes.io&#x2F;hostname: node01</span><br><span class="line">  name: node01</span><br><span class="line">  resourceVersion: &quot;4242&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;node01</span><br><span class="line">  uid: cd612df6-feab-11e8-9a0b-0242ac110096</span><br><span class="line">spec: &#123;&#125;</span><br><span class="line">status:</span><br><span class="line">  addresses:</span><br><span class="line">  - address: 172.17.0.152</span><br><span class="line">    type: InternalIP</span><br><span class="line">  - address: node01</span><br><span class="line">    type: Hostname</span><br><span class="line">  allocatable:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: &quot;89032026784&quot;</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3894788Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  capacity:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: 96605932Ki</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3997188Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  conditions:</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has sufficient disk space available</span><br><span class="line">    reason: KubeletHasSufficientDisk</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: OutOfDisk</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has sufficient memory available</span><br><span class="line">    reason: KubeletHasSufficientMemory</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: MemoryPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has no disk pressure</span><br><span class="line">    reason: KubeletHasNoDiskPressure</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: DiskPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:50:47Z</span><br><span class="line">    message: kubelet has sufficient PID available</span><br><span class="line">    reason: KubeletHasSufficientPID</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: PIDPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-13T08:39:41Z</span><br><span class="line">    lastTransitionTime: 2018-12-13T07:51:37Z</span><br><span class="line">    message: kubelet is posting ready status. AppArmor enabled</span><br><span class="line">    reason: KubeletReady</span><br><span class="line">    status: &quot;True&quot;</span><br><span class="line">    type: Ready</span><br><span class="line">  daemonEndpoints:</span><br><span class="line">    kubeletEndpoint:</span><br><span class="line">      Port: 10250</span><br><span class="line">  images:</span><br><span class="line">  - names:</span><br><span class="line">    - k8s.gcr.io&#x2F;kube-apiserver-amd64@sha256:956bea8c139620c9fc823fb81ff9b5647582b53bd33904302987d56ab24fc187</span><br><span class="line">    - k8s.gcr.io&#x2F;kube-apiserver-amd64:v1.11.3</span><br><span class="line">    sizeBytes: 186676561</span><br><span class="line">  nodeInfo:</span><br><span class="line">    architecture: amd64</span><br><span class="line">    bootID: 89ced22c-f7f8-4c4d-ad0d-d10887ab900e</span><br><span class="line">    containerRuntimeVersion: docker:&#x2F;&#x2F;18.6.0</span><br><span class="line">    kernelVersion: 4.4.0-62-generic</span><br><span class="line">    kubeProxyVersion: v1.11.3</span><br><span class="line">    kubeletVersion: v1.11.3</span><br><span class="line">    machineID: 26ba042302eea8095d6576975c120eeb</span><br><span class="line">    operatingSystem: linux</span><br><span class="line">    osImage: Ubuntu 16.04.2 LTS</span><br><span class="line">    systemUUID: 26ba042302eea8095d6576975c120eeb</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>kubelet</code> 不仅将自己注册给了 <code>kube-apiserver</code>，同时它所在机器的信息也都进行了上报，包括 CPU，内存，IP 信息等。</p>
<p>这其中有我们在第 2 节中提到的关于 <code>Node</code> 状态相关的一些信息，可以对照着看看。</p>
<p>当然这里除了这些信息外，还有些值得注意的，比如 <code>daemonEndpoints</code> 之类的，可以看到目前 <code>kubelet</code> 监听在了 <code>10250</code> 端口，这个端口可通过 <code>--port</code> 配置，但是之后会被废弃掉，我们是写入了 <code>/var/lib/kubelet/config.yaml</code> 的配置文件中。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">master $ cat &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml</span><br><span class="line">address: 0.0.0.0</span><br><span class="line">apiVersion: kubelet.config.k8s.io&#x2F;v1beta1</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: false</span><br><span class="line">  webhook:</span><br><span class="line">    cacheTTL: 2m0s</span><br><span class="line">    enabled: true</span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt</span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">  webhook:</span><br><span class="line">    cacheAuthorizedTTL: 5m0s</span><br><span class="line">    cacheUnauthorizedTTL: 30s</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">cgroupsPerQOS: true</span><br><span class="line">clusterDNS:</span><br><span class="line">- 10.96.0.10</span><br><span class="line">clusterDomain: cluster.local</span><br><span class="line">containerLogMaxFiles: 5</span><br><span class="line">containerLogMaxSize: 10Mi</span><br><span class="line">contentType: application&#x2F;vnd.kubernetes.protobuf</span><br><span class="line">cpuCFSQuota: true</span><br><span class="line">cpuManagerPolicy: none</span><br><span class="line">cpuManagerReconcilePeriod: 10s</span><br><span class="line">enableControllerAttachDetach: true</span><br><span class="line">enableDebuggingHandlers: true</span><br><span class="line">enforceNodeAllocatable:</span><br><span class="line">- pods</span><br><span class="line">eventBurst: 10</span><br><span class="line">eventRecordQPS: 5</span><br><span class="line">evictionHard:</span><br><span class="line">  imagefs.available: 15%</span><br><span class="line">  memory.available: 100Mi</span><br><span class="line">  nodefs.available: 10%</span><br><span class="line">  nodefs.inodesFree: 5%</span><br><span class="line">evictionPressureTransitionPeriod: 5m0s</span><br><span class="line">failSwapOn: true</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">healthzBindAddress: 127.0.0.1</span><br><span class="line">healthzPort: 10248</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">imageMinimumGCAge: 2m0s</span><br><span class="line">iptablesDropBit: 15</span><br><span class="line">iptablesMasqueradeBit: 14</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">kubeAPIBurst: 10</span><br><span class="line">kubeAPIQPS: 5</span><br><span class="line">makeIPTablesUtilChains: true</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">maxPods: 110</span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">oomScoreAdj: -999</span><br><span class="line">podPidsLimit: -1</span><br><span class="line">port: 10250</span><br><span class="line">registryBurst: 10</span><br><span class="line">registryPullQPS: 5</span><br><span class="line">resolvConf: &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">rotateCertificates: true</span><br><span class="line">runtimeRequestTimeout: 2m0s</span><br><span class="line">serializeImagePulls: true</span><br><span class="line">staticPodPath: &#x2F;etc&#x2F;kubernetes&#x2F;manifests</span><br><span class="line">streamingConnectionIdleTimeout: 4h0m0s</span><br><span class="line">syncFrequency: 1m0s</span><br><span class="line">volumeStatsAggPeriod: 1m0s</span><br><span class="line">master $</span><br></pre></td></tr></table></figure></div>

<p>这其中有一些需要关注的配置：</p>
<ul>
<li><p><code>maxPods</code>：最大的 <code>Pod</code> 数</p>
</li>
<li><p><code>healthzBindAddress</code> 和 <code>healthzPort</code>：配置了健康检查所监听的地址和端口</p>
<p>我们可用以下方式进行验证：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">master $ curl 127.0.0.1:10248&#x2F;healthz</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></div>
</li>
<li><p><code>authentication</code> 和 <code>authorization</code> ：认证授权相关</p>
</li>
<li><p><code>evictionHard</code>：涉及到 <code>kubelet</code> 的驱逐策略，对 <code>Pod</code> 调度分配之类的影响很大</p>
</li>
</ul>
<p>其余部分，可参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">手册内容</a></p>
<h3 id="Pod-管理"><a href="#Pod-管理" class="headerlink" title="Pod 管理"></a>Pod 管理</h3><p>从上面的配置以及我们之前的介绍中，<code>kube-scheduler</code> 处理了 <code>Pod</code> 应该调度至哪个 <code>Node</code>，而 <code>kubelet</code> 则是保障该 <code>Pod</code> 能按照预期，在对应 <code>Node</code> 上启动并保持工作。</p>
<p>同时，<code>kubelet</code> 在保障 <code>Pod</code> 能按预期工作，主要是做了两方面的事情：</p>
<ul>
<li>健康检查：通过 <code>LivenessProbe</code> 和 <code>ReadinessProbe</code> 探针进行检查，判断是否健康及是否已经准备好接受请求。</li>
<li>资源监控：通过 <code>cAdvisor</code> 进行资源监。</li>
</ul>
<h2 id="kubelet-是如何工作的"><a href="#kubelet-是如何工作的" class="headerlink" title="kubelet 是如何工作的"></a><code>kubelet</code> 是如何工作的</h2><p>大致的功能已经介绍了，我们来看下它大体的实现。</p>
<p>首先是在 <code>cmd/kubelet/app/server.go</code> 文件中的 <code>Run</code> 方法：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) error &#123;</span><br><span class="line">	glog.Infof(&quot;Version: %+v&quot;, version.Get())</span><br><span class="line">	if err :&#x3D; initForOS(s.KubeletFlags.WindowsService); err !&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;failed OS init: %v&quot;, err)</span><br><span class="line">	&#125;</span><br><span class="line">	if err :&#x3D; run(s, kubeDeps, stopCh); err !&#x3D; nil &#123;</span><br><span class="line">		return fmt.Errorf(&quot;failed to run Kubelet: %v&quot;, err)</span><br><span class="line">	&#125;</span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>这个方法看起来很简单那，它是在读取完一系列的配置和校验之后开始被调用的，在调用过程中，会在日志中输出当前的版本号，如果你的 <code>kubelet</code> 已经正常运行，当你执行 <code>journalctl -u kubelet</code> 的时候，便会看到一条相关的日志输出。</p>
<p>之后，便是一个 <code>run</code> 方法，其中包含着各种环境检查，容器管理，<code>cAdvisor</code> 初始化之类的操作，直到 <code>kubelet</code> 基本正确运行后，则会调用 <code>pkg/kubelet/kubelet.go</code> 中的一个 <code>BirthCry</code> 方法，该方法从命名就可以看出来，它其实就是宣告 <code>kubelet</code> 已经启动：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">func (kl *Kubelet) BirthCry() &#123;</span><br><span class="line">	kl.recorder.Eventf(kl.nodeRef, v1.EventTypeNormal, events.StartingKubelet, &quot;Starting kubelet.&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>后续则是关于注册，<code>Pod</code> 管理以及资源相关的处理逻辑，内容较多，这里就不再展开了。</p>
<h2 id="总结-14"><a href="#总结-14" class="headerlink" title="总结"></a>总结</h2><p>本节中我们介绍了 <code>kubelet</code> 的主要功能和基本实现，了解到了它不仅可将自身注册到集群，同时还承担着保障 <code>Pod</code> 可在该 <code>Node</code> 上按预期工作。另外 <code>kubelet</code> 其实还承担着清理 <code>Node</code> 上一些由 K8S 调度 <code>Pod</code> 所造成的磁盘占用之类的工作。</p>
<p>从上面的配置中基本能看出来一些，这部分的功能大多数情况下不需要大家人为干预，所以也就不再展开了。</p>
<p>当 <code>Pod</code> 在 <code>Node</code> 上正常运行之后，若是需要对外提供服务，则需要将其暴露出来。下节，我们来介绍下 <code>kube-proxy</code> 是如何来处理这些工作的。</p>
<h1 id="庖丁解牛：kube-proxy"><a href="#庖丁解牛：kube-proxy" class="headerlink" title="庖丁解牛：kube-proxy"></a>庖丁解牛：kube-proxy</h1><h2 id="整体概览-9"><a href="#整体概览-9" class="headerlink" title="整体概览"></a>整体概览</h2><p>在第 3 节中，我们了解到 <code>kube-proxy</code> 的存在，而在第 7 中，我们学习到了如何将运行于 K8S 中的服务以 <code>Service</code> 的方式暴露出来，以供访问。</p>
<p>本节，我们来介绍下 <code>kube-proxy</code> 了解下它是如何支撑起这种类似服务发现和代理相关功能的。</p>
<h2 id="kube-proxy-是什么"><a href="#kube-proxy-是什么" class="headerlink" title="kube-proxy 是什么"></a><code>kube-proxy</code> 是什么</h2><p><code>kube-proxy</code> 是 K8S 运行于每个 <code>Node</code> 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。</p>
<p>我们已经知道，当 <code>Pod</code> 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的服务的异常，所以通常情况下，我们都会使用 <code>Service</code> 将后端 <code>Pod</code> 暴露出来，而 <code>Service</code> 则较为稳定。</p>
<p>还是以我们之前的 <a href="https://github.com/tao12345666333/saythx"><code>SayThx</code></a> 项目为例，但我们只部署其中没有任何依赖的后端资源 <code>Redis</code> 。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">master $ git clone https:&#x2F;&#x2F;github.com&#x2F;tao12345666333&#x2F;saythx.git</span><br><span class="line">Cloning into &#39;saythx&#39;...</span><br><span class="line">remote: Enumerating objects: 110, done.</span><br><span class="line">remote: Counting objects: 100% (110&#x2F;110), done.</span><br><span class="line">remote: Compressing objects: 100% (82&#x2F;82), done.</span><br><span class="line">remote: Total 110 (delta 27), reused 102 (delta 20), pack-reused 0</span><br><span class="line">Receiving objects: 100% (110&#x2F;110), 119.42 KiB | 0 bytes&#x2F;s, done.</span><br><span class="line">Resolving deltas: 100% (27&#x2F;27), done.</span><br><span class="line">Checking connectivity... done.</span><br><span class="line">master $ cd saythx&#x2F;deploy</span><br><span class="line">master $ ls</span><br><span class="line">backend-deployment.yaml  frontend-deployment.yaml  namespace.yaml         redis-service.yaml</span><br><span class="line">backend-service.yaml     frontend-service.yaml     redis-deployment.yaml  work-deployment.yaml</span><br></pre></td></tr></table></figure></div>

<p>进入配置文件所在目录后，开始创建相关资源：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl  apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl -n work get all</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-wsn2w   1&#x2F;1       Running   0          21s</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269&#x2F;TCP   6s</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            1           21s</span><br><span class="line"></span><br><span class="line">NAME                                     DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d   1         1         1         21s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 Redis 正在运行，并通过 <code>NodePort</code> 类型的 <code>Service</code> 暴露出来，我们访问来确认下。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">master $ docker run --rm -it --network host redis:alpine redis-cli -p 31269</span><br><span class="line">Unable to find image &#39;redis:alpine&#39; locally</span><br><span class="line">alpine: Pulling from library&#x2F;redis</span><br><span class="line">4fe2ade4980c: Already exists</span><br><span class="line">fb758dc2e038: Pull complete</span><br><span class="line">989f7b0c858b: Pull complete</span><br><span class="line">8dd99d530347: Pull complete</span><br><span class="line">7137334fa8f0: Pull complete</span><br><span class="line">30610ca64487: Pull complete</span><br><span class="line">Digest: sha256:8fd83c5986f444f1a5521e3eda7395f0f21ff16d33cc3b89d19ca7c58293c5dd</span><br><span class="line">Status: Downloaded newer image for redis:alpine</span><br><span class="line">127.0.0.1:31269&gt; set name kubernetes</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:31269&gt; get name </span><br><span class="line">&quot;kubernetes&quot;</span><br></pre></td></tr></table></figure></div>

<p>可以看到已经可以正常访问。接下来，我们来看下 <code>31269</code> 这个端口的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">master $ netstat  -ntlp |grep 31269</span><br><span class="line">tcp6       0      0 :::31269                :::*                    LISTEN      2716&#x2F;kube-proxy</span><br></pre></td></tr></table></figure></div>

<p>可以看到该端口是由 <code>kube-proxy</code> 所占用的。</p>
<p>接下来，查看当前集群的 <code>Service</code> 和 <code>Endpoint</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work get svc</span><br><span class="line">NAME           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269&#x2F;TCP   10m</span><br><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS        AGE</span><br><span class="line">saythx-redis   10.32.0.2:6379   10m</span><br><span class="line">master $ kubectl -n work get pod -o wide</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE</span><br><span class="line">saythx-redis-8558c7d7d-wsn2w   1&#x2F;1       Running   0          12m       10.32.0.2   node01    &lt;none&gt;</span><br></pre></td></tr></table></figure></div>

<p>可以很直观的看到 <code>Endpoint</code> 当中的便是 <code>Pod</code> 的 IP，现在我们将该服务进行扩容（实际情况下并不会这样处理）。</p>
<p>直接通过 <code>kubectl scale</code> 操作</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl  -n work scale --replicas&#x3D;2 deploy&#x2F;saythx-redis</span><br><span class="line">deployment.extensions&#x2F;saythx-redis scaled</span><br><span class="line">master $ kubectl  -n work get all</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-sslpj   1&#x2F;1       Running   0          10s</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-wsn2w   1&#x2F;1       Running   0          16m</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269&#x2F;TCP   16m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   2         2         2            2           16m</span><br></pre></td></tr></table></figure></div>

<p>查看 <code>Endpoint</code> 信息：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS                       AGE</span><br><span class="line">saythx-redis   10.32.0.2:6379,10.32.0.3:6379   17m</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Endpoint</code> 已经自动发生了变化，而这也意味着 <code>Service</code> 代理的后端节点将增加一个。</p>
<h2 id="kube-proxy-如何工作"><a href="#kube-proxy-如何工作" class="headerlink" title="kube-proxy 如何工作"></a><code>kube-proxy</code> 如何工作</h2><p><code>kube-proxy</code> 在 Linux 系统上当前支持三种模式，可通过 <code>--proxy-mode</code> 配置：</p>
<ul>
<li><code>userspace</code>：这是很早期的一种方案，但效率上显著不足，不推荐使用。</li>
<li><code>iptables</code>：当前的默认模式。比 <code>userspace</code> 要快，但问题是会给机器上产生很多 <code>iptables</code> 规则。</li>
<li><code>ipvs</code>：为了解决 <code>iptables</code> 的性能问题而引入，采用增量的方式进行更新。</li>
</ul>
<p>下面我们以 <code>iptables</code> 的模式稍作介绍。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">master $ iptables -t nat -L </span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SERVICES  all  --  anywhere             anywhere             &#x2F;* kubernetes service portals *&#x2F;</span><br><span class="line">DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SERVICES  all  --  anywhere             anywhere             &#x2F;* kubernetes service portals *&#x2F;</span><br><span class="line">DOCKER     all  --  anywhere            !127.0.0.0&#x2F;8          ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain POSTROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-POSTROUTING  all  --  anywhere             anywhere             &#x2F;* kubernetes postrouting rules *&#x2F;</span><br><span class="line">MASQUERADE  all  --  172.18.0.0&#x2F;24        anywhere</span><br><span class="line"></span><br><span class="line">Chain DOCKER (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">RETURN     all  --  anywhere             anywhere</span><br><span class="line"></span><br><span class="line">Chain KUBE-MARK-DROP (0 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">MARK       all  --  anywhere             anywhere             MARK or 0x8000</span><br><span class="line"></span><br><span class="line">Chain KUBE-MARK-MASQ (7 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">MARK       all  --  anywhere             anywhere             MARK or 0x4000</span><br><span class="line"></span><br><span class="line">Chain KUBE-NODEPORTS (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  tcp  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F; tcp dpt:31269</span><br><span class="line">KUBE-SVC-SMQNAAUIAENDDGYQ  tcp  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F; tcp dpt:31269</span><br><span class="line"></span><br><span class="line">Chain KUBE-POSTROUTING (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">MASQUERADE  all  --  anywhere             anywhere             &#x2F;* kubernetes service traffic requiring SNAT *&#x2F; mark match 0x4000&#x2F;0x4000</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-2LZPYBS4HUAJKDFL (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F; tcp to:10.32.0.2:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-3E4LNQKKWZF7G6SH (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.1            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F; tcp to:10.32.0.1:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-3IDG7DUGN3QC2UZF (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  172.17.0.120         anywhere             &#x2F;* default&#x2F;kubernetes:https *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* default&#x2F;kubernetes:https *&#x2F; tcp to:172.17.0.120:6443</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-JZWS2VPNIEMNMNB2 (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F;</span><br><span class="line">DNAT       udp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F; udp to:10.32.0.2:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-OEY6JJQSBCQPRKHS (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.1            anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F;</span><br><span class="line">DNAT       udp  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns *&#x2F; udp to:10.32.0.1:53</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-QX7VDAS5KDY6V3EV (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F;</span><br><span class="line">DNAT       tcp  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F; tcp to:10.32.0.2:6379</span><br><span class="line"></span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SVC-SMQNAAUIAENDDGYQ  tcp  --  anywhere             10.103.193.175       &#x2F;* work&#x2F;saythx-redis: cluster IP *&#x2F; tcp dpt:6379</span><br><span class="line">KUBE-NODEPORTS  all  --  anywhere             anywhere             &#x2F;* kubernetes service nodeports; NOTE: this must be the last rule in this chain *&#x2F; ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain KUBE-SVC-ERIFXISQEP7F7OF4 (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SEP-3E4LNQKKWZF7G6SH  all  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F; statistic mode random probability 0.50000000000</span><br><span class="line">KUBE-SEP-2LZPYBS4HUAJKDFL  all  --  anywhere             anywhere             &#x2F;* kube-system&#x2F;kube-dns:dns-tcp *&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Chain KUBE-SVC-SMQNAAUIAENDDGYQ (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SEP-QX7VDAS5KDY6V3EV  all  --  anywhere             anywhere             &#x2F;* work&#x2F;saythx-redis: *&#x2F;</span><br></pre></td></tr></table></figure></div>

<p>以上输出已经尽量删掉了无关的内容。</p>
<p>当开始访问的时候先要经过 <code>PREROUTING</code> 链，转到 <code>KUBE-SERVICES</code> 链，当查询到匹配的规则之后，请求将转向 <code>KUBE-SVC-SMQNAAUIAENDDGYQ</code> 链，进而到达 <code>KUBE-SEP-QX7VDAS5KDY6V3EV</code> 对应于我们的 <code>Pod</code>。(注：为了简洁，上述 iptables 规则是部署一个 <code>Pod</code> 时的场景)</p>
<p>当搞懂了这些之后，如果你想了解这些 <code>iptables</code> 规则实际又是如何创建和维护的，那可以参考下 <code>proxier</code> 的具体实现，这里不再展开。</p>
<h2 id="总结-15"><a href="#总结-15" class="headerlink" title="总结"></a>总结</h2><p>本节中我们介绍了 <code>kube-proxy</code> 的主要功能和基本流程，了解到了它对于服务注册发现和代理访问等起到了很大的作用。而它在 Linux 下的代理模式也有 <code>userspace</code>，<code>iptables</code> 和 <code>ipvs</code> 等。</p>
<p>默认情况下我们使用 <code>iptables</code> 的代理模式，当创建新的 <code>Service</code> ，或者 <code>Pod</code> 进行变化时，<code>kube-proxy</code> 便会去维护 <code>iptables</code> 规则，以确保请求可以正确的到达后端服务。</p>
<p>当然，本节中并没有提到 <code>kube-proxy</code> 的 <code>session affinity</code> 相关的特性，如有需要可进行下尝试。</p>
<p>下节，我们将介绍实际运行着容器的 <code>Docker</code>，大致了解下在 K8S 中它所起的作用，及他们之间的交互方式。</p>
<h1 id="庖丁解牛：Container-Runtime-（Docker）"><a href="#庖丁解牛：Container-Runtime-（Docker）" class="headerlink" title="庖丁解牛：Container Runtime （Docker）"></a>庖丁解牛：Container Runtime （Docker）</h1><h2 id="整体概览-10"><a href="#整体概览-10" class="headerlink" title="整体概览"></a>整体概览</h2><p>我们在第 3 节的时候，提到过 <code>Container Runtime</code> 的概念，也大致介绍过它的主要作用在于下载镜像，运行容器等。</p>
<p>经过我们前面的学习，<code>kube-scheduler</code> 决定了 <code>Pod</code> 将被调度到哪个 <code>Node</code> 上，而 <code>kubelet</code> 则负责 <code>Pod</code> 在此 <code>Node</code> 上可按预期工作。如果没有 <code>Container Runtime</code>，那 <code>Pod</code> 中的 <code>container</code> 在该 <code>Node</code> 上也便无法正常启动运行了。</p>
<p>本节中，我们以当前最为通用的 <code>Container Runtime</code> Docker 为例进行介绍。</p>
<h2 id="Container-Runtime-是什么"><a href="#Container-Runtime-是什么" class="headerlink" title="Container Runtime 是什么"></a>Container Runtime 是什么</h2><p><code>Container Runtime</code> 我们通常叫它容器运行时，而这一概念的产生也是由于容器化技术和 K8S 的大力发展，为了统一工业标准，也为了避免 K8S 绑定于特定的容器运行时，所以便成立了 <a href="https://www.opencontainers.org/" target="_blank" rel="noopener">Open Container Initiative (OCI)</a> 组织，致力于将容器运行时标准化和容器镜像标准化。</p>
<p>凡是遵守此标准的实现，均可由标准格式的镜像启动相应的容器，并完成一些特定的操作。</p>
<h2 id="Docker-是什么"><a href="#Docker-是什么" class="headerlink" title="Docker 是什么"></a>Docker 是什么</h2><p>Docker 是一个容器管理平台，它最初是被设计用于快速创建，发布和运行容器的工具，不过随着它的发展，其中集成了越来越多的功能。</p>
<p>Docker 也可以说是一个包含标准容器运行时的工具集，当前版本中默认的 <code>runtime</code> 称之为 <code>runc</code>。 关于 <code>runc</code> 相关的一些内容可参考<a href="http://moelove.info/2018/11/23/runc-1.0-rc6-发布之际/" target="_blank" rel="noopener">我之前的一篇文章</a>。</p>
<p>当然，这里提到了 <strong>默认的运行时</strong> 那也就意味着它可支持其他的运行时实现。</p>
<h2 id="CRI-是什么"><a href="#CRI-是什么" class="headerlink" title="CRI 是什么"></a>CRI 是什么</h2><p>说到这里，我们就会发现，K8S 作为目前云原生技术体系中最重要的一环，为了让它更有扩展性，当然也不会将自己完全局限于某一种特定的容器运行时。</p>
<p>自 K8S 1.5 （2016 年 11 月）开始，新增了一个容器运行时的插件 API，并称之为 <code>CRI</code> （Container Runtime Interface），通过 <code>CRI</code> 可以支持 <code>kubelet</code> 使用不同的容器运行时，而不需要重新编译。</p>
<p><code>CRI</code> 主要是基于 gRPC 实现了 <code>RuntimeService</code> 和 <code>ImageService</code> 这两个服务，可以参考 <code>pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto</code> 中的 API 定义。由于本节侧重于 <code>Container Runtime/Docker</code> 这里就不对 <code>CRI</code> 的具体实现进行展开了。</p>
<p>只要继续将 <code>kubelet</code> 当作 agent 的角色，而它与基于 <code>CRI</code> 实现的 <code>CRI shim</code> 服务进行通信理解即可。</p>
<h2 id="Docker-如何工作"><a href="#Docker-如何工作" class="headerlink" title="Docker 如何工作"></a>Docker 如何工作</h2><p>这里我们主要介绍在 K8S 中一些 Docker 常见的动作。</p>
<h3 id="部署一个-Redis"><a href="#部署一个-Redis" class="headerlink" title="部署一个 Redis"></a>部署一个 Redis</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl run redis --image&#x3D;redis</span><br><span class="line">deployment.apps&#x2F;redis created</span><br><span class="line">master $ kubectl get all</span><br><span class="line">NAME                        READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod&#x2F;redis-bb7894d65-7vsj8   0&#x2F;1       ContainerCreating   0          6s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443&#x2F;TCP   26m</span><br><span class="line"></span><br><span class="line">NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;redis   1         1         1            0           6s</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;redis-bb7894d65   1         1         0         6s</span><br></pre></td></tr></table></figure></div>

<p>我们直接使用 <code>kubectl run</code> 的方式部署了一个 Redis</p>
<h3 id="查看详情"><a href="#查看详情" class="headerlink" title="查看详情"></a>查看详情</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl describe pod&#x2F;redis-bb7894d65-7vsj8</span><br><span class="line">Name:               redis-bb7894d65-7vsj8</span><br><span class="line">Namespace:          default</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               node01&#x2F;172.17.0.21</span><br><span class="line">Start Time:         Sat, 15 Dec 2018 04:48:49 +0000</span><br><span class="line">Labels:             pod-template-hash&#x3D;663450821</span><br><span class="line">                    run&#x3D;redis</span><br><span class="line">Annotations:        &lt;none&gt;</span><br><span class="line">Status:             Running</span><br><span class="line">IP:                 10.40.0.1</span><br><span class="line">Controlled By:      ReplicaSet&#x2F;redis-bb7894d65</span><br><span class="line">Containers:</span><br><span class="line">  redis:</span><br><span class="line">    Container ID:   docker:&#x2F;&#x2F;ab87085456aca76825dd639bcde27160d9c2c84cac5388585bcc9ed3afda6522</span><br><span class="line">    Image:          redis</span><br><span class="line">    Image ID:       docker-pullable:&#x2F;&#x2F;redis@sha256:010a8bd5c6a9d469441aa35187d18c181e3195368bce309348b3ee639fce96e0</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sat, 15 Dec 2018 04:48:57 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount from default-token-zxt27 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             True</span><br><span class="line">  ContainersReady   True</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-zxt27:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-zxt27</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io&#x2F;not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io&#x2F;unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From               Message</span><br><span class="line">  ----    ------     ----  ----               -------</span><br><span class="line">  Normal  Scheduled  7m    default-scheduler  Successfully assigned default&#x2F;redis-bb7894d65-7vsj8to node01</span><br><span class="line">  Normal  Pulling    7m    kubelet, node01    pulling image &quot;redis&quot;</span><br><span class="line">  Normal  Pulled     7m    kubelet, node01    Successfully pulled image &quot;redis&quot;</span><br><span class="line">  Normal  Created    7m    kubelet, node01    Created container</span><br><span class="line">  Normal  Started    7m    kubelet, node01    Started container</span><br></pre></td></tr></table></figure></div>

<p>可以通过 <code>kubectl describe</code> 查看该 <code>Pod</code> 的事件详情。这里主要有几个阶段。</p>
<h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Normal  Scheduled  7m    default-scheduler  Successfully assigned default&#x2F;redis-bb7894d65-7vsj8to node01</span><br></pre></td></tr></table></figure></div>

<p>在第 15 小节 <code>kube-scheduler</code> 中我们介绍过，通过 <code>kube-scheduler</code> 可以决定 <code>Pod</code> 会调度到哪个 <code>Node</code>。本例中，<code>redis-bb7894d65-7vsj8to</code> 被调度到了 <code>node01</code>。</p>
<h4 id="pull-镜像"><a href="#pull-镜像" class="headerlink" title="pull 镜像"></a>pull 镜像</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Normal  Pulling    7m    kubelet, node01    pulling image &quot;redis&quot;</span><br><span class="line">Normal  Pulled     7m    kubelet, node01    Successfully pulled image &quot;redis&quot;</span><br></pre></td></tr></table></figure></div>

<p>这里 <code>kubelet</code> 及该节点上的 <code>Container Runtime</code> （Docker）开始发挥作用，先拉取镜像。如果此刻你登录 <code>node01</code> 的机器，执行 <code>docker pull redis</code> 便可同步看到拉取进度。</p>
<h4 id="创建镜像并启动"><a href="#创建镜像并启动" class="headerlink" title="创建镜像并启动"></a>创建镜像并启动</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Normal  Created    7m    kubelet, node01    Created container</span><br><span class="line">Normal  Started    7m    kubelet, node01    Started container</span><br></pre></td></tr></table></figure></div>

<p>拉取镜像完成后，便会开始创建并启动该容器，并返回任务结果。此刻登录 <code>node01</code> 机器，便会看到当前在运行的容器了。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node01 $ docker ps |grep redis</span><br><span class="line">ab87085456ac        redis@sha256:010a8bd5c6a9d469441aa35187d18c181e3195368bce309348b3ee639fce96e0  &quot;docker-entrypoint...&quot;   19 minutes ago      Up 19 minutes                           k8s_redis_redis-bb7894d65-7vsj8_default_b693b56c-0024-11e9-9bab-0242ac11000a_0</span><br><span class="line">8f264abd82fe        k8s.gcr.io&#x2F;pause:3.1  &quot;&#x2F;pause&quot;                 19 minutes ago      Up 19 minutes                           k8s_POD_redis-bb7894d65-7vsj8_default_b693b56c-0024-11e9-9bab-0242ac11000a_0</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-16"><a href="#总结-16" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了 <code>Container Runtime</code> 的基本概念，及 K8S 为了能增加扩展性，提供了统一的 <code>CRI</code> 插件接口，可用于支持多种容器运行时。</p>
<p>当前使用最为广泛的是 <a href="https://github.com/moby/moby/"><code>Docker</code></a>，当前还支持的主要有 <a href="https://github.com/opencontainers/runc"><code>runc</code></a>，<a href="https://github.com/containerd/containerd"><code>Containerd</code></a>，<a href="https://github.com/hyperhq/runv"><code>runV</code></a> 以及 <a href="https://github.com/rkt/rkt"><code>rkt</code></a> 等。</p>
<p>由于 Docker 的知识点很多，关于 Docker 的实践和内部原理可参考我之前的一次分享 <a href="https://github.com/tao12345666333/slides/raw/master/2018.09.13-Tech-Talk-Time/Docker实战和基本原理-张晋涛.pdf">Docker 实战和基本原理</a>。</p>
<p>在使用 K8S 时，也有极个别情况需要通过排查 Docker 的日志来分析问题。</p>
<p>至此，K8S 中主要的核心组件我们已经介绍完毕，下节我们主要集中于在 K8S 环境中，如何定位和解决问题，以及类似刚才提到的需要通过 Docker 进行排查问题的情况。</p>
<h1 id="Troubleshoot"><a href="#Troubleshoot" class="headerlink" title="Troubleshoot"></a>Troubleshoot</h1><h2 id="整体概览-11"><a href="#整体概览-11" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的介绍，我们已经了解到了 K8S 的基础知识，核心组件原理以及如何在 K8S 中部署服务及管理服务等。</p>
<p>但在生产环境中，我们所面临的环境多种多样，可能会遇到各种问题。本节将结合我们已经了解到的知识，介绍一些常见问题定位和解决的思路或方法，以便大家在生产中使用 K8S 能如鱼得水。</p>
<h2 id="应用部署问题"><a href="#应用部署问题" class="headerlink" title="应用部署问题"></a>应用部署问题</h2><p>首先我们从应用部署相关的问题来入手。这里仍然使用我们的<a href="https://github.com/tao12345666333/saythx">示例项目 SayThx</a>。</p>
<p>clone 该项目，进入到 deploy 目录中，先 <code>kubectl apply -f namespace.yaml</code> 或者 <code>kubectl create ns work</code> 来创建一个用于实验的 <code>Namespace</code> 。</p>
<h3 id="使用-describe-排查问题"><a href="#使用-describe-排查问题" class="headerlink" title="使用 describe 排查问题"></a>使用 <code>describe</code> 排查问题</h3><p>对 <code>redis-deployment.yaml</code> 稍作修改，按以下方式操作：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl -n work get all</span><br><span class="line">NAME                                READY     STATUS             RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-7574c98f5d-v66fx   0&#x2F;1       ImagePullBackOff   0          9s</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            0           9s</span><br><span class="line"></span><br><span class="line">NAME                                      DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-7574c98f5d   1         1         0         9s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 此刻的状态是 <code>ImagePullBackOff</code>，这个状态表示镜像拉取失败，<code>kubelet</code> 退出镜像拉取。</p>
<p>我们在前面的内容中介绍过 <code>kubelet</code> 的作用之一就是负责镜像拉取，而实际上，在镜像方面的错误主要预设了 6 种，分别是 <code>ImagePullBackOff</code>，<code>ImageInspectError</code>，<code>ErrImagePull</code>，<code>ErrImageNeverPull</code>，<code>RegistryUnavailable</code>，<code>InvalidImageName</code>。</p>
<p>当遇到以上所述情况时，便可定位为镜像相关异常。</p>
<p>我们回到上面的问题当中，定位问题所在。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work describe pod&#x2F;saythx-redis-7574c98f5d-v66fx</span><br><span class="line">Name:               saythx-redis-7574c98f5d-v66fx</span><br><span class="line">Namespace:          work</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               node01&#x2F;172.17.0.132</span><br><span class="line">Start Time:         Tue, 18 Dec 2018 17:27:56 +0000</span><br><span class="line">Labels:             app&#x3D;redis</span><br><span class="line">                    pod-template-hash&#x3D;3130754918</span><br><span class="line">Annotations:        &lt;none&gt;</span><br><span class="line">Status:             Pending</span><br><span class="line">IP:                 10.40.0.1</span><br><span class="line">Controlled By:      ReplicaSet&#x2F;saythx-redis-7574c98f5d</span><br><span class="line">Containers:</span><br><span class="line">  redis:</span><br><span class="line">    Container ID:</span><br><span class="line">    Image:          redis:5xx</span><br><span class="line">    Image ID:</span><br><span class="line">    Port:           6379&#x2F;TCP</span><br><span class="line">    Host Port:      0&#x2F;TCP</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       ImagePullBackOff</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount from default-token-787w5 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             False</span><br><span class="line">  ContainersReady   False</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-787w5:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-787w5</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io&#x2F;not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io&#x2F;unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason          Age                 From               Message</span><br><span class="line">  ----     ------          ----                ----               -------</span><br><span class="line">  Normal   Scheduled       11m                 default-scheduler  Successfully assigned work&#x2F;saythx-redis-7574c98f5d-v66fx to node01</span><br><span class="line">  Normal   SandboxChanged  10m                 kubelet, node01    Pod sandbox changed, it will bekilled and re-created.</span><br><span class="line">  Normal   BackOff         9m (x6 over 10m)    kubelet, node01    Back-off pulling image &quot;redis:5xx&quot;</span><br><span class="line">  Normal   Pulling         9m (x4 over 10m)    kubelet, node01    pulling image &quot;redis:5xx&quot;</span><br><span class="line">  Warning  Failed          9m (x4 over 10m)    kubelet, node01    Failed to pull image &quot;redis:5xx&quot;: rpc error: code &#x3D; Unknown desc &#x3D; Error response from daemon: manifest for redis:5xx not found</span><br><span class="line">  Warning  Failed          9m (x4 over 10m)    kubelet, node01    Error: ErrImagePull</span><br><span class="line">  Warning  Failed          49s (x44 over 10m)  kubelet, node01    Error: ImagePullBackOff</span><br></pre></td></tr></table></figure></div>

<p>可以看到我们现在 pull 的镜像是 <code>redis:5xx</code> 而实际上并不存在此 tag 的镜像，所以导致拉取失败。</p>
<h3 id="使用-events-排查问题"><a href="#使用-events-排查问题" class="headerlink" title="使用 events 排查问题"></a>使用 <code>events</code> 排查问题</h3><p>当然，我们还有另一种方式同样可进行问题排查：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work get events</span><br><span class="line">LAST SEEN   FIRST SEEN   COUNT     NAME                                             KIND         SUBOBJECT                TYPE      REASON              SOURCE                  MESSAGE</span><br><span class="line">21m         21m          1         saythx-redis.15717d6361a741a8                    Deployment                        Normal    ScalingReplicaSet   deployment-controller   Scaled up replica set saythx-redis-7574c98f5d to 1</span><br><span class="line">21m         21m          1         saythx-redis-7574c98f5d-qwxgm.15717d6363eb60ff   Pod                        Normal    Scheduled           default-scheduler       Successfully assigned work&#x2F;saythx-redis-7574c98f5d-qwxgm to node01</span><br><span class="line">21m         21m          1         saythx-redis-7574c98f5d.15717d636309afa8         ReplicaSet                        Normal    SuccessfulCreate    replicaset-controller   Created pod: saythx-redis-7574c98f5d-qwxgm</span><br><span class="line">20m         21m          2         saythx-redis-7574c98f5d-qwxgm.15717d63fa501b3f   Pod          spec.containers&#123;redis&#125;   Normal    BackOff             kubelet, node01         Back-off pulling image &quot;redis:5xx&quot;</span><br><span class="line">20m         21m          2         saythx-redis-7574c98f5d-qwxgm.15717d63fa5049a9   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ImagePullBackOff</span><br><span class="line">20m         21m          3         saythx-redis-7574c98f5d-qwxgm.15717d6393a1993c   Pod          spec.containers&#123;redis&#125;   Normal    Pulling             kubelet, node01         pulling image &quot;redis:5xx&quot;</span><br><span class="line">20m         21m          3         saythx-redis-7574c98f5d-qwxgm.15717d63e11efc7a   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ErrImagePull</span><br><span class="line">20m         21m          3         saythx-redis-7574c98f5d-qwxgm.15717d63e11e9c25   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Failed to pull image &quot;redis:5xx&quot;: rpc error: code &#x3D; Unknown desc &#x3D; Error response from daemon: manifest for redis:5xxnot found</span><br><span class="line">20m         20m          1         saythx-redis-54984ff94-2bb6g.15717d6dc03799cd    Pod          spec.containers&#123;redis&#125;   Normal    Killing             kubelet, node01         Killing container with id docker:&#x2F;&#x2F;redis:Need to kill Pod</span><br><span class="line">19m         19m          1         saythx-redis-7574c98f5d-v66fx.15717d72356528ec   Pod                        Normal    Scheduled           default-scheduler       Successfully assigned work&#x2F;saythx-redis-7574c98f5d-v66fx to node01</span><br><span class="line">19m         19m          1         saythx-redis-7574c98f5d.15717d722f7f1732         ReplicaSet                        Normal    SuccessfulCreate    replicaset-controller   Created pod: saythx-redis-7574c98f5d-v66fx</span><br><span class="line">19m         19m          1         saythx-redis.15717d722b49e758                    Deployment                        Normal    ScalingReplicaSet   deployment-controller   Scaled up replica set saythx-redis-7574c98f5d to 1</span><br><span class="line">19m         19m          1         saythx-redis-7574c98f5d-v66fx.15717d731a09b0ad   Pod                        Normal    SandboxChanged      kubelet, node01         Pod sandbox changed, it will be killed and re-created.</span><br><span class="line">18m         19m          6         saythx-redis-7574c98f5d-v66fx.15717d733ab20b3d   Pod          spec.containers&#123;redis&#125;   Normal    BackOff             kubelet, node01         Back-off pulling image &quot;redis:5xx&quot;</span><br><span class="line">18m         19m          4         saythx-redis-7574c98f5d-v66fx.15717d729de13541   Pod          spec.containers&#123;redis&#125;   Normal    Pulling             kubelet, node01         pulling image &quot;redis:5xx&quot;</span><br><span class="line">18m         19m          4         saythx-redis-7574c98f5d-v66fx.15717d72e6ded95d   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ErrImagePull</span><br><span class="line">18m         19m          4         saythx-redis-7574c98f5d-v66fx.15717d72e6de7b1c   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Failed to pull image &quot;redis:5xx&quot;: rpc error: code &#x3D; Unknown desc &#x3D; Error response from daemon: manifest for redis:5xxnot found</span><br><span class="line">4m          19m          66        saythx-redis-7574c98f5d-v66fx.15717d733ab23f2c   Pod          spec.containers&#123;redis&#125;   Warning   Failed              kubelet, node01         Error: ImagePullBackOff</span><br><span class="line">master</span><br></pre></td></tr></table></figure></div>

<p>我们在之前介绍时，也提到过 <code>kubelet</code> 或者 <code>kube-scheduler</code> 等组件会接受某些事件等，<code>event</code> 便是用于记录集群内各处发生的事件之类的。</p>
<h3 id="修正错误"><a href="#修正错误" class="headerlink" title="修正错误"></a>修正错误</h3><ul>
<li><p>修正配置文件</p>
<p>修正配置文件，然后 <code>kubectl apply -f redis-deployment.yaml</code> 便可应用修正后的配置文件。这种方法比较推荐，并且可以将修改过的位置纳入到版本控制系统中，有利于后续维护。</p>
</li>
<li><p>在线修改配置</p>
<p>使用 <code>kubectl -n work edit deploy/saythx-redis</code>，会打开默认的编辑器，我们可以将使用的镜像及 tag 修正为 <code>redis:5</code> 保存退出，便会自动应用新的配置。这种做法比较适合比较紧急或者资源是直接通过命令行创建等情况。 <strong>非特殊情况尽量不要在线修改。</strong> 且这样修改并不利于后期维护。</p>
</li>
</ul>
<h3 id="通过详细内容排查错误"><a href="#通过详细内容排查错误" class="headerlink" title="通过详细内容排查错误"></a>通过详细内容排查错误</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ vi redis-service.yaml # 稍微做了点修改</span><br><span class="line">master $ kubectl apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl -n work get pods,svc</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-z8prg   1&#x2F;1       Running   0          47s</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.108.202.170   &lt;none&gt;        6379:32355&#x2F;TCP   16s</span><br></pre></td></tr></table></figure></div>

<p>通过以上的输出，大多数情况下我们的 <code>Service</code> 应该是可以可以正常访问了，现在我们进行下测试：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">master $ docker run --rm -it --net host redis redis-cli -p 32355</span><br><span class="line">Unable to find image &#39;redis:latest&#39; locally</span><br><span class="line">latest: Pulling from library&#x2F;redis</span><br><span class="line">a5a6f2f73cd8: Pull complete</span><br><span class="line">a6d0f7688756: Pull complete</span><br><span class="line">53e16f6135a5: Pull complete</span><br><span class="line">f52b0cc4e76a: Pull complete</span><br><span class="line">e841feee049e: Pull complete</span><br><span class="line">ccf45e5191d0: Pull complete</span><br><span class="line">Digest: sha256:bf65ecee69c43e52d0e065d094fbdfe4df6e408d47a96e56c7a29caaf31d3c35</span><br><span class="line">Status: Downloaded newer image for redis:latest</span><br><span class="line">Could not connect to Redis at 127.0.0.1:32355: Connection refused</span><br><span class="line">not connected&gt;</span><br></pre></td></tr></table></figure></div>

<p>我们先来介绍这里的测试方法。 使用 Docker 的 Redis 官方镜像， <code>--net host</code> 是使用宿主机网络； <code>--rm</code> 表示停止完后即清除； <code>-it</code> 分别表示获取输入及获取 TTY。</p>
<p>通过以上测试发现不能正常连接，故而说明 <code>Service</code> 还是未配置好。使用前面提到的方法也可以进行排查，不过这里提供另一种排查这类问题的思路。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS        AGE</span><br><span class="line">saythx-redis   10.32.0.4:6380   9m</span><br></pre></td></tr></table></figure></div>

<p>通过之前的章节，我们已经知道 <code>Service</code> 工作的时候是按 <code>Endpoints</code> 来的，这里我们发现此处的 <code>Endpoints</code> 是 <code>6380</code> 与我们预期的 <code>6379</code> 并不相同。所以问题定位于端口配置有误。</p>
<p>前面已经说过修正方法了，不再赘述。当修正完成后，再次验证：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work get endpoints</span><br><span class="line">NAME           ENDPOINTS        AGE</span><br><span class="line">saythx-redis   10.32.0.4:6379   15m</span><br></pre></td></tr></table></figure></div>

<p><code>Endpoints</code> 已经正常，验证下服务是否可用：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ docker run --rm -it --net host redis redis-cli -p 32355</span><br><span class="line">127.0.0.1:32355&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure></div>

<p>验证无误。</p>
<h2 id="集群问题"><a href="#集群问题" class="headerlink" title="集群问题"></a>集群问题</h2><p>由于我们有多个节点，况且在集群搭建和维护过程中，也会比较常见到集群相关的问题。这里我们先举个实际例子进行分析：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl get nodes</span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    Ready      master    58m       v1.11.3</span><br><span class="line">node01    NotReady   &lt;none&gt;    58m       v1.11.3</span><br></pre></td></tr></table></figure></div>

<p>通过 kubectl 查看，发现有一个节点 NotReady ，这在搭建集群的过程中也有可能遇到。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl get  node&#x2F;node01 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubeadm.alpha.kubernetes.io&#x2F;cri-socket: &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">    node.alpha.kubernetes.io&#x2F;ttl: &quot;0&quot;</span><br><span class="line">    volumes.kubernetes.io&#x2F;controller-managed-attach-detach: &quot;true&quot;</span><br><span class="line">  creationTimestamp: 2018-12-19T16:46:59Z</span><br><span class="line">  labels:</span><br><span class="line">    beta.kubernetes.io&#x2F;arch: amd64</span><br><span class="line">    beta.kubernetes.io&#x2F;os: linux</span><br><span class="line">    kubernetes.io&#x2F;hostname: node01</span><br><span class="line">  name: node01</span><br><span class="line">  resourceVersion: &quot;4850&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;node01</span><br><span class="line">  uid: b440d3d5-03ad-11e9-917e-0242ac110035</span><br><span class="line">spec: &#123;&#125;</span><br><span class="line">status:</span><br><span class="line">  addresses:</span><br><span class="line">  - address: 172.17.0.66</span><br><span class="line">    type: InternalIP</span><br><span class="line">  - address: node01</span><br><span class="line">    type: Hostname</span><br><span class="line">  allocatable:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: &quot;89032026784&quot;</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3894652Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  capacity:</span><br><span class="line">    cpu: &quot;4&quot;</span><br><span class="line">    ephemeral-storage: 96605932Ki</span><br><span class="line">    hugepages-1Gi: &quot;0&quot;</span><br><span class="line">    hugepages-2Mi: &quot;0&quot;</span><br><span class="line">    memory: 3997052Ki</span><br><span class="line">    pods: &quot;110&quot;</span><br><span class="line">  conditions:</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: OutOfDisk</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: MemoryPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: DiskPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T16:46:59Z</span><br><span class="line">    message: kubelet has sufficient PID available</span><br><span class="line">    reason: KubeletHasSufficientPID</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: PIDPressure</span><br><span class="line">  - lastHeartbeatTime: 2018-12-19T17:42:16Z</span><br><span class="line">    lastTransitionTime: 2018-12-19T17:43:00Z</span><br><span class="line">    message: Kubelet stopped posting node status.</span><br><span class="line">    reason: NodeStatusUnknown</span><br><span class="line">    status: Unknown</span><br><span class="line">    type: Ready</span><br><span class="line">  daemonEndpoints:</span><br><span class="line">    kubeletEndpoint:</span><br><span class="line">      Port: 10250</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></div>

<p>我们之前介绍 <code>kubelet</code> 时说过， <code>kubelet</code> 的作用之一便是将自身注册至 <code>kube-apiserver</code>。</p>
<p>这里的 message 信息说明 <code>kubelet</code> 不再向 <code>kube-apiserver</code> 发送心跳包之类的了，所以被判定为 NotReady 的状态。</p>
<p>接下来，我们登录 node01 机器查看 <code>kubelet</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">node01 $ systemctl status kubelet</span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kubelet.service; enabled; vendor preset: enabled)</span><br><span class="line">  Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d</span><br><span class="line">           └─kubeadm.conf</span><br><span class="line">   Active: inactive (dead) since Wed 2018-12-19 17:42:17 UTC; 18min ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;home&#x2F;</span><br><span class="line">  Process: 1693 ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_</span><br><span class="line"> Main PID: 1693 (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br></pre></td></tr></table></figure></div>

<p>可以看到该机器上 <code>kubelet</code> 没有启动。现在将其启动，稍等片刻看看节群中 <code>Node</code> 的状态。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    1h        v1.11.3</span><br><span class="line">node01    Ready     &lt;none&gt;    1h        v1.11.3</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-17"><a href="#总结-17" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了 K8S 中常用的问题排查和解决思路，但实际生产环境中情况会有和更多不确定因素，掌握本节中介绍的基础，有利于之后生产环境中进行常规问题的排查。</p>
<p>当然，本节只是介绍通过 kubectl 来定位和解决问题，个别情况下我们需要登录相关的节点，实际去使用 <code>Docker</code> 工具等进行问题的详细排查。</p>
<p>至此，K8S 的基础原理和常规问题排查思路等都已经通过包括本节在内的 19 小节介绍完毕，相信你现在已经迫不及待的想要使用 K8S 了。</p>
<p>不过 kubectl 作为命令行工具也许有些人会不习惯使用，下节，我们将介绍 K8S 的扩展组件 <code>kube-dashboard</code> 了解它的主要功能及带给我们的便利。</p>
<h1 id="扩展增强：Dashboard"><a href="#扩展增强：Dashboard" class="headerlink" title="扩展增强：Dashboard"></a>扩展增强：Dashboard</h1><h2 id="整体概览-12"><a href="#整体概览-12" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的介绍，想必你已经迫不及待的想要将应用部署至 K8S 中，但总是使用 <code>kubectl</code> 或者 <code>Helm</code> 等命令行工具也许不太直观，你可能想要一眼就看到集群当前的状态，或者想要更方便的对集群进行管理。</p>
<p>本节将介绍一个 Web 项目 <a href="https://github.com/kubernetes/dashboard"><code>Dashboard</code></a> 可用于部署容器化的应用程序，管理集群中的资源，甚至是排查和解决问题。</p>
<p>当然它和大多数 Dashboard 类的项目类似，也为集群的状态提供了一个很直观的展示。</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/23/167d6b6e66c60e89?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><p>要想使用 Dashboard，首先我们需要安装它，而 Dashboard 的安装其实也很简单。不过对于国内用户需要注意的是需要解决网络问题，或替换镜像地址等。</p>
<p>这里我们安装当前最新版 <code>v1.10.1</code> 的 Dashboard：</p>
<ul>
<li><p>对于已经解决网络问题的用户：</p>
<p>可使用官方推荐做法进行安装，以下链接是使用了我提交了 path 的版本，由于官方最近的一次更新导致配置文件中的镜像搞错了。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;tao12345666333&#x2F;dashboard&#x2F;67970554aa9275cccec1d1ee5fbf89ae81b3b614&#x2F;aio&#x2F;deploy&#x2F;recommended&#x2F;kubernetes-dashboard.yaml</span><br><span class="line">secret&#x2F;kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount&#x2F;kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard created</span><br><span class="line">service&#x2F;kubernetes-dashboard created</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>也可使用我修改过的这份（使用 Docker Hub 同步了镜像）仓库地址 <a href="https://github.com/tao12345666333/k8s-dashboard">GitHub</a>, 国内 <a href="https://gitee.com/K8S-release/k8s-dashboard" target="_blank" rel="noopener">Gitee</a>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f https:&#x2F;&#x2F;gitee.com&#x2F;K8S-release&#x2F;k8s-dashboard&#x2F;raw&#x2F;master&#x2F;kubernetes-dashboard.yaml</span><br><span class="line">secret&#x2F;kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount&#x2F;kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard created</span><br><span class="line">service&#x2F;kubernetes-dashboard created</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<p>当已经执行完以上步骤后，可检查下是否安装成功：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system get all  -l k8s-app&#x3D;kubernetes-dashboard</span><br><span class="line">NAME                                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;kubernetes-dashboard-67896bc598-dhdpz   1&#x2F;1       Running   0          3m</span><br><span class="line"></span><br><span class="line">NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service&#x2F;kubernetes-dashboard   ClusterIP   10.109.92.207   &lt;none&gt;        443&#x2F;TCP   3m</span><br><span class="line"></span><br><span class="line">NAME                                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard   1         1         1            1           3m</span><br><span class="line"></span><br><span class="line">NAME                                              DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;kubernetes-dashboard-67896bc598   1         1         1         3m</span><br></pre></td></tr></table></figure></div>

<p>可以看到 <code>Pod</code> 已经在正常运行，接下来便是访问 Dashboard.</p>
<h2 id="访问-Dashboard"><a href="#访问-Dashboard" class="headerlink" title="访问 Dashboard"></a>访问 Dashboard</h2><p>以当前的部署方式，<code>Service</code> 使用了 <code>ClusterIP</code> 的类型，所以在集群外不能直接访问。我们先使用 <code>kubectl</code> 提供的 <code>port-forward</code> 功能进行访问。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system port-forward pod&#x2F;kubernetes-dashboard-67896bc598-dhdpz 8443</span><br><span class="line">Forwarding from 127.0.0.1:8443 -&gt; 8443</span><br><span class="line">Forwarding from [::1]:8443 -&gt; 8443</span><br></pre></td></tr></table></figure></div>

<p>还记得，我们在第 5 节时候安装过一个名为 <code>socat</code> 的依赖项吗？ <code>socat</code> 的主要功能便是端口转发。</p>
<p>现在在浏览器打开 <a href="https://127.0.0.1:8443/" target="_blank" rel="noopener"><code>https://127.0.0.1:8443</code></a> 便可看到如下的登录界面。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1176" height="565"></svg>)</p>
<p>对于我们的 <strong>新版本</strong> 而言，我们 <strong>使用令牌登录</strong> 的方式。</p>
<h3 id="查找-Token"><a href="#查找-Token" class="headerlink" title="查找 Token"></a>查找 Token</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system get serviceaccount -l k8s-app&#x3D;kubernetes-dashboard -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  kind: ServiceAccount</span><br><span class="line">  metadata:</span><br><span class="line">    annotations:</span><br><span class="line">      kubectl.kubernetes.io&#x2F;last-applied-configuration: |</span><br><span class="line">        &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;ServiceAccount&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;k8s-app&quot;:&quot;kubernetes-dashboard&quot;&#125;,&quot;name&quot;:&quot;kubernetes-dashboard&quot;,&quot;namespace&quot;:&quot;kube-system&quot;&#125;&#125;</span><br><span class="line">    creationTimestamp: 2018-12-20T17:27:14Z</span><br><span class="line">    labels:</span><br><span class="line">      k8s-app: kubernetes-dashboard</span><br><span class="line">    name: kubernetes-dashboard</span><br><span class="line">    namespace: kube-system</span><br><span class="line">    resourceVersion: &quot;1400&quot;</span><br><span class="line">    selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;serviceaccounts&#x2F;kubernetes-dashboard</span><br><span class="line">    uid: 7e01ddda-047c-11e9-b55c-0242ac11002a</span><br><span class="line">  secrets:</span><br><span class="line">  - name: kubernetes-dashboard-token-6ck2l</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure></div>

<p>首先，我们查看刚才创建出的 <code>serviceaccount</code> 可以看到其中有配置 <code>secrets</code> 。</p>
<p>查看该 <code>secret</code> 详情获得 Token</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system describe secrets kubernetes-dashboard-token-6ck2l</span><br><span class="line">Name:         kubernetes-dashboard-token-6ck2l</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io&#x2F;service-account.name&#x3D;kubernetes-dashboard</span><br><span class="line">              kubernetes.io&#x2F;service-account.uid&#x3D;7e01ddda-047c-11e9-b55c-0242ac11002a</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io&#x2F;service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">ca.crt:     1025 bytes</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi02Y2sybCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjdlMDFkZGRhLTA0N2MtMTFlOS1iNTVjLTAyNDJhYzExMDAyYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.WZ5YRUkGlKRSpkBFCk3BrZ6p2t1qVxEs7Kb18DP5X2C2lfMhDrB931PeN05uByLD6biz_4IQvKh4xmvY2RqekfV1BLCfcIiMUbc1lcXGbhH4g4vrsjYx3NZifaBh_5HuBlEL5zs5e_zFkPEhhIqjsY3KueFEuGwxTAsqGBQwawc-v6wqzB3Gzb01o1iN5aTb37PVG5gTTE8cQLih_urKhvdNEKBSRg_zHQlYjFrtUUWYRYMlYz_sWmamYVXHy_7NvKrBfw44WU5tLxMITkoUEGVwROBnHf_BcWVedozLg2uLVontB12YvhmTfJCDEAJ8o937bS-Fq3tLfu_xM40fqw</span><br></pre></td></tr></table></figure></div>

<p>将此处的 token 填入输入框内便可登录，<strong>注意这里使用的是 <code>describe</code>。</strong></p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="635"></svg>)</p>
<h3 id="修正权限"><a href="#修正权限" class="headerlink" title="修正权限"></a>修正权限</h3><p>但是我们注意到这里有很多提示 <code>configmaps is forbidden: User &quot;system:serviceaccount:kube-system:kubernetes-dashboard&quot; cannot list resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;default&quot;</code> 。根据我们前面的介绍，这很明显就是用户权限不足。</p>
<p>我们已经知道，当前我们的集群是开启了 <code>RBAC</code> 的，所以这里我们还是以前面学到的方法创建一个用户并进行授权。</p>
<ul>
<li><p>创建 ServiceAccount：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>创建 RoleBinding: 这里为了方便直接绑定了 <code>cluster-admin</code> 的 ClusterRole ，但是生产环境下，请按照实际情况进行授权，参考前面第 8 节相关的内容。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<p>使用以上配置创建了用户和绑定，然后还是同样的办法获取 Token。</p>
<p>点击 Dashboard 右上角，退出登录后，重新使用新的 Token 进行登录。登录完成后便可看到如下图：</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="607"></svg>)</p>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><p>点击右上角的 <strong>+创建</strong> 可进入创建页面，现在支持三种模式：从文本框输入；从文件创建；直接创建应用。</p>
<p>我们仍然以我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 为例。先 <code>clone</code> 该项目，并进入项目的 <code>deploy</code> 目录中。将 <code>namespace.yaml</code> 的内容复制进输入框，点击上传按钮，便可创建名为 <code>work</code> 的 <code>Namespace</code> 了。</p>
<p>通过以下命令验证：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl get ns</span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">default       Active    2h</span><br><span class="line">kube-public   Active    2h</span><br><span class="line">kube-system   Active    2h</span><br><span class="line">work          Active    10s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 Namespace 已经创建成功。或者刷新下网页，点击左侧的命名空间即可看到当前的所有 <code>Namespace</code> 。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="196" height="246"></svg>)</p>
<p>我们先将左侧的命名空间选择为 <strong>全部命名空间</strong> 或 <strong>work</strong> (当刷新过网页后) ，接下来继续点击右上角的 <strong>+创建</strong> 按钮，将 <code>redis-deployment.yaml</code> 的内容复制进输入框，点击上传按钮，部署 Redis 。</p>
<p>部署成功后，点击 部署 ，点击刚才的 <code>saythx-redis</code> 便可看到其详情。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="603"></svg>)</p>
<p>点击左侧的容器组，便可看到刚才部署的 Pod，</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="612"></svg>)</p>
<p>在此页面的右上角，可以点击命令行按钮，打开新标签页进入其内部执行命令。</p>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="609"></svg>)</p>
<p>或者是点击日志按钮，可打开新标签页，查看日志。</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/23/167d6bafed569ab1?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="总结-18"><a href="#总结-18" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了 <code>Kubernetes Dashboard</code> 的基本功能，以及如何安装和使用它。</p>
<p>Dashboard 相比 <code>kubectl</code> 为用户提供了一种相对直观的 Web 端操作方式，但是并不能完全取代 <code>kubectl</code>，这两者应该是相辅相成的。</p>
<p>如果你所需的功能相对简单或是想要给某些用户提供一种通过 Web 操作的方式，那便推荐使用 Dashboard。Dashboard 的后端使用了 K8S 的 <a href="https://github.com/kubernetes/client-go"><code>client-go</code></a> ，前端主要使用了 <a href="https://angular.io/" target="_blank" rel="noopener">Angular</a>，有兴趣可以大致看看其源代码，对于开发基于 K8S 的云平台会有些启发。</p>
<p>下节，我们将介绍用于 DNS 和服务发现的插件 <a href="https://coredns.io/" target="_blank" rel="noopener">CoreDNS</a>，学习如何利用它完成这些需求。并且它在 K8S 1.13 版本中，已经成为了默认的 DNS server。</p>
<h1 id="扩展增强：CoreDNS"><a href="#扩展增强：CoreDNS" class="headerlink" title="扩展增强：CoreDNS"></a>扩展增强：CoreDNS</h1><h2 id="整体概览-13"><a href="#整体概览-13" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们知道在 K8S 中有一套默认的<a href="https://github.com/kubernetes/dns">集群内 DNS 服务</a>，我们通常把它叫做 <code>kube-dns</code>，它基于 SkyDNS，为我们在服务注册发现方面提供了很大的便利。</p>
<p>比如，在我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 中，各组件便是依赖 DNS 进行彼此间的调用。</p>
<p>本节，我们将介绍的 <a href="https://coredns.io/" target="_blank" rel="noopener">CoreDNS</a> 是 CNCF 旗下又一孵化项目，在 K8S 1.9 版本中加入并进入 Alpha 阶段。我们当前是以 K8S 1.11 的版本进行介绍，它并不是默认的 DNS 服务，但是<a href="https://github.com/kubernetes/enhancements/issues/427">它作为 K8S 的 DNS 插件的功能已经 GA</a> 。</p>
<p>CoreDNS 在 K8S 1.13 版本中才正式成为<a href="https://kubernetes.io/blog/2018/12/03/kubernetes-1-13-release-announcement/" target="_blank" rel="noopener">默认的 DNS 服务</a>。</p>
<h2 id="CoreDNS-是什么"><a href="#CoreDNS-是什么" class="headerlink" title="CoreDNS 是什么"></a>CoreDNS 是什么</h2><p>首先，我们需要明确 CoreDNS 是一个独立项目，它不仅可支持在 K8S 中使用，你也可以在你任何需要 DNS 服务的时候使用它。</p>
<p>CoreDNS 使用 Go 语言实现，部署非常方便。</p>
<p>它的扩展性很强，很多功能特性都是通过插件完成的，它不仅有大量的<a href="https://coredns.io/plugins/" target="_blank" rel="noopener">内置插件</a>，同时也有很丰富的<a href="https://coredns.io/explugins/" target="_blank" rel="noopener">第三方插件</a>。甚至你自己<a href="https://coredns.io/2016/12/19/writing-plugins-for-coredns/" target="_blank" rel="noopener">写一个插件</a>也非常的容易。</p>
<h2 id="如何安装使用-CoreDNS"><a href="#如何安装使用-CoreDNS" class="headerlink" title="如何安装使用 CoreDNS"></a>如何安装使用 CoreDNS</h2><p>我们这里主要是为了说明如何在 K8S 环境中使用它，所以对于独立安装部署它不做说明。</p>
<p>本小册中我们使用的是 K8S 1.11 版本，在第 5 小节 《搭建 Kubernetes 集群》中，我们介绍了使用 <code>kubeadm</code> 搭建集群。</p>
<p>使用 <code>kubeadm</code> 创建集群时候 <code>kubeadm init</code> 可以传递 <code>--feature-gates</code> 参数，用于启用一些额外的特性。</p>
<p>比如在之前版本中，我们可以通过 <code>kubeadm init --feature-gates CoreDNS=true</code> 在创建集群时候启用 CoreDNS。</p>
<p>而在 1.11 版本中，使用 <code>kubeadm</code> 创建集群时 <code>CoreDNS</code> 已经被默认启用，这也从侧面证明了 CoreDNS 在 K8S 中达到了生产可用的状态。</p>
<p>我们来看一下创建集群时的日志输出：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubeadm init</span><br><span class="line">[init] using Kubernetes version: v1.11.3               </span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">...</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br></pre></td></tr></table></figure></div>

<p>可以看到创建时已经启用了 CoreDNS 的扩展，待集群创建完成后，可用过以下方式进行查看：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system get all  -l k8s-app&#x3D;kube-dns -o wide</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE</span><br><span class="line">pod&#x2F;coredns-78fcdf6894-5zbx4   1&#x2F;1       Running   0          1h        10.32.0.3   node01    &lt;none&gt;</span><br><span class="line">pod&#x2F;coredns-78fcdf6894-cxdw8   1&#x2F;1       Running   0          1h        10.32.0.2   node01    &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE       SELECTOR</span><br><span class="line">service&#x2F;kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP   1h        k8s-app&#x3D;kube-dns</span><br><span class="line"></span><br><span class="line">NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES                     SELECTOR</span><br><span class="line">deployment.apps&#x2F;coredns   2         2         2            2           1h        coredns      k8s.gcr.io&#x2F;coredns:1.1.3   k8s-app&#x3D;kube-dns</span><br><span class="line"></span><br><span class="line">NAME                                 DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES                   SELECTOR</span><br><span class="line">replicaset.apps&#x2F;coredns-78fcdf6894   2         2         2         1h        coredns      k8s.gcr.io&#x2F;coredns:1.1.3   k8s-app&#x3D;kube-dns,pod-template-hash&#x3D;3497892450</span><br></pre></td></tr></table></figure></div>

<p>这里主要是为了兼容 K8S 原有的 <code>kube-dns</code> 所以标签和 <code>Service</code> 的名字都还使用了 <code>kube-dns</code>，但实际在运行的则是 CoreDNS。</p>
<h2 id="验证-CoreDNS-功能"><a href="#验证-CoreDNS-功能" class="headerlink" title="验证 CoreDNS 功能"></a>验证 CoreDNS 功能</h2><p>从上面的输出我们看到 CoreDNS 的 <code>Pod</code> 运行正常，现在测试下它是否能正确解析。仍然以我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 为例，先 clone 项目，进入到项目的 deploy 目录中。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">master $ cd saythx&#x2F;deploy&#x2F;</span><br><span class="line">master $ ls</span><br><span class="line">backend-deployment.yaml  frontend-deployment.yaml  namespace.yaml         redis-service.yaml</span><br><span class="line">backend-service.yaml     frontend-service.yaml     redis-deployment.yaml  work-deployment.yaml</span><br><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;work created</span><br><span class="line">master $ kubectl apply -f redis-deployment.yaml</span><br><span class="line">deployment.apps&#x2F;saythx-redis created</span><br><span class="line">master $ kubectl apply -f redis-service.yaml</span><br><span class="line">service&#x2F;saythx-redis created</span><br></pre></td></tr></table></figure></div>

<ul>
<li>查看其部署情况：</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n work get all</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;saythx-redis-8558c7d7d-8v4lp   1&#x2F;1       Running   0          2m</span><br><span class="line"></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service&#x2F;saythx-redis   NodePort   10.109.215.147   &lt;none&gt;        6379:31438&#x2F;TCP   2m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;saythx-redis   1         1         1            1           2m</span><br><span class="line"></span><br><span class="line">NAME                                     DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;saythx-redis-8558c7d7d   1         1         1         2m</span><br></pre></td></tr></table></figure></div>

<ul>
<li>验证 DNS 是否正确解析:</li>
</ul>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 使用 AlpineLinux 的镜像创建一个 Pod 并进入其中</span><br><span class="line">master $ kubectl run alpine -it --rm --restart&#x3D;&#39;Never&#39; --image&#x3D;&#39;alpine&#39; sh</span><br><span class="line">If you don&#39;t see a command prompt, try pressing enter.</span><br><span class="line">&#x2F; # apk add --no-cache bind-tools</span><br><span class="line">fetch http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.8&#x2F;main&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">fetch http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.8&#x2F;community&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">(1&#x2F;5) Installing libgcc (6.4.0-r9)</span><br><span class="line">(2&#x2F;5) Installing json-c (0.13.1-r0)</span><br><span class="line">(3&#x2F;5) Installing libxml2 (2.9.8-r1)</span><br><span class="line">(4&#x2F;5) Installing bind-libs (9.12.3-r0)</span><br><span class="line">(5&#x2F;5) Installing bind-tools (9.12.3-r0)</span><br><span class="line">Executing busybox-1.28.4-r2.trigger</span><br><span class="line">OK: 9 MiB in 18 packages</span><br><span class="line"></span><br><span class="line"># 安装完 dig 命令所在包之后，使用 dig 命令进行验证</span><br><span class="line">&#x2F; # dig @10.32.0.2 saythx-redis.work.svc.cluster.local +noall +answer</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.12.3 &lt;&lt;&gt;&gt; @10.32.0.2 saythx-redis.work.svc.cluster.local +noall +answer</span><br><span class="line">; (1 server found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">saythx-redis.work.svc.cluster.local. 5 IN A     10.109.215.147</span><br></pre></td></tr></table></figure></div>

<p>通过以上操作，可以看到相应的 <code>Service</code> 记录可被正确解析。这里有几个点需要注意：</p>
<ul>
<li><p>域名解析是可跨 <code>Namespace</code> 的</p>
<p>刚才的示例中，我们没有指定 <code>Namespace</code> 所以刚才我们所在的 <code>Namespace</code> 是 <code>default</code>。而我们的解析实验成功了。说明 CoreDNS 的解析是全局的。<strong>虽然解析是全局的，但不代表网络互通</strong></p>
</li>
<li><p>域名有特定格式</p>
<p>可以看到刚才我们使用的完整域名是 <code>saythx-redis.work.svc.cluster.local</code> , 注意开头的便是 <strong>Service 名.Namespace 名</strong> 当然，我们也可以直接通过 <code>host</code> 命令查询:</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F; # host -t srv  saythx-redis.work</span><br><span class="line">saythx-redis.work.svc.cluster.local has SRV record 0 100 6379 saythx-redis.work.svc.cluster.local.</span><br></pre></td></tr></table></figure></div>

</li>
</ul>
<h2 id="配置和监控"><a href="#配置和监控" class="headerlink" title="配置和监控"></a>配置和监控</h2><p>CoreDNS 使用 <code>ConfigMap</code> 的方式进行配置，但是如果更改了配置，<code>Pod</code> 重启后才会生效。</p>
<p>我们通过以下命令可查看其配置：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n kube-system get configmap coredns -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           upstream</span><br><span class="line">           fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        proxy . &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">    &#125;</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2018-12-22T16:45:47Z</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: &quot;217&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;configmaps&#x2F;coredns</span><br><span class="line">  uid: 0882e51b-0609-11e9-b25e-0242ac110057</span><br></pre></td></tr></table></figure></div>

<p><code>Corefile</code> 便是它的配置文件，可以看到它启动了类似 <code>kubernetes</code>, <code>prometheus</code> 等插件。</p>
<p>注意 <code>kubernetes</code> 插件的配置，使用的域是 <code>cluster.local</code> ，这也是上面我们提到域名格式时候后半部分未解释的部分。</p>
<p>至于 <code>prometheus</code> 插件，则是监听在 9153 端口上提供了符合 Prometheus 标准的 metrics 接口，可用于监控等。关于监控的部分，可参考第 23 节。</p>
<h2 id="总结-19"><a href="#总结-19" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 CoreDNS 的基本情况，它是以 Go 编写的灵活可扩展的 DNS 服务器。</p>
<p>使用 CoreDNS 代替 kube-dns 主要是为了解决一些 kube-dns 时期的问题，比如说原先 kube-dns 的时候，一个 Pod 中还需要包含 <code>kube-dns</code>, <code>sidecar</code> 和 <code>dnsmasq</code> 的容器，而每当 <code>dnsmasq</code> 出现漏洞时，就不得不让 K8S 发个安全补丁才能进行更新。</p>
<p>CoreDNS 有丰富的插件，可以满足更多样的应用需求，同时 <code>kubernetes</code> 插件还包含了一些独特的功能，比如 Pod 验证之类的，可增加安全性。</p>
<p>同时 CoreDNS 在 1.13 版本中会作为默认的 DNS 服务器使用，所以应该给它更多的关注。</p>
<p>在下节，我们将介绍 <code>Ingress</code>，看看如果使用不同于之前使用的 <code>NodePort</code> 的方式将服务暴露于外部。</p>
<h1 id="服务增强：Ingress"><a href="#服务增强：Ingress" class="headerlink" title="服务增强：Ingress"></a>服务增强：Ingress</h1><h2 id="整体概览-14"><a href="#整体概览-14" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们已经知道 K8S 中有 <code>Service</code> 的概念，同时默认情况下还有 <code>CoreDNS</code> 完成集群内部的域名解析等工作，以此完成基础的服务注册发现能力。</p>
<p>在第 7 节中，我们介绍了 <code>Service</code> 的 4 种基础类型，在前面的介绍中，我们一般都在使用 <code>ClusterIP</code> 或 <code>NodePort</code> 等方式将服务暴露在集群内或者集群外。</p>
<p>本节，我们将介绍另一种处理服务访问的方式 <code>Ingress</code>。</p>
<h2 id="Ingress-是什么"><a href="#Ingress-是什么" class="headerlink" title="Ingress 是什么"></a>Ingress 是什么</h2><p>通过 <code>kubectl explain ingress</code> 命令，我们来看下对 Ingress 的描述。</p>
<blockquote>
<p>Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend. An Ingress can be configured to give services externally-reachable urls, load balance traffic, terminate SSL, offer name based virtual hosting etc.</p>
</blockquote>
<p><code>Ingress</code> 是一组允许外部请求进入集群的路由规则的集合。它可以给 <code>Service</code> 提供集群外部访问的 URL，负载均衡，SSL 终止等。</p>
<p>直白点说，<code>Ingress</code> 就类似起到了智能路由的角色，外部流量到达 <code>Ingress</code> ，再由它按已经制定好的规则分发到不同的后端服务中去。</p>
<p>看起来它很像我们使用的负载均衡器之类的。那你可能会问，<code>Ingress</code> 与 <code>LoadBalancer</code> 类型的 <code>Service</code> 的区别是什么呢？</p>
<ul>
<li><p><code>Ingress</code> 不是一种 <code>Service</code> 类型</p>
<p><code>Ingress</code> 是 K8S 中的一种资源类型，我们可以直接通过 <code>kubectl get ingress</code> 的方式获取我们已有的 <code>Ingress</code> 资源。</p>
</li>
<li><p><code>Ingress</code> 可以有多种控制器（实现）</p>
<p>通过之前的介绍，我们知道 K8S 中有很多的 <code>Controller</code> (控制器)，而这些 <code>Controller</code> 已经打包进了 <code>kube-controller-manager</code> 中，通过 <code>--controllers</code> 参数控制启用哪些。</p>
<p>但是 <code>Ingress</code> 的 <code>Controller</code> 并没有包含在其中，而且有多种选择。</p>
<p>由社区维护（或是说官方支持的）有两个：适用于 Google Cloud 的 <a href="https://github.com/kubernetes/ingress-gce">GLBC</a>，当你使用 GKE 的时候，便会看到它；和 <a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a> 它是使用 <code>ConfigMap</code> 存储 NGINX 配置实现的。</p>
<p>第三方的实现还有：基于 Envoy 的 <a href="https://github.com/heptio/contour">Contour</a>; F5 的 <a href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/v1.7/" target="_blank" rel="noopener">F5 BIG-IP Controller</a>; 基于 HAProxy 的 <a href="https://github.com/jcmoraisjr/haproxy-ingress">haproxy-ingress</a>; 基于 Istio 的 <a href="https://istio.io/docs/tasks/traffic-management/ingress/" target="_blank" rel="noopener">Control Ingress Traffic</a>; 现代化的反向代理服务器 <a href="https://github.com/containous/traefik">Traefik</a>; 以及 Kong 支持的 <a href="https://konghq.com/blog/kubernetes-ingress-controller-for-kong/" target="_blank" rel="noopener">Kong Ingress Controller for Kubernetes</a> 和 NGINX 官方支持的 <a href="https://github.com/nginxinc/kubernetes-ingress">NGINX Ingress Controller</a>。</p>
<p>这里可以看到 K8S 社区和 NGINX 都有 NGINX Ingress Controller，很多人在一开始接触 Ingress 的时候便陷入了选择的苦恼中，除去前面的那些选择外，单 NGINX 的控制器就有两个，到底应该怎么选。</p>
<p>这里提供两点建议：</p>
<ul>
<li>可能多数人使用的都是 NGINX 而非 NGINX Plus，如果你需要会话保持（Session persistence）的话，那你应该选择 K8S 社区维护的版本</li>
<li>即使我们平时使用 NGINX 的时候，也常常会有动态配置的需求，如果你仍然有这样的需求，那你还是继续使用 K8S 社区维护的版本（其中内置了 Lua 支持）。</li>
</ul>
</li>
</ul>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>前面也已经说到了，单纯的创建一个 <code>Ingress</code> 资源没什么意义，我们需要先配置一个 <code>Controller</code> ，才能让它正常工作。国内使用 GKE 的可能不是很多，为了更加通用，这里我们选择 K8S 社区维护的 NGINX Ingress Controller。</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>整个安装过程其实也比较简单，具体步骤如下（以下步骤中都将直接展示该步骤所需的 YAML 配置文件）：</p>
<ul>
<li><p>创建 <code>Namespace</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br></pre></td></tr></table></figure></div>

<p>将以上内容保存为 namespace.yaml 文件，然后执行 <code>kubectl apply -f namespace.yaml</code> 即可。以下步骤均类似，不再赘述。 注意：这里创建 <code>Namespace</code> 只是为了保持集群相对规范，非强制，但推荐此做法。</p>
</li>
<li><p>创建 <code>ConfigMap</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></div>

<p>这里创建了几个 <code>ConfigMap</code>，主要是给 <code>Controller</code> 使用。</p>
</li>
<li><p>由于我们的集群使用 <code>kubeadm</code> 创建时，默认开启了 <code>RBAC</code> ，所以这里需要相应的创建对应的 <code>Role</code> 和 <code>RoleBinding</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-serviceaccount</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - endpoints</span><br><span class="line">      - nodes</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses&#x2F;status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    resourceNames:</span><br><span class="line">      - &quot;ingress-controller-leader-nginx&quot;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role-nisa-binding</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole-nisa-binding</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></div>

<p>关于 <code>RBAC</code> 相关的内容，可查看第 8 节 《安全重点: 认证和授权》，了解此处的配置及其含义。</p>
</li>
<li><p>部署 NGINX Ingress Controller</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">      app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">        app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;port: &quot;10254&quot;</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: taobeier&#x2F;nginx-ingress-controller:0.21.0</span><br><span class="line">          args:</span><br><span class="line">            - &#x2F;nginx-ingress-controller</span><br><span class="line">            - --configmap&#x3D;$(POD_NAMESPACE)&#x2F;nginx-configuration</span><br><span class="line">            - --tcp-services-configmap&#x3D;$(POD_NAMESPACE)&#x2F;tcp-services</span><br><span class="line">            - --udp-services-configmap&#x3D;$(POD_NAMESPACE)&#x2F;udp-services</span><br><span class="line">            - --publish-service&#x3D;$(POD_NAMESPACE)&#x2F;ingress-nginx</span><br><span class="line">            - --annotations-prefix&#x3D;nginx.ingress.kubernetes.io</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: &#x2F;healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: &#x2F;healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br></pre></td></tr></table></figure></div>

<p>注意，这里的镜像是我从官方镜像直接同步的，为了解决国内无法下载镜像的情况。</p>
<p>另外，在启动参数中，指定了我们第二步中创建的 <code>ConfigMap</code> 。以及，在此部署中，用到了之前尚未详细说明的 <code>readinessProbe</code> 和 <code>livenessProbe</code>：我们之前在详解 <code>kubelet</code> 时，大致提到过关于它所具备的职责，这两个配置主要是用于做探针，用户检查 Pod 是否已经准备好接受请求流量和是否存活。</p>
<p>这里还进行了 <code>annotations</code> 里面标注了关于 <code>Prometheus</code> 的相关内容，我们会在下节中描述。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n ingress-nginx get all</span><br><span class="line">NAME                                            READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;nginx-ingress-controller-6f647f7866-659ph   1&#x2F;1       Running   0          75s</span><br><span class="line"></span><br><span class="line">NAME                                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;nginx-ingress-controller   1         1         1            1           75s</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;nginx-ingress-controller-6f647f7866   1         1         1         75s</span><br></pre></td></tr></table></figure></div>

<p>可以看到 NGINX Ingress Controller 已经部署成功。</p>
</li>
<li><p><strong>将 NGINX Ingress Controller 暴露至集群外</strong></p>
<p>经过前面的介绍，我们已经知道 Ingress 的作用在于将集群外的请求流量转向集群内的服务，而我们知道，默认情况下集群外和集群内是不互通的，所以必须将 NGINX Ingress Controller 暴露至集群外，以便让其能接受来自集群外的请求。</p>
<p>将其暴露的方式有很多种，这里我们选择我们之前已经介绍过的 <code>NodePort</code> 的方式。选择它主要有以下原因：</p>
<ul>
<li>我们可以使用纯的 LB 实现完成服务暴露，比如 <a href="https://metallb.universe.tf/" target="_blank" rel="noopener">MetalLB</a>，但它还处于 Beta 阶段，尚未有大规模生产环境使用的验证。</li>
<li>我们可以直接使用宿主机的网络，只需设置 <code>hostNetwork: true</code> 即可，但这个方式可能会带来安全问题。</li>
<li>我们可以选择 External IPs 的方式，但这种方式无法保留请求的源 IP，所以并不是很好。</li>
<li>其实我们一般会选择自己提供边缘节点的方式，不过这种方式是建立在 <code>NodePort</code> 的方式之上，并且需要提供额外的组件，此处就暂不做展开了。</li>
</ul>
<p>我们使用以下的配置，将 NGINX Ingress Controller 暴露至集群外</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - name: http</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">    - name: https</span><br><span class="line">      port: 443</span><br><span class="line">      targetPort: 443</span><br><span class="line">      protocol: TCP</span><br><span class="line">  selector:</span><br><span class="line">    app.kubernetes.io&#x2F;name: ingress-nginx</span><br><span class="line">    app.kubernetes.io&#x2F;part-of: ingress-nginx</span><br></pre></td></tr></table></figure></div>

<p>创建该 <code>Service</code>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl -n ingress-nginx get svc                                                  </span><br><span class="line">NAME            TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">ingress-nginx   NodePort   10.0.38.53   &lt;none&gt;        80:30871&#x2F;TCP,443:30356&#x2F;TCP   11s</span><br></pre></td></tr></table></figure></div>

<p>现在，我们直接访问 <code>Node:Port</code> 便可访问到该 Controller。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">master $ curl 172.17.0.3:30871                    </span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;404 Not Found&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;404 Not Found&lt;&#x2F;h1&gt;&lt;&#x2F;center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx&#x2F;1.15.6&lt;&#x2F;center&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure></div>

<p>由于我们并没有设置任何的默认响应后端，所以当直接请求时，便返回 404 。</p>
</li>
</ul>
<h3 id="实践-1"><a href="#实践-1" class="headerlink" title="实践"></a>实践</h3><p>将我们的示例项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 通过 <code>Ingress</code> 的方式进行访问。</p>
<p>该示例项目的部署，不再进行赘述。可在 <a href="https://github.com/tao12345666333/saythx/blob/ingress/deploy/ingress.yaml">ingress 分支</a> 查看此处所需配置。</p>
<p>在我们将 NGINX Ingress Controller 及 SayThx 项目部署好之后，我们使用以下的配置创建 <code>Ingress</code> 资源。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: saythx-ing</span><br><span class="line">  namespace: work</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io&#x2F;ssl-redirect: &quot;false&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: saythx.moelove.info</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: saythx-frontend</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>创建</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f ingress.yaml         </span><br><span class="line">ingress.extensions&#x2F;saythx-ing created</span><br><span class="line">master $ kubectl -n work get ing</span><br><span class="line">NAME         HOSTS                 ADDRESS   PORTS     AGE</span><br><span class="line">saythx-ing   saythx.moelove.info             80        23s</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>验证</p>
<p>这里来解释下刚才的配置文件。首先，指定了 <code>host: saythx.moelove.info</code> 表示我们想要以 <code>saythx.moelove.info</code> 这个域名来访问它。<code>path</code> 直接写 <code>/</code> 表示所有的请求都转发至名为 <code>saythx-frontend</code> 的服务。</p>
<p>与我们平时使用 NGINX 基本一致。 现在编辑本地的 HOSTS 文件绑定 Node 的IP 与 <code>saythx.moelove.info</code> 这个域名。使用浏览器进行访问 <code>saythx.moelove.info:刚才 Controller 使用 NodePort 暴露服务时的端口</code>：</p>
</li>
</ul>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="586"></svg>)</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以看到已经成功访问。</span><br></pre></td></tr></table></figure></div>

<h2 id="总结-20"><a href="#总结-20" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>Ingress</code> 的基本情况，了解了它是 K8S 中的一种资源对象，主要负责将集群外部流量与集群内服务的通信。但它的正常工作离不开 <code>Ingress Controller</code> ，当前官方团队维护的主要有两个 GLBC 和 NGINX Ingress Controller。</p>
<p>我们大致介绍了现有的 Controller 实现，也实践了如何部署 NGINX Ingress Controller 以及如何使用 Ingress 将我们的示例项目暴露至集群外。</p>
<p>NGINX Ingress Controller 的使用，比较符合我们平时使用 NGINX 的习惯，相对来说也比较灵活，后续可看实际情况再进行更多的实践。</p>
<p>至此，K8S 集群的管理，相关原理以及服务的部署等内容就基本介绍完毕。下节，我们将介绍生产实践中至关重要的一环 <strong>监控</strong> 相关的实践。</p>
<h1 id="监控实践：对-K8S-集群进行监控"><a href="#监控实践：对-K8S-集群进行监控" class="headerlink" title="监控实践：对 K8S 集群进行监控"></a>监控实践：对 K8S 集群进行监控</h1><h2 id="整体概览-15"><a href="#整体概览-15" class="headerlink" title="整体概览"></a>整体概览</h2><p>通过前面的学习，我们对 K8S 有了一定的了解，也具备了一定的集群管理和排错能力。但如果要应用于生产环境中，不可能随时随地的都盯着集群，我们需要扩展我们对集群的感知能力。</p>
<p>本节，我们将介绍下 K8S 集群监控相关的内容。</p>
<h2 id="监控什么"><a href="#监控什么" class="headerlink" title="监控什么"></a>监控什么</h2><p>除去 K8S 外，我们平时自己开发的系统或者负责的项目，一般都是有监控的。监控可以提升我们的感知能力，便于我们及时了解集群的变化，以及知道哪里出现了问题。</p>
<p>K8S 是一个典型的分布式系统，组件很多，那么监控的目标，就变的很重要了。</p>
<p>总体来讲，对 K8S 集群的监控的话，主要有以下方面：</p>
<ul>
<li>节点情况</li>
<li>K8S 集群自身状态</li>
<li>部署在 K8S 内的应用的状态</li>
</ul>
<h2 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h2><p>对于 K8S 的监控，我们选择 CNCF 旗下次于 K8S 毕业的项目<a href="https://prometheus.io/" target="_blank" rel="noopener"> Prometheus </a>。</p>
<p>Prometheus 是一个非常灵活易于扩展的监控系统，它通过各种 <code>exporter</code> 暴露数据，并由 <code>prometheus server</code> 定时去拉数据，然后存储。</p>
<p>它自己提供了一个简单的前端界面，可在其中使用 <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank" rel="noopener">PromQL </a>的语法进行查询，并进行图形化展示。</p>
<h2 id="安装-Prometheus"><a href="#安装-Prometheus" class="headerlink" title="安装 Prometheus"></a>安装 Prometheus</h2><blockquote>
<p>这里推荐一个项目 <a href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a>, 尽管该项目还处于 Beta 阶段，但是它给在 K8S 中搭建基于 Prometheus 的监控提供了很大的便利。</p>
</blockquote>
<p>我们此处选择以一般的方式进行部署，带你了解其整体的过程。</p>
<ul>
<li><p>创建一个独立的 <code>Namespace</code>：</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: monitoring</span><br><span class="line"></span><br><span class="line"># 将文件保存为 namespace.yaml 的文件，并执行 kubectl apply -f namespace.yaml 即可，后面不再赘述。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl apply -f namespace.yaml</span><br><span class="line">namespace&#x2F;monitoring created</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>RBAC</p>
<p>我们的集群使用 <code>kubeadm</code> 创建，默认开启了 <code>RBAC</code>，所以现在需要创建相关的 Role 和 binding。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: prometheus</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: prometheus-k8s</span><br><span class="line">  namespace: monitoring</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources:</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes&#x2F;proxy</span><br><span class="line">  - services</span><br><span class="line">  - endpoints</span><br><span class="line">  - pods</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br><span class="line">- nonResourceURLs: [&quot;&#x2F;metrics&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-k8s</span><br><span class="line">  namespace: monitoring</span><br></pre></td></tr></table></figure></div>

<p>执行创建</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl  apply -f rbac.yaml</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;prometheus created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;prometheus created</span><br><span class="line">serviceaccount&#x2F;prometheus-k8s created</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>创建 Promethes 的配置文件</p>
<p>其中的内容主要参考 <a href="https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml">Prometheus 官方提供的示例</a> 和 <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config" target="_blank" rel="noopener">Prometheus 官方文档</a>。</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-core</span><br><span class="line">  namespace: monitoring</span><br><span class="line">data:</span><br><span class="line">  prometheus.yaml: |</span><br><span class="line">    global:</span><br><span class="line">      scrape_interval: 30s</span><br><span class="line">      scrape_timeout: 30s</span><br><span class="line">    scrape_configs:</span><br><span class="line">    - job_name: &#39;kubernetes-apiservers&#39;</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: endpoints</span><br><span class="line">      scheme: https</span><br><span class="line">    </span><br><span class="line">      tls_config:</span><br><span class="line">        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]</span><br><span class="line">        action: keep</span><br><span class="line">        regex: default;kubernetes;https</span><br><span class="line">    </span><br><span class="line">    # Scrape config for nodes (kubelet).</span><br><span class="line">    - job_name: &#39;kubernetes-nodes&#39;</span><br><span class="line">      scheme: https</span><br><span class="line">    </span><br><span class="line">      tls_config:</span><br><span class="line">        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: node</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_node_label_(.+)</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: kubernetes.default.svc:443</span><br><span class="line">      - source_labels: [__meta_kubernetes_node_name]</span><br><span class="line">        regex: (.+)</span><br><span class="line">        target_label: __metrics_path__</span><br><span class="line">        replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics</span><br><span class="line">    </span><br><span class="line">    # Scrape config for Kubelet cAdvisor.</span><br><span class="line">    - job_name: &#39;kubernetes-cadvisor&#39;</span><br><span class="line">    </span><br><span class="line">      scheme: https</span><br><span class="line">    </span><br><span class="line">      tls_config:</span><br><span class="line">        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: node</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_node_label_(.+)</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: kubernetes.default.svc:443</span><br><span class="line">      - source_labels: [__meta_kubernetes_node_name]</span><br><span class="line">        regex: (.+)</span><br><span class="line">        target_label: __metrics_path__</span><br><span class="line">        replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics&#x2F;cadvisor</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-service-endpoints&#39;</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: endpoints</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_service_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_service_name]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_name</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-services&#39;</span><br><span class="line">    </span><br><span class="line">      metrics_path: &#x2F;probe</span><br><span class="line">      params:</span><br><span class="line">        module: [http_2xx]</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: service</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - source_labels: [__address__]</span><br><span class="line">        target_label: __param_target</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: blackbox-exporter.example.com:9115</span><br><span class="line">      - source_labels: [__param_target]</span><br><span class="line">        target_label: instance</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_service_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_service_name]</span><br><span class="line">        target_label: kubernetes_name</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-ingresses&#39;</span><br><span class="line">    </span><br><span class="line">      metrics_path: &#x2F;probe</span><br><span class="line">      params:</span><br><span class="line">        module: [http_2xx]</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: ingress</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]</span><br><span class="line">        regex: (.+);(.+);(.+)</span><br><span class="line">        replacement: $&#123;1&#125;:&#x2F;&#x2F;$&#123;2&#125;$&#123;3&#125;</span><br><span class="line">        target_label: __param_target</span><br><span class="line">      - target_label: __address__</span><br><span class="line">        replacement: blackbox-exporter.example.com:9115</span><br><span class="line">      - source_labels: [__param_target]</span><br><span class="line">        target_label: instance</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_ingress_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_ingress_name]</span><br><span class="line">        target_label: kubernetes_name</span><br><span class="line">    </span><br><span class="line">    - job_name: &#39;kubernetes-pods&#39;</span><br><span class="line">    </span><br><span class="line">      kubernetes_sd_configs:</span><br><span class="line">      - role: pod</span><br><span class="line">    </span><br><span class="line">      relabel_configs:</span><br><span class="line">      - action: labelmap</span><br><span class="line">        regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">      - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_namespace</span><br><span class="line">      - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">        action: replace</span><br><span class="line">        target_label: kubernetes_pod_name</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>部署 Prometheus</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-core</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: core</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: prometheus-main</span><br><span class="line">      labels:</span><br><span class="line">        app: prometheus</span><br><span class="line">        component: core</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: prometheus-k8s</span><br><span class="line">      containers:</span><br><span class="line">      - name: prometheus</span><br><span class="line">        image: taobeier&#x2F;prometheus:v2.6.0</span><br><span class="line">        args:</span><br><span class="line">          - &#39;--storage.tsdb.retention&#x3D;24h&#39;</span><br><span class="line">          - &#39;--storage.tsdb.path&#x3D;&#x2F;prometheus&#39;</span><br><span class="line">          - &#39;--config.file&#x3D;&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yaml&#39;</span><br><span class="line">        ports:</span><br><span class="line">        - name: webui</span><br><span class="line">          containerPort: 9090</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 500M</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 500M</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &#x2F;prometheus</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;prometheus</span><br><span class="line">      volumes:</span><br><span class="line">      - name: data</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      - name: config-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: prometheus-core</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看部署情况</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">master $ kubectl  -n monitoring get all</span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;prometheus-core-86b8455f76-mvrn4   1&#x2F;1       Running   0          12s</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;prometheus-core   1         1         1            1           12s</span><br><span class="line"></span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     AGE</span><br><span class="line">replicaset.apps&#x2F;prometheus-core-86b8455f76   1         1         1         12s</span><br></pre></td></tr></table></figure></div>

<p>Prometheus 的主体就已经部署完成。</p>
</li>
<li><p>使用 <code>Service</code> 将 <code>Promethes</code> 的服务暴露出来</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: core</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 9090</span><br><span class="line">    targetPort: 9090</span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: core</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></div>

<p>这里为了方便演示，直接使用了 <code>NodePort</code> 的方式暴露服务。当然你也可以参考上一节，使用 <code>Ingress</code> 的方式将服务暴露出来。</p>
</li>
<li><p>查询当前状态</p>
<p>我们使用 Promethes 自带的 PromQL 语法，查询在当前 <code>monitoring</code> Namespace 中 up 的任务。这里对查询的结果暂不进行展开。</p>
</li>
</ul>
<p>![img](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1275" height="766"></svg>)</p>
<h2 id="安装-Node-exporter"><a href="#安装-Node-exporter" class="headerlink" title="安装 Node exporter"></a>安装 Node exporter</h2><p>我们刚才在介绍时，提到过 <code>Promethes</code> 支持多种 <code>exporter</code> 暴露指标。我们现在使用 <a href="https://github.com/prometheus/node_exporter">Node exporter</a> 完成对集群中机器的基础监控。</p>
<p>这里有一个需要考虑的点：</p>
<ul>
<li><p>使用什么方式部署 Node exporter ？</p>
<p>Node exporter 有已经编译好的二进制文件，可以很方便的进行部署。当我们要监控集群中所有的机器时，我们是该将它直接部署在机器上，还是部署在集群内？</p>
<p>我建议是直接部署在集群内，使用 <code>DaemonSet</code> 的方式进行部署。这里的考虑是当我们直接部署在宿主机上时，我们最起码需要保证两点：1. Promethes 服务可与它正常通信（Promethes 采用 Pull 的方式采集数据） ；2. 需要服务保活，如果 exporter 挂掉了，那自然就取不到数据。</p>
<p><code>DaemonSet</code> 是一种很合适的的部署方式，可直接将 Node exporter 部署至集群的每个节点上。</p>
</li>
<li><p>创建 <code>DaemonSet</code></p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-node-exporter</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: node-exporter</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: prometheus-node-exporter</span><br><span class="line">      labels:</span><br><span class="line">        app: prometheus</span><br><span class="line">        component: node-exporter</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">      - image: taobeier&#x2F;node-exporter:v0.17.0</span><br><span class="line">        name: prometheus-node-exporter</span><br><span class="line">        ports:</span><br><span class="line">        - name: prom-node-exp</span><br><span class="line">          containerPort: 9100</span><br><span class="line">          hostPort: 9100</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      hostPID: true</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>让 Promethes 抓取数据</p>
<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    prometheus.io&#x2F;scrape: &#39;true&#39;</span><br><span class="line">  name: prometheus-node-exporter</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: node-exporter</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">    - name: prometheus-node-exporter</span><br><span class="line">      port: 9100</span><br><span class="line">      protocol: TCP</span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br><span class="line">    component: node-exporter</span><br><span class="line">  type: ClusterIP</span><br></pre></td></tr></table></figure></div>

<p>这里我们直接使用了添加 <code>annotations</code> 的方式，让 Promethes 自动的通过 Kubernetes SD 发现我们新添加的 exporter （或者说资源）</p>
<p>我们访问 Promethes 的 web 端，进行验证。</p>
</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/24/167df6106b58b2fb?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p>
<h2 id="总结-21"><a href="#总结-21" class="headerlink" title="总结"></a>总结</h2><p>在本节中，我们介绍了 <code>Prometheus</code> 的基本情况，也部署了 <code>Prometheus</code> 的主体服务。</p>
<p>但这是结束么？这并不是，这才刚刚开始。</p>
<p>我们提到 <code>Prometheus</code> 支持多种 <code>exporter</code> 暴露各种指标，而且我们还可以使用 <a href="https://grafana.com/" target="_blank" rel="noopener">Grafana</a> 作为我们监控的展示手段。</p>
<p>如果要做 Dashboard 推荐使用 <a href="https://grafana.com/dashboards/162" target="_blank" rel="noopener">Kubernetes cluster monitoring (via Prometheus)</a> 。</p>
<p>另外，监控其实涉及的内容很多，包括数据持久化方式。以及是否考虑与集群外的 Prometheus 集群做邦联模式等。这里需要考虑的实际情况较多，暂不一一展开了。</p>
<p>Prometheus 已经从 CNCF 毕业，其在云原生时代下作为标准的监控技术栈也基本确立。至于应用监控，也可使用它的 SDK 来完成。</p>
<p>下节，我们将对本小册进行一次总结。</p>
<h1 id="总结-22"><a href="#总结-22" class="headerlink" title="总结"></a>总结</h1><h2 id="快速回顾"><a href="#快速回顾" class="headerlink" title="快速回顾"></a>快速回顾</h2><p>经过了前面 23 节的内容，我们从 K8S 的基础概念入手，通过其基础架构了解到了 K8S 中所涉及到的各类组件。</p>
<p>通过动手实践，使用 <code>minikube</code> 搭建了本地的集群，使用 <code>kubeadm</code> 完成了服务器上的集群搭建，对 K8S 的部署有了更加清晰的认识。</p>
<p>这里再推荐另一种正在快速迭代的方式 <a href="https://github.com/kubernetes-sigs/kind">Kubernetes In Docker</a> 可以很方便的创建廉价的 K8S 集群，目前至支持单节点集群，多节点支持正在开发中。</p>
<p>后面，我们通过学习 <code>kubectl</code> 的使用，部署了 Redis 服务，了解到了一个服务在 K8S 中部署的操作，以及如何将服务暴露至集群外，以便访问。</p>
<p>当集群真正要被使用之前，权限管控也愈发重要，我们通过学习 <code>RBAC</code> 的相关知识，学习到了如何在 K8S 集群中创建权限可控的用户，而这部分的内容在后续小节中也被频繁用到。</p>
<p>接下来，我们以我们实际的一个项目 <a href="https://github.com/tao12345666333/saythx">SayThx</a> 为例，一步步的完成了项目的部署，在此过程中也学习到了配置文件的编写规范和要求。</p>
<p>当项目变大时，维护项目的更新也变成了一件很麻烦的事情。由此，我们引入了 <code>Helm</code> 作为我们的包管理软件，并使用它进行了项目的部署。</p>
<p>在此过程中也学习到了 Helm 的架构，以及如何编写一个 <code>Chart</code> 等知识。</p>
<p>前面我们主要集中于如何使用 K8S 上，接下了庖丁解牛系列便带我们一同深入至 K8S 内部，了解到了各基础组件的实际工作原理，也深入到了源码内部，了解其实现逻辑。</p>
<p>有这些理论知识作为基础，我们便可以大胆的将应用部署至 K8S 之上了。但实际环境可能多种多样，你可以会遇到各种各样的问题。</p>
<p>这里我们介绍了一些常见的 Troubleshoot 的方法，以便你在后续使用 K8S 的过程中遇到问题也可以快速的定位并解决问题。</p>
<p>此外，我们学习了 K8S 的一些扩展，比如 Dashboard 和 CoreDNS ， Dashboard 是一个比较直观的管理资源的方式，它也还在快速的发展和迭代中。</p>
<p>CoreDNS 在 K8S 1.13 中已经成为默认的 DNS 服务器，相信在不久之后， CoreDNS 也将会从 CNCF 毕业。</p>
<p>我们介绍了 <code>Ingress</code> 和在 K8S 中使用 <code>Promethes</code> 进行监控，不过监控涉及的方面很多，除了集群自身的监控外，应用层的监控也同样很重要。另外，监控和告警也是相辅相成的，在已有监控数据的前提下，如何更智能更优雅的告警也是我们需要考虑的点。否则，很容易造成告警风暴，有用的告警被忽略之类的。</p>
<h2 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h2><p>基于 K8S 的生态已经在逐步形成，只靠一本小册还远远不够，我们需要更多的对操作系统的了解，对 K8S 及其生态的了解。</p>
<p>以下推荐一些扩展阅读：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/container" target="_blank" rel="noopener">K8S 生态</a></li>
<li><a href="https://kubernetes.io/" target="_blank" rel="noopener">K8S 网站</a></li>
<li><a href="https://www.cncf.io/newsroom/blog/" target="_blank" rel="noopener">CNCF 博客</a></li>
<li><a href="https://github.com/kubernetes/">K8S 组织</a></li>
<li><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker 文档</a></li>
<li><a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="noopener">Promethes 文档</a></li>
<li><a href="https://grafana.com/" target="_blank" rel="noopener">Grafana 主页</a></li>
<li><a href="https://www.fluentd.org/" target="_blank" rel="noopener">Fluentd 主页</a></li>
</ul>
<h2 id="总结-23"><a href="#总结-23" class="headerlink" title="总结"></a>总结</h2><p>围绕 K8S 的云原生生态已经逐步形成，希望本小册能在你未来发展道路上起到一定的帮助。</p>
<p>K8S 涉及的知识面很广，小册中篇幅有限未能一一详解，欢迎大家共同讨探。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/frank-wong.github.io/tags/K8S/" rel="tag"><i class="fa fa-tag"></i> K8S</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/frank-wong.github.io/posts/f1efb8a3/" rel="next" title="Docker 常用命令">
                <i class="fa fa-chevron-left"></i> Docker 常用命令
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/frank-wong.github.io/images/jd.png"
                alt="Frank.Wong" />
            
              <p class="site-author-name" itemprop="name">Frank.Wong</p>
              <p class="site-description motion-element" itemprop="description">雄关漫道真如铁，而今迈步从头越</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
                <a href="/frank-wong.github.io/archives">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/frank-wong.github.io/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/frank-wong.github.io/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <!-- modify icon to fire by szw -->
                <i class="fa fa-history fa-" aria-hidden="true"></i>
                近期文章
              </div>
              <ul class="links-of-blogroll-list">
                
                
                  <li class="recent_posts_li">
                    <a href="/frank-wong.github.io/posts/f32b05cc/" title="青云 Kubernetes" target="_blank">青云 Kubernetes</a>
                  </li>
                
                  <li class="recent_posts_li">
                    <a href="/frank-wong.github.io/posts/f1efb8a3/" title="Docker 常用命令" target="_blank">Docker 常用命令</a>
                  </li>
                
              </ul>
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#开篇：-Kubernetes-是什么以及为什么需要它"><span class="nav-number">1.</span> <span class="nav-text">开篇： Kubernetes 是什么以及为什么需要它</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kubernetes-是什么"><span class="nav-number">1.1.</span> <span class="nav-text">Kubernetes 是什么?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么需要-Kubernetes"><span class="nav-number">1.2.</span> <span class="nav-text">为什么需要 Kubernetes</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#初步认识：Kubernetes-基础概念"><span class="nav-number">2.</span> <span class="nav-text">初步认识：Kubernetes 基础概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Node"><span class="nav-number">2.1.</span> <span class="nav-text">Node</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Node-状态"><span class="nav-number">2.1.1.</span> <span class="nav-text">Node 状态</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#地址"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">地址</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#容量"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">容量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#条件"><span class="nav-number">2.1.1.4.</span> <span class="nav-text">条件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deployment-和-Pod"><span class="nav-number">2.2.</span> <span class="nav-text">Deployment 和 Pod</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Container-Runtime"><span class="nav-number">2.3.</span> <span class="nav-text">Container Runtime</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">2.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#宏观认识：整体架构"><span class="nav-number">3.</span> <span class="nav-text">宏观认识：整体架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#C-S-架构"><span class="nav-number">3.1.</span> <span class="nav-text">C&#x2F;S 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Master"><span class="nav-number">3.2.</span> <span class="nav-text">Master</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster-state-store"><span class="nav-number">3.2.1.</span> <span class="nav-text">Cluster state store</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#API-Server"><span class="nav-number">3.2.2.</span> <span class="nav-text">API Server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Controller-Manager"><span class="nav-number">3.2.3.</span> <span class="nav-text">Controller Manager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scheduler"><span class="nav-number">3.2.4.</span> <span class="nav-text">Scheduler</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Node-1"><span class="nav-number">3.3.</span> <span class="nav-text">Node</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kubelet"><span class="nav-number">3.3.1.</span> <span class="nav-text">Kubelet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Container-runtime"><span class="nav-number">3.3.2.</span> <span class="nav-text">Container runtime</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kube-Proxy"><span class="nav-number">3.3.3.</span> <span class="nav-text">Kube Proxy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-1"><span class="nav-number">3.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#搭建-Kubernetes-集群-本地快速搭建"><span class="nav-number">4.</span> <span class="nav-text">搭建 Kubernetes 集群 - 本地快速搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#方案选择"><span class="nav-number">4.1.</span> <span class="nav-text">方案选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KIND"><span class="nav-number">4.2.</span> <span class="nav-text">KIND</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">4.2.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装"><span class="nav-number">4.2.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建集群"><span class="nav-number">4.2.3.</span> <span class="nav-text">创建集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Minikube"><span class="nav-number">4.3.</span> <span class="nav-text">Minikube</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前期准备"><span class="nav-number">4.3.2.</span> <span class="nav-text">前期准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-kubectl"><span class="nav-number">4.3.3.</span> <span class="nav-text">安装 kubectl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-Minikube"><span class="nav-number">4.3.4.</span> <span class="nav-text">安装 Minikube</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建第一个-K8S-集群"><span class="nav-number">4.3.5.</span> <span class="nav-text">创建第一个 K8S 集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过-Dashboard-查看集群当前状态"><span class="nav-number">4.3.6.</span> <span class="nav-text">通过 Dashboard 查看集群当前状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#相关链接"><span class="nav-number">4.3.7.</span> <span class="nav-text">相关链接:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-2"><span class="nav-number">4.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#动手实践：搭建一个-Kubernetes-集群-生产可用"><span class="nav-number">5.</span> <span class="nav-text">动手实践：搭建一个 Kubernetes 集群 - 生产可用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#方案选择-1"><span class="nav-number">5.1.</span> <span class="nav-text">方案选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装基础组件"><span class="nav-number">5.2.</span> <span class="nav-text">安装基础组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前期准备-1"><span class="nav-number">5.2.1.</span> <span class="nav-text">前期准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-kubectl-1"><span class="nav-number">5.2.2.</span> <span class="nav-text">安装 kubectl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-kubeadm-和-kubelet"><span class="nav-number">5.2.3.</span> <span class="nav-text">安装 kubeadm 和 kubelet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#配置"><span class="nav-number">5.3.</span> <span class="nav-text">配置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-kubelet"><span class="nav-number">5.3.1.</span> <span class="nav-text">配置 kubelet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动"><span class="nav-number">5.4.</span> <span class="nav-text">启动</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装前置依赖-crictl"><span class="nav-number">5.4.1.</span> <span class="nav-text">安装前置依赖 crictl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装前置依赖-socat"><span class="nav-number">5.4.2.</span> <span class="nav-text">安装前置依赖 socat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化集群"><span class="nav-number">5.4.3.</span> <span class="nav-text">初始化集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#验证"><span class="nav-number">5.5.</span> <span class="nav-text">验证</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-kubectl"><span class="nav-number">5.5.1.</span> <span class="nav-text">配置 kubectl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置集群网络"><span class="nav-number">5.5.2.</span> <span class="nav-text">配置集群网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#新增-Node"><span class="nav-number">5.5.3.</span> <span class="nav-text">新增 Node</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-3"><span class="nav-number">5.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#集群管理：初识-kubectl"><span class="nav-number">6.</span> <span class="nav-text">集群管理：初识 kubectl</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览"><span class="nav-number">6.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基础配置"><span class="nav-number">6.2.</span> <span class="nav-text">基础配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从-get-说起"><span class="nav-number">6.3.</span> <span class="nav-text">从 get 说起</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#答疑解惑-explain"><span class="nav-number">6.4.</span> <span class="nav-text">答疑解惑 explain</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-4"><span class="nav-number">6.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#集群管理：以-Redis-为例-部署及访问"><span class="nav-number">7.</span> <span class="nav-text">集群管理：以 Redis 为例-部署及访问</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#从-kubectl-run-开始"><span class="nav-number">7.1.</span> <span class="nav-text">从 kubectl run 开始</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用最小的-Redis-镜像"><span class="nav-number">7.2.</span> <span class="nav-text">使用最小的 Redis 镜像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署"><span class="nav-number">7.3.</span> <span class="nav-text">部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deployment"><span class="nav-number">7.3.1.</span> <span class="nav-text">Deployment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReplicaSet"><span class="nav-number">7.3.2.</span> <span class="nav-text">ReplicaSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Service"><span class="nav-number">7.3.3.</span> <span class="nav-text">Service</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#类型"><span class="nav-number">7.3.3.1.</span> <span class="nav-text">类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实践"><span class="nav-number">7.3.3.2.</span> <span class="nav-text">实践</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod"><span class="nav-number">7.3.4.</span> <span class="nav-text">Pod</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-5"><span class="nav-number">7.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#安全重点-认证和授权"><span class="nav-number">8.</span> <span class="nav-text">安全重点: 认证和授权</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-1"><span class="nav-number">8.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#认证（Authentication）"><span class="nav-number">8.2.</span> <span class="nav-text">认证（Authentication）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#授权（Authorization）"><span class="nav-number">8.3.</span> <span class="nav-text">授权（Authorization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#角色（Role）"><span class="nav-number">8.4.</span> <span class="nav-text">角色（Role）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#查看用户权限"><span class="nav-number">8.5.</span> <span class="nav-text">查看用户权限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实践：创建权限可控的用户"><span class="nav-number">8.6.</span> <span class="nav-text">实践：创建权限可控的用户</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-NameSpace"><span class="nav-number">8.6.1.</span> <span class="nav-text">创建 NameSpace</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建用户"><span class="nav-number">8.6.2.</span> <span class="nav-text">创建用户</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建私钥"><span class="nav-number">8.6.2.1.</span> <span class="nav-text">创建私钥</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定-subject-信息，传递用户名和组名"><span class="nav-number">8.6.2.2.</span> <span class="nav-text">使用私钥生成证书请求。前面已经讲过关于认证的部分，在这里需要指定 subject 信息，传递用户名和组名</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-CA-进行签名。K8S-默认的证书目录为-etc-kubernetes-pki。"><span class="nav-number">8.6.2.3.</span> <span class="nav-text">使用 CA 进行签名。K8S 默认的证书目录为 &#x2F;etc&#x2F;kubernetes&#x2F;pki。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#添加-context"><span class="nav-number">8.6.2.4.</span> <span class="nav-text">添加 context</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用新用户测试访问"><span class="nav-number">8.6.2.5.</span> <span class="nav-text">使用新用户测试访问</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-Role"><span class="nav-number">8.6.2.6.</span> <span class="nav-text">创建 Role</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-Rolebinding"><span class="nav-number">8.6.2.7.</span> <span class="nav-text">创建 Rolebinding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试用户权限"><span class="nav-number">8.6.2.8.</span> <span class="nav-text">测试用户权限</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-6"><span class="nav-number">8.7.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#应用发布-部署实际项目"><span class="nav-number">9.</span> <span class="nav-text">应用发布: 部署实际项目</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-2"><span class="nav-number">9.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#构建镜像"><span class="nav-number">9.2.</span> <span class="nav-text">构建镜像</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前端"><span class="nav-number">9.2.1.</span> <span class="nav-text">前端</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#后端"><span class="nav-number">9.2.2.</span> <span class="nav-text">后端</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Work"><span class="nav-number">9.2.3.</span> <span class="nav-text">Work</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#构建发布"><span class="nav-number">9.2.4.</span> <span class="nav-text">构建发布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#容器编排-Docker-Compose"><span class="nav-number">9.3.</span> <span class="nav-text">容器编排 Docker Compose</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#编写配置文件并部署"><span class="nav-number">9.4.</span> <span class="nav-text">编写配置文件并部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Namespace"><span class="nav-number">9.4.1.</span> <span class="nav-text">Namespace</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Redis-资源"><span class="nav-number">9.4.2.</span> <span class="nav-text">Redis 资源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Redis-service"><span class="nav-number">9.4.3.</span> <span class="nav-text">Redis service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#后端服务"><span class="nav-number">9.4.4.</span> <span class="nav-text">后端服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#后端-Service"><span class="nav-number">9.4.5.</span> <span class="nav-text">后端 Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前端-1"><span class="nav-number">9.4.6.</span> <span class="nav-text">前端</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前端-Service"><span class="nav-number">9.4.7.</span> <span class="nav-text">前端 Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Work-1"><span class="nav-number">9.4.8.</span> <span class="nav-text">Work</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#扩缩容"><span class="nav-number">9.5.</span> <span class="nav-text">扩缩容</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-7"><span class="nav-number">9.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#应用管理-初识-Helm"><span class="nav-number">10.</span> <span class="nav-text">应用管理: 初识 Helm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-3"><span class="nav-number">10.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Helm-介绍"><span class="nav-number">10.2.</span> <span class="nav-text">Helm 介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Helm-安装"><span class="nav-number">10.3.</span> <span class="nav-text">Helm 安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#客户端-helm"><span class="nav-number">10.3.1.</span> <span class="nav-text">客户端 helm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#服务端-Tiller"><span class="nav-number">10.3.2.</span> <span class="nav-text">服务端 Tiller</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#本地安装"><span class="nav-number">10.3.2.1.</span> <span class="nav-text">本地安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#默认安装"><span class="nav-number">10.3.2.2.</span> <span class="nav-text">默认安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#手动安装"><span class="nav-number">10.3.2.3.</span> <span class="nav-text">手动安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RBAC-使用"><span class="nav-number">10.3.2.4.</span> <span class="nav-text">RBAC 使用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Helm-概念"><span class="nav-number">10.4.</span> <span class="nav-text">Helm 概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Chart"><span class="nav-number">10.4.1.</span> <span class="nav-text">Chart</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Release"><span class="nav-number">10.4.2.</span> <span class="nav-text">Release</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Repository"><span class="nav-number">10.4.3.</span> <span class="nav-text">Repository</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Config"><span class="nav-number">10.4.4.</span> <span class="nav-text">Config</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Helm-的工作原理"><span class="nav-number">10.5.</span> <span class="nav-text">Helm 的工作原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-8"><span class="nav-number">10.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#部署实践-以-Helm-部署项目"><span class="nav-number">11.</span> <span class="nav-text">部署实践: 以 Helm 部署项目</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概览"><span class="nav-number">11.1.</span> <span class="nav-text">概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Helm-chart"><span class="nav-number">11.2.</span> <span class="nav-text">Helm chart</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Chart-结构"><span class="nav-number">11.2.1.</span> <span class="nav-text">Chart 结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Chart-yaml"><span class="nav-number">11.2.1.1.</span> <span class="nav-text">Chart.yaml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#charts"><span class="nav-number">11.2.1.2.</span> <span class="nav-text">charts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#helmignore"><span class="nav-number">11.2.1.3.</span> <span class="nav-text">.helmignore</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#templates"><span class="nav-number">11.2.1.4.</span> <span class="nav-text">templates</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#values-yaml"><span class="nav-number">11.2.1.5.</span> <span class="nav-text">values.yaml</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编写-chart"><span class="nav-number">11.2.2.</span> <span class="nav-text">编写 chart</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署-1"><span class="nav-number">11.3.</span> <span class="nav-text">部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#直接部署"><span class="nav-number">11.3.1.</span> <span class="nav-text">直接部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#打包"><span class="nav-number">11.3.2.</span> <span class="nav-text">打包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#访问服务"><span class="nav-number">11.3.3.</span> <span class="nav-text">访问服务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-9"><span class="nav-number">11.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：kube-apiserver"><span class="nav-number">12.</span> <span class="nav-text">庖丁解牛：kube-apiserver</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-4"><span class="nav-number">12.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#REST-API-Server"><span class="nav-number">12.2.</span> <span class="nav-text">REST API Server</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#认证（Authentication）-1"><span class="nav-number">12.2.1.</span> <span class="nav-text">认证（Authentication）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#授权（Authorization）-1"><span class="nav-number">12.2.2.</span> <span class="nav-text">授权（Authorization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#准入控制（Admission-Control）"><span class="nav-number">12.2.3.</span> <span class="nav-text">准入控制（Admission Control）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理请求"><span class="nav-number">12.2.4.</span> <span class="nav-text">处理请求</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-10"><span class="nav-number">12.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：etcd"><span class="nav-number">13.</span> <span class="nav-text">庖丁解牛：etcd</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-5"><span class="nav-number">13.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd-是什么"><span class="nav-number">13.2.</span> <span class="nav-text">etcd 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd-有什么作用"><span class="nav-number">13.3.</span> <span class="nav-text">etcd 有什么作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd-是如何被使用的"><span class="nav-number">13.4.</span> <span class="nav-text">etcd 是如何被使用的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-11"><span class="nav-number">13.5.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考链接"><span class="nav-number">13.6.</span> <span class="nav-text">参考链接</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：controller-manager"><span class="nav-number">14.</span> <span class="nav-text">庖丁解牛：controller-manager</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-6"><span class="nav-number">14.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-controller-manager-是什么"><span class="nav-number">14.2.</span> <span class="nav-text">kube-controller-manager 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-controller-manager-有什么作用"><span class="nav-number">14.3.</span> <span class="nav-text">kube-controller-manager 有什么作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-controller-manager-是如何工作的"><span class="nav-number">14.4.</span> <span class="nav-text">kube-controller-manager 是如何工作的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-12"><span class="nav-number">14.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：kube-scheduler"><span class="nav-number">15.</span> <span class="nav-text">庖丁解牛：kube-scheduler</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-7"><span class="nav-number">15.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-scheduler-是什么"><span class="nav-number">15.2.</span> <span class="nav-text">kube-scheduler 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-scheduler-有什么作用"><span class="nav-number">15.3.</span> <span class="nav-text">kube-scheduler 有什么作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-scheduler-是如何工作的"><span class="nav-number">15.4.</span> <span class="nav-text">kube-scheduler 是如何工作的</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#处理阶段"><span class="nav-number">15.4.1.</span> <span class="nav-text">处理阶段</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-13"><span class="nav-number">15.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：kubelet"><span class="nav-number">16.</span> <span class="nav-text">庖丁解牛：kubelet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-8"><span class="nav-number">16.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-是什么"><span class="nav-number">16.2.</span> <span class="nav-text">kubelet 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-有什么作用"><span class="nav-number">16.3.</span> <span class="nav-text">kubelet 有什么作用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#节点管理"><span class="nav-number">16.3.1.</span> <span class="nav-text">节点管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod-管理"><span class="nav-number">16.3.2.</span> <span class="nav-text">Pod 管理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-是如何工作的"><span class="nav-number">16.4.</span> <span class="nav-text">kubelet 是如何工作的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-14"><span class="nav-number">16.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：kube-proxy"><span class="nav-number">17.</span> <span class="nav-text">庖丁解牛：kube-proxy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-9"><span class="nav-number">17.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-proxy-是什么"><span class="nav-number">17.2.</span> <span class="nav-text">kube-proxy 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-proxy-如何工作"><span class="nav-number">17.3.</span> <span class="nav-text">kube-proxy 如何工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-15"><span class="nav-number">17.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#庖丁解牛：Container-Runtime-（Docker）"><span class="nav-number">18.</span> <span class="nav-text">庖丁解牛：Container Runtime （Docker）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-10"><span class="nav-number">18.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Container-Runtime-是什么"><span class="nav-number">18.2.</span> <span class="nav-text">Container Runtime 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Docker-是什么"><span class="nav-number">18.3.</span> <span class="nav-text">Docker 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CRI-是什么"><span class="nav-number">18.4.</span> <span class="nav-text">CRI 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Docker-如何工作"><span class="nav-number">18.5.</span> <span class="nav-text">Docker 如何工作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#部署一个-Redis"><span class="nav-number">18.5.1.</span> <span class="nav-text">部署一个 Redis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看详情"><span class="nav-number">18.5.2.</span> <span class="nav-text">查看详情</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#调度"><span class="nav-number">18.5.2.1.</span> <span class="nav-text">调度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pull-镜像"><span class="nav-number">18.5.2.2.</span> <span class="nav-text">pull 镜像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建镜像并启动"><span class="nav-number">18.5.2.3.</span> <span class="nav-text">创建镜像并启动</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-16"><span class="nav-number">18.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Troubleshoot"><span class="nav-number">19.</span> <span class="nav-text">Troubleshoot</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-11"><span class="nav-number">19.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#应用部署问题"><span class="nav-number">19.2.</span> <span class="nav-text">应用部署问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-describe-排查问题"><span class="nav-number">19.2.1.</span> <span class="nav-text">使用 describe 排查问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-events-排查问题"><span class="nav-number">19.2.2.</span> <span class="nav-text">使用 events 排查问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#修正错误"><span class="nav-number">19.2.3.</span> <span class="nav-text">修正错误</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过详细内容排查错误"><span class="nav-number">19.2.4.</span> <span class="nav-text">通过详细内容排查错误</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集群问题"><span class="nav-number">19.3.</span> <span class="nav-text">集群问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-17"><span class="nav-number">19.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#扩展增强：Dashboard"><span class="nav-number">20.</span> <span class="nav-text">扩展增强：Dashboard</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-12"><span class="nav-number">20.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何安装"><span class="nav-number">20.2.</span> <span class="nav-text">如何安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#访问-Dashboard"><span class="nav-number">20.3.</span> <span class="nav-text">访问 Dashboard</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#查找-Token"><span class="nav-number">20.3.1.</span> <span class="nav-text">查找 Token</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#修正权限"><span class="nav-number">20.3.2.</span> <span class="nav-text">修正权限</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署应用"><span class="nav-number">20.4.</span> <span class="nav-text">部署应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-18"><span class="nav-number">20.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#扩展增强：CoreDNS"><span class="nav-number">21.</span> <span class="nav-text">扩展增强：CoreDNS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-13"><span class="nav-number">21.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CoreDNS-是什么"><span class="nav-number">21.2.</span> <span class="nav-text">CoreDNS 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何安装使用-CoreDNS"><span class="nav-number">21.3.</span> <span class="nav-text">如何安装使用 CoreDNS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#验证-CoreDNS-功能"><span class="nav-number">21.4.</span> <span class="nav-text">验证 CoreDNS 功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#配置和监控"><span class="nav-number">21.5.</span> <span class="nav-text">配置和监控</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-19"><span class="nav-number">21.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#服务增强：Ingress"><span class="nav-number">22.</span> <span class="nav-text">服务增强：Ingress</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-14"><span class="nav-number">22.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ingress-是什么"><span class="nav-number">22.2.</span> <span class="nav-text">Ingress 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何使用"><span class="nav-number">22.3.</span> <span class="nav-text">如何使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-1"><span class="nav-number">22.3.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实践-1"><span class="nav-number">22.3.2.</span> <span class="nav-text">实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-20"><span class="nav-number">22.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#监控实践：对-K8S-集群进行监控"><span class="nav-number">23.</span> <span class="nav-text">监控实践：对 K8S 集群进行监控</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体概览-15"><span class="nav-number">23.1.</span> <span class="nav-text">整体概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#监控什么"><span class="nav-number">23.2.</span> <span class="nav-text">监控什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prometheus"><span class="nav-number">23.3.</span> <span class="nav-text">Prometheus</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装-Prometheus"><span class="nav-number">23.4.</span> <span class="nav-text">安装 Prometheus</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装-Node-exporter"><span class="nav-number">23.5.</span> <span class="nav-text">安装 Node exporter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-21"><span class="nav-number">23.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结-22"><span class="nav-number">24.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#快速回顾"><span class="nav-number">24.1.</span> <span class="nav-text">快速回顾</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#扩展阅读"><span class="nav-number">24.2.</span> <span class="nav-text">扩展阅读</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-23"><span class="nav-number">24.3.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Frank.Wong</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">86.3k</span>
  
</div>

<!--
  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>
-->



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/frank-wong.github.io/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/frank-wong.github.io/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/frank-wong.github.io/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/frank-wong.github.io/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/frank-wong.github.io/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/frank-wong.github.io/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/frank-wong.github.io/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/frank-wong.github.io/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/frank-wong.github.io/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/frank-wong.github.io/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/frank-wong.github.io/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/frank-wong.github.io/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
  
  <link rel="stylesheet" href="/frank-wong.github.io/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/frank-wong.github.io/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  

  

  

<script src="/frank-wong.github.io/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/frank-wong.github.io/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
